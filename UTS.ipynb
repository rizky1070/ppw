{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddcc36a9",
   "metadata": {},
   "source": [
    "# UTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5999886",
   "metadata": {},
   "source": [
    "# 1.\tLakukan analisa klasifikasikan berita dengan extraksi fitur model topik modelling dengan classifier naïve bayes dan SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5a27a",
   "metadata": {},
   "source": [
    "# Preprocessing Berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b0c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from berita_cnn.csv\n",
    "df = pd.read_csv('Berita.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f942e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f790c530",
   "metadata": {},
   "source": [
    "#   PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan data \"isi\"\n",
    "df['berita']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fbcff8",
   "metadata": {},
   "source": [
    "## Hapus Missing Value dan Data Duplicat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0070d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapus baris dengan Missing Value di 'berita'\n",
    "df.dropna(subset=['berita'], inplace=True)\n",
    "\n",
    "# Hapus data duplikat\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b5941",
   "metadata": {},
   "source": [
    "## Test Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Fungsi untuk membersihkan teks\n",
    "def clean_text(text):\n",
    "    # Pastikan input adalah string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    text = text.lower() # 1. Ubah ke huruf kecil\n",
    "    \n",
    "    # 2. Ganti karakter non-breaking space (U+00A0) dengan spasi biasa\n",
    "    text = text.replace(u'\\xa0', u' ')\n",
    "    \n",
    "    # 3. Hapus awalan kota dan sumber berita \n",
    "    # Pola: NAMA_KOTA,BANGSAONLINE.COM-\n",
    "    text = re.sub(r'^\\w+\\s*,\\s*bangsaonline\\.com[–-]?\\s*', '', text)\n",
    "    \n",
    "    # 4. Hapus semua karakter yang BUKAN huruf, angka, atau spasi\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 5. Hapus semua angka\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 6. Ganti spasi ganda/lebih menjadi satu spasi & hapus spasi di awal/akhir\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Terapkan pembersihan ke kolom 'isi'\n",
    "df['cleaned_isi'] = df['berita'].apply(clean_text)\n",
    "\n",
    "# Tampilkan DataFrame\n",
    "display(df[['berita', 'cleaned_isi']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ea2ff7",
   "metadata": {},
   "source": [
    "## Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67fa1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Perintah untuk menginstal library menggunakan path Python yang sedang aktif\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a2cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') \n",
    "\n",
    "# Fungsi untuk melakukan tokenisasi\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Terapkan tokenisasi ke kolom 'cleaned_isi'\n",
    "df['tokenized_isi'] = df['cleaned_isi'].apply(tokenize_text)\n",
    "\n",
    "# Tampilkan DataFrame dengan kolom hasil tokenisasi\n",
    "display(df[['cleaned_isi', 'tokenized_isi']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07d506",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Dapatkan Stop Word bahasa Indonesia\n",
    "list_stopwords = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Fungsi untuk menghapus stop words\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in list_stopwords]\n",
    "\n",
    "# Terapkan penghapusan Stop Word ke kolom 'tokenized_isi'\n",
    "df['stopwords_removed_isi'] = df['tokenized_isi'].apply(remove_stopwords)\n",
    "\n",
    "# Tampilkan DataFrame\n",
    "display(df[['tokenized_isi', 'stopwords_removed_isi']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f1e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Gabungkan semua token setelah stopword removal menjadi satu daftar\n",
    "all_words_after_stopwords = [word for tokens in df['stopwords_removed_isi'] for word in tokens]\n",
    "\n",
    "# Hitung frekuensi setiap kata\n",
    "word_frequencies = Counter(all_words_after_stopwords)\n",
    "\n",
    "# Menampilkan kata-kata yang paling umum dan frekuensinya\n",
    "print(\"Top Most Frequent Words (Without Stemming):\")\n",
    "for word, frequency in word_frequencies.most_common(20): # Menampilkan 20 kata teratas\n",
    "    print(f\"{word}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat DataFrame baru dengan isi berita asli, hasil preprocessing, dan kategori\n",
    "processed_df = df[['berita', 'stopwords_removed_isi', 'kategori']].copy()\n",
    "\n",
    "# Ganti nama kolom 'stopwords_removed_isi' menjadi 'hasil_preprocessing'\n",
    "processed_df.rename(columns={'stopwords_removed_isi': 'hasil_preprocessing'}, inplace=True)\n",
    "\n",
    "# Konversi frekuensi kata ke DataFrame\n",
    "frequency_df = pd.DataFrame.from_dict(word_frequencies, orient='index', columns=['frequency'])\n",
    "frequency_df.index.name = 'word'\n",
    "frequency_df.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "\n",
    "# Simpan ke dua file CSV terpisah\n",
    "processed_df.to_csv('hasil_preprocessing_beritaUTS.csv', index=False, encoding='utf-8')\n",
    "frequency_df.to_csv('frekuensi_kata_beritaUTS.csv', encoding='utf-8')\n",
    "\n",
    "print(\"Hasil preprocessing disimpan di 'hasil_preprocessing_beritaUTS.csv'\")\n",
    "print(\"Frekuensi kata disimpan di 'frekuensi_kata_beritaUTS.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_preprocessing = \"hasil_preprocessing_beritaUTS.csv\"  \n",
    "df = pd.read_csv(hasil_preprocessing)\n",
    "\n",
    "# Tampilkan data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frekuensi_kata = \"frekuensi_kata_beritaUTS.csv\"  \n",
    "df = pd.read_csv(frekuensi_kata)\n",
    "\n",
    "# Tampilkan data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c8c45",
   "metadata": {},
   "source": [
    "# Analisa klasifikasikan berita dengan extraksi fitur model topik modelling dengan classifier naïve bayes dan SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ab22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP\n",
    "\n",
    "# Import library dasar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Import untuk LDA & Koherensi\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore, CoherenceModel, HdpModel\n",
    "\n",
    "# Import untuk Klasifikasi & Evaluasi\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Import untuk Visualisasi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Semua library berhasil diimpor.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat Dataset \n",
    "try:\n",
    "    df = pd.read_csv('hasil_preprocessing_beritaUTS.csv')\n",
    "    print(f\"\\nDataset berhasil dimuat. Jumlah data: {len(df)} baris.\")\n",
    "    print(\"Contoh data awal:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nError: File tidak ditemukan. Pastikan nama file CSV sudah benar.\")\n",
    "    # Jika file tidak ditemukan, hentikan proses.\n",
    "    # df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EKSTRAKSI FITUR LDA & PENCARIAN TOPIK OPTIMAL\n",
    "\n",
    "# Siapkan data untuk Gensim\n",
    "documents = [doc.split() for doc in df['hasil_preprocessing']]\n",
    "dictionary = Dictionary(documents)\n",
    "\n",
    "# Filter Kamus \n",
    "# Membuang kata yang terlalu jarang atau terlalu sering muncul.\n",
    "# Ini meningkatkan kualitas topik secara signifikan.\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "print(f\"\\nKamus dibuat dan difilter. Jumlah kata unik: {len(dictionary)}\")\n",
    "\n",
    "\n",
    "# Estimasi Jumlah Topik dengan HDP \n",
    "print(\"\\n--- Menjalankan HDP untuk estimasi jumlah topik... ---\")\n",
    "hdp_model = HdpModel(corpus=corpus, id2word=dictionary)\n",
    "estimated_num_topics = len(hdp_model.print_topics())\n",
    "print(f\"✅ HDP mengestimasi ada sekitar: {estimated_num_topics} topik.\")\n",
    "\n",
    "\n",
    "# Mencari Jumlah Topik Terbaik dengan Plot Koherensi \n",
    "def compute_coherence_values_multicore(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    start_time = time.time()\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics,\n",
    "                             random_state=42, passes=10, workers=3)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        current_coherence = coherencemodel.get_coherence()\n",
    "        coherence_values.append(current_coherence)\n",
    "        print(f\"Selesai menghitung untuk {num_topics} topik. Skor Koherensi: {current_coherence:.4f}\")\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal waktu pencarian koherensi: {total_time/60:.2f} menit\")\n",
    "    return coherence_values\n",
    "\n",
    "# Atur rentang pencarian di sekitar hasil HDP\n",
    "search_start = max(2, estimated_num_topics - 15)\n",
    "search_limit = estimated_num_topics + 20\n",
    "search_step = 5\n",
    "\n",
    "print(f\"\\n--- Menjalankan pencarian koherensi dari {search_start} hingga {search_limit} topik... ---\")\n",
    "coherence_values = compute_coherence_values_multicore(dictionary=dictionary, corpus=corpus, texts=documents,\n",
    "                                                    start=search_start, limit=search_limit, step=search_step)\n",
    "\n",
    "# Tampilkan grafik\n",
    "x = range(search_start, search_limit, search_step)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, coherence_values, marker='o')\n",
    "plt.title(\"Pencarian Jumlah Topik Optimal\", fontsize=16)\n",
    "plt.xlabel(\"Jumlah Topik (num_topics)\")\n",
    "plt.ylabel(\"Skor Koherensi (c_v)\")\n",
    "plt.xticks(x)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Pilih jumlah topik terbaik (yang memiliki skor koherensi tertinggi)\n",
    "optimal_num_topics = x[np.argmax(coherence_values)]\n",
    "print(f\"\\n✅ Jumlah topik optimal yang ditemukan: {optimal_num_topics}\")\n",
    "\n",
    "\n",
    "# Latih Model LDA Final & Ekstrak Fitur ---\n",
    "print(\"\\n--- Melatih model LDA final dengan topik optimal... ---\")\n",
    "lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=optimal_num_topics,\n",
    "                         random_state=42, passes=15)\n",
    "\n",
    "print(\"\\n--- Topik-topik yang Ditemukan oleh Model LDA ---\")\n",
    "    # Tampilkan 15 kata teratas untuk setiap topik\n",
    "for idx, topic in lda_model.print_topics(num_words=15):\n",
    "    print(f\"Topik: {idx}\")\n",
    "    print(f\"Kata-kata: {topic}\\n\")\n",
    "\n",
    "# Ekstrak fitur (distribusi topik) untuk setiap dokumen\n",
    "def get_lda_features(lda_model, bow_corpus):\n",
    "    features = []\n",
    "    for doc_bow in bow_corpus:\n",
    "        topic_distribution = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
    "        doc_features = [0.0] * optimal_num_topics\n",
    "        for topic_id, prob in topic_distribution:\n",
    "            doc_features[topic_id] = prob\n",
    "        features.append(doc_features)\n",
    "    return np.array(features)\n",
    "\n",
    "X = get_lda_features(lda_model, corpus)\n",
    "y = df['kategori'].values\n",
    "\n",
    "print(\"Ekstraksi fitur LDA selesai.\")\n",
    "print(f\"Bentuk matriks fitur (X): {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde8d09",
   "metadata": {},
   "source": [
    "## Menggunakan SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc09ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PELATIHAN DAN EVALUASI MODEL KLASIFIKASI\n",
    "\n",
    "# Bagi Data menjadi Latih dan Uji ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"\\nData dibagi. Jumlah data latih: {len(X_train)}, Jumlah data uji: {len(X_test)}\")\n",
    "\n",
    "\n",
    "# Latih Model Klasifikasi (SVM) \n",
    "classifier = SVC(kernel='linear', random_state=42, probability=True)\n",
    "\n",
    "print(\"\\n--- Memulai pelatihan model SVM... ---\")\n",
    "classifier.fit(X_train, y_train)\n",
    "print(\"Pelatihan selesai.\")\n",
    "\n",
    "\n",
    "# Evaluasi Kinerja Model\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n--- HASIL EVALUASI AKHIR ---\")\n",
    "print(f\"Akurasi Model: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nLaporan Klasifikasi (Classification Report):\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1d38e",
   "metadata": {},
   "source": [
    "## Menggunakan Naivi Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efac061",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PELATIHAN DAN EVALUASI MODEL KLASIFIKASI\n",
    "\n",
    "# Bagi Data menjadi Latih dan Uji ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"\\nData dibagi. Jumlah data latih: {len(X_train)}, Jumlah data uji: {len(X_test)}\")\n",
    "\n",
    "classifier_nb = MultinomialNB()\n",
    "print(\"\\nMemulai pelatihan model Naive Bayes...\")\n",
    "classifier_nb.fit(X_train, y_train)\n",
    "print(\"Pelatihan selesai.\")\n",
    "\n",
    "y_pred_nb = classifier_nb.predict(X_test)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(\"\\n--- HASIL EVALUASI (NAIVE BAYES) ---\")\n",
    "print(f\"Akurasi Model: {accuracy_nb * 100:.2f}%\")\n",
    "print(\"\\nLaporan Klasifikasi (Classification Report):\")\n",
    "print(classification_report(y_test, y_pred_nb, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84481650",
   "metadata": {},
   "source": [
    "## Kesimpulan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12410e1",
   "metadata": {},
   "source": [
    "# 2.\tLakukan  analisa clutering dokumen pada data email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0df27",
   "metadata": {},
   "source": [
    "# Preprocessing Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from berita_cnn.csv\n",
    "df = pd.read_csv('spam.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110fecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a160a4",
   "metadata": {},
   "source": [
    "#   PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b191ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan data \"isi\"\n",
    "df['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed0104",
   "metadata": {},
   "source": [
    "## Hapus Missing Value dan Data Duplicat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2081e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapus baris dengan Missing Value di 'berita'\n",
    "df.dropna(subset=['Text'], inplace=True)\n",
    "\n",
    "# Hapus data duplikat\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6eefff",
   "metadata": {},
   "source": [
    "## Test Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ba19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Fungsi untuk membersihkan teks\n",
    "def clean_text(text):\n",
    "    # Pastikan input adalah string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    text = text.lower() # 1. Ubah ke huruf kecil\n",
    "    \n",
    "    # 2. Ganti karakter non-breaking space (U+00A0) dengan spasi biasa\n",
    "    text = text.replace(u'\\xa0', u' ')\n",
    "    \n",
    "    # 3. Hapus awalan kota dan sumber berita \n",
    "    # Pola: NAMA_KOTA,BANGSAONLINE.COM-\n",
    "    text = re.sub(r'^\\w+\\s*,\\s*bangsaonline\\.com[–-]?\\s*', '', text)\n",
    "    \n",
    "    # 4. Hapus semua karakter yang BUKAN huruf, angka, atau spasi\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 5. Hapus semua angka\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 6. Ganti spasi ganda/lebih menjadi satu spasi & hapus spasi di awal/akhir\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Terapkan pembersihan ke kolom 'isi'\n",
    "df['cleaned_isi'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Tampilkan DataFrame\n",
    "display(df[['Text', 'cleaned_isi']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad01b5",
   "metadata": {},
   "source": [
    "## Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2091cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Perintah untuk menginstal library menggunakan path Python yang sedang aktif\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f64b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') \n",
    "\n",
    "# Fungsi untuk melakukan tokenisasi\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Terapkan tokenisasi ke kolom 'cleaned_isi'\n",
    "df['tokenized_isi'] = df['cleaned_isi'].apply(tokenize_text)\n",
    "\n",
    "# Tampilkan DataFrame dengan kolom hasil tokenisasi\n",
    "display(df[['cleaned_isi', 'tokenized_isi']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82245cbe",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb1222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Dapatkan Stop Word bahasa Indonesia\n",
    "list_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Fungsi untuk menghapus stop words\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in list_stopwords]\n",
    "\n",
    "# Terapkan penghapusan Stop Word ke kolom 'tokenized_isi'\n",
    "df['stopwords_removed_isi'] = df['tokenized_isi'].apply(remove_stopwords)\n",
    "\n",
    "# Tampilkan DataFrame\n",
    "display(df[['tokenized_isi', 'stopwords_removed_isi']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Gabungkan semua token setelah stopword removal menjadi satu daftar\n",
    "all_words_after_stopwords = [word for tokens in df['stopwords_removed_isi'] for word in tokens]\n",
    "\n",
    "# Hitung frekuensi setiap kata\n",
    "word_frequencies = Counter(all_words_after_stopwords)\n",
    "\n",
    "# Menampilkan kata-kata yang paling umum dan frekuensinya\n",
    "print(\"Top Most Frequent Words (Without Stemming):\")\n",
    "for word, frequency in word_frequencies.most_common(20): # Menampilkan 20 kata teratas\n",
    "    print(f\"{word}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74284ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat DataFrame baru dengan isi berita asli, hasil preprocessing, dan kategori\n",
    "processed_df = df[['Text', 'stopwords_removed_isi']].copy()\n",
    "\n",
    "# Ganti nama kolom 'stopwords_removed_isi' menjadi 'hasil_preprocessing'\n",
    "processed_df.rename(columns={'stopwords_removed_isi': 'hasil_preprocessing'}, inplace=True)\n",
    "\n",
    "# Konversi frekuensi kata ke DataFrame\n",
    "frequency_df = pd.DataFrame.from_dict(word_frequencies, orient='index', columns=['frequency'])\n",
    "frequency_df.index.name = 'word'\n",
    "frequency_df.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "\n",
    "# Simpan ke dua file CSV terpisah\n",
    "processed_df.to_csv('hasil_preprocessing_emailUTS.csv', index=False, encoding='utf-8')\n",
    "frequency_df.to_csv('frekuensi_kata_emailUTS.csv', encoding='utf-8')\n",
    "\n",
    "print(\"Hasil preprocessing disimpan di 'hasil_preprocessing_emailUTS.csv'\")\n",
    "print(\"Frekuensi kata disimpan di 'frekuensi_kata_emailUTS.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_preprocessing = \"hasil_preprocessing_emailUTS.csv\"  \n",
    "df = pd.read_csv(hasil_preprocessing)\n",
    "\n",
    "# Tampilkan data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08672c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "frekuensi_kata = \"frekuensi_kata_emailUTS.csv\"  \n",
    "df = pd.read_csv(frekuensi_kata)\n",
    "\n",
    "# Tampilkan data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c01a9c",
   "metadata": {},
   "source": [
    "# Analisa clutering dokumen pada data email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d47650",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP UNTUK CLUSTERING EMAIL\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from ast import literal_eval # PENTING: Untuk membaca kolom 'tokens' dari CSV\n",
    "\n",
    "# Import untuk LDA & Koherensi\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore, CoherenceModel, HdpModel\n",
    "\n",
    "# Import untuk Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Import untuk Visualisasi\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score # Import Silhouette Score\n",
    "\n",
    "\n",
    "print(\"Semua library untuk clustering berhasil diimpor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat Dataset \n",
    "try:\n",
    "    df = pd.read_csv('hasil_preprocessing_emailUTS.csv')\n",
    "    print(f\"\\nDataset berhasil dimuat. Jumlah data: {len(df)} baris.\")\n",
    "    print(\"Contoh data awal:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nError: File tidak ditemukan. Pastikan nama file CSV sudah benar.\")\n",
    "    # Jika file tidak ditemukan, hentikan proses.\n",
    "    # df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7547b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['hasil_preprocessing'].apply(literal_eval)\n",
    "print(f\"Data bersih dimuat. Jumlah data: {len(df)} baris.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EKSTRAKSI FITUR LDA & PENCARIAN TOPIK OPTIMAL\n",
    "\n",
    "# Siapkan data untuk Gensim\n",
    "documents = df['tokens'].tolist()\n",
    "dictionary = Dictionary(documents)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "print(f\"\\nKamus dibuat dan difilter. Jumlah kata unik: {len(dictionary)}\")\n",
    "\n",
    "\n",
    "# Estimasi Jumlah Topik dengan HDP \n",
    "print(\"\\n--- Menjalankan HDP untuk estimasi jumlah topik... ---\")\n",
    "hdp_model = HdpModel(corpus=corpus, id2word=dictionary)\n",
    "estimated_num_topics = len(hdp_model.print_topics())\n",
    "print(f\"✅ HDP mengestimasi ada sekitar: {estimated_num_topics} topik.\")\n",
    "\n",
    "\n",
    "# Mencari Jumlah Topik Terbaik dengan Plot Koherensi \n",
    "def compute_coherence_values_multicore(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    start_time = time.time()\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics,\n",
    "                             random_state=42, passes=10, workers=3)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        current_coherence = coherencemodel.get_coherence()\n",
    "        coherence_values.append(current_coherence)\n",
    "        print(f\"Selesai menghitung untuk {num_topics} topik. Skor Koherensi: {current_coherence:.4f}\")\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal waktu pencarian koherensi: {total_time/60:.2f} menit\")\n",
    "    return coherence_values\n",
    "\n",
    "# Atur rentang pencarian di sekitar hasil HDP\n",
    "search_start = max(2, estimated_num_topics - 15)\n",
    "search_limit = estimated_num_topics + 20\n",
    "search_step = 5\n",
    "\n",
    "print(f\"\\n--- Menjalankan pencarian koherensi dari {search_start} hingga {search_limit} topik... ---\")\n",
    "coherence_values = compute_coherence_values_multicore(dictionary=dictionary, corpus=corpus, texts=documents,\n",
    "                                                    start=search_start, limit=search_limit, step=search_step)\n",
    "\n",
    "# Tampilkan grafik\n",
    "x = range(search_start, search_limit, search_step)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, coherence_values, marker='o')\n",
    "plt.title(\"Pencarian Jumlah Topik Optimal\", fontsize=16)\n",
    "plt.xlabel(\"Jumlah Topik (num_topics)\")\n",
    "plt.ylabel(\"Skor Koherensi (c_v)\")\n",
    "plt.xticks(x)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Pilih jumlah topik terbaik (yang memiliki skor koherensi tertinggi)\n",
    "optimal_num_topics = x[np.argmax(coherence_values)]\n",
    "print(f\"\\n✅ Jumlah topik optimal yang ditemukan: {optimal_num_topics}\")\n",
    "\n",
    "\n",
    "# Latih Model LDA Final & Ekstrak Fitur ---\n",
    "print(\"\\n--- Melatih model LDA final dengan topik optimal... ---\")\n",
    "lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=optimal_num_topics,\n",
    "                         random_state=42, passes=15)\n",
    "\n",
    "print(\"\\n--- Topik-topik yang Ditemukan oleh Model LDA ---\")\n",
    "    # Tampilkan 15 kata teratas untuk setiap topik\n",
    "for idx, topic in lda_model.print_topics(num_words=15):\n",
    "    print(f\"Topik: {idx}\")\n",
    "    print(f\"Kata-kata: {topic}\\n\")\n",
    "\n",
    "# Ekstrak fitur (distribusi topik) untuk setiap dokumen\n",
    "def get_lda_features(lda_model, bow_corpus):\n",
    "    features = []\n",
    "    for doc_bow in bow_corpus:\n",
    "        topic_distribution = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
    "        doc_features = [0.0] * optimal_num_topics\n",
    "        for topic_id, prob in topic_distribution:\n",
    "            doc_features[topic_id] = prob\n",
    "        features.append(doc_features)\n",
    "    return np.array(features)\n",
    "\n",
    "X = get_lda_features(lda_model, corpus)\n",
    "\n",
    "print(\"Ekstraksi fitur LDA selesai.\")\n",
    "print(f\"Bentuk matriks fitur (X): {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menemukan K Optimal dengan Silhouette Score \n",
    "print(\"\\n--- Mencari K Optimal dengan Silhouette Score (Metode Kuantitatif) ---\")\n",
    "K_range = range(2, 16)\n",
    "silhouette_scores = []\n",
    "for k in K_range:\n",
    "    kmeans_model = KMeans(n_clusters=k, random_state=42, n_init='auto').fit(X)\n",
    "    score = silhouette_score(X, kmeans_model.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"Untuk K = {k}, Silhouette Score-nya adalah {score:.4f}\")\n",
    "\n",
    "# Visualisasikan hasilnya\n",
    "plt.figure(figsize=(8, 6)) # Ukuran plot disesuaikan\n",
    "plt.plot(K_range, silhouette_scores, 'ro-')\n",
    "plt.xlabel('Jumlah Cluster (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score untuk Setiap Nilai K')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Jalankan K-Means dengan K Terbaik \n",
    "# Pilih K yang memberikan Silhouette Score TERTINGGI secara otomatis\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\n✅ Berdasarkan Silhouette Score, K terbaik adalah: {optimal_k}\")\n",
    "\n",
    "print(f\"\\n--- Menjalankan K-Means final dengan K={optimal_k}... ---\")\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "# Ganti 'df' dengan nama DataFrame yang berisi data bersih Anda\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Analisis Hasil Cluster ---\n",
    "print(\"\\nContoh email dari setiap cluster yang ditemukan:\")\n",
    "for i in range(optimal_k):\n",
    "    print(f\"\\n----- CLUSTER {i} -----\")\n",
    "    sample_emails = df[df['cluster'] == i]['Text'].head(2).tolist()\n",
    "    for email_text in sample_emails:\n",
    "        print(f\"- {str(email_text)[:250]}...\")\n",
    "\n",
    "print(\"\\n✅ Proses clustering selesai.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
