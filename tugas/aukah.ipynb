{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd04c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "def get_fakultas_prodi_list():\n",
    "    \"\"\"\n",
    "    Fungsi untuk mengambil daftar semua Fakultas dan Prodi di dalamnya.\n",
    "    Struktur scraping disesuaikan dengan sidebar di mana prodi berada di dalam fakultas.\n",
    "    \"\"\"\n",
    "    prodi_list = []\n",
    "    try:\n",
    "        # URL yang berisi sidebar navigasi fakultas dan prodi\n",
    "        url_nav = \"https://pta.trunojoyo.ac.id/c_search/byfac\"\n",
    "        r = requests.get(url_nav)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        \n",
    "        # 1. Targetkan sidebar navigasi utama\n",
    "        sidebar_nav = soup.select_one('div.box.sidebar_nav')\n",
    "        if not sidebar_nav:\n",
    "            print(\"Sidebar navigasi tidak ditemukan.\")\n",
    "            return []\n",
    "\n",
    "        # 2. Ambil semua item list <li> dari fakultas (level pertama)\n",
    "        fakultas_items = sidebar_nav.select_one('ul').find_all('li', recursive=False)\n",
    "        \n",
    "        for item_fakultas in fakultas_items:\n",
    "            # 3. Ambil nama fakultas dari link <a> pertama di dalam <li>\n",
    "            anchor_fakultas = item_fakultas.find('a', recursive=False)\n",
    "            if not anchor_fakultas:\n",
    "                continue\n",
    "            nama_fakultas = anchor_fakultas.get_text(strip=True)\n",
    "            \n",
    "            # 4. Cari daftar <ul> bersarang yang berisi prodi\n",
    "            ul_prodi = item_fakultas.find('ul')\n",
    "            if not ul_prodi:\n",
    "                continue\n",
    "\n",
    "            # 5. Ambil semua link <a> prodi di dalam daftar bersarang tersebut\n",
    "            for link_prodi in ul_prodi.select('li a'):\n",
    "                nama_prodi = link_prodi.get_text(strip=True)\n",
    "                href = link_prodi.get('href')\n",
    "                # Ekstrak ID prodi dari URL\n",
    "                prodi_id = href.strip('/').split('/')[-1]\n",
    "                \n",
    "                if prodi_id.isdigit():\n",
    "                    prodi_list.append({\n",
    "                        \"id_prodi\": int(prodi_id),\n",
    "                        \"nama_prodi\": nama_prodi,\n",
    "                        \"nama_fakultas\": nama_fakultas\n",
    "                    })\n",
    "                    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengambil daftar fakultas dan prodi: {e}\")\n",
    "        \n",
    "    return prodi_list\n",
    "\n",
    "def scrape_all():\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk melakukan scraping data dari semua prodi,\n",
    "    kini dengan informasi fakultas.\n",
    "    \"\"\"\n",
    "    # 1. Dapatkan daftar lengkap prodi beserta fakultasnya\n",
    "    daftar_prodi_lengkap = get_fakultas_prodi_list()\n",
    "\n",
    "    if not daftar_prodi_lengkap:\n",
    "        print(\"Tidak ada prodi yang bisa di-scrape. Program berhenti.\")\n",
    "        return\n",
    "\n",
    "    # Inisialisasi dictionary dengan kolom baru \"nama_fakultas\"\n",
    "    data = {\n",
    "        \"nama_fakultas\": [],\n",
    "        \"id_prodi\": [],\n",
    "        \"nama_prodi\": [],\n",
    "        \"penulis\": [],\n",
    "        \"judul\": [],\n",
    "        \"pembimbing_pertama\": [],\n",
    "        \"pembimbing_kedua\": [],\n",
    "        \"abstrak\": [],\n",
    "        \"abstrak_inggris\": []\n",
    "    }\n",
    "\n",
    "    # 2. Lakukan looping untuk setiap prodi dari daftar yang didapat\n",
    "    for prodi in daftar_prodi_lengkap:\n",
    "        prodi_id = prodi[\"id_prodi\"]\n",
    "        nama_prodi = prodi[\"nama_prodi\"]\n",
    "        nama_fakultas = prodi[\"nama_fakultas\"]\n",
    "        page = 1\n",
    "        jurnal_ditemukan_per_prodi = 0\n",
    "        \n",
    "        while True:\n",
    "            url = f\"https://pta.trunojoyo.ac.id/c_search/byprod/{prodi_id}/{page}\"\n",
    "            try:\n",
    "                r = requests.get(url)\n",
    "                r.raise_for_status()\n",
    "                soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "                jurnals = soup.select('li[data-cat=\"#luxury\"]')\n",
    "\n",
    "                if not jurnals:\n",
    "                    break\n",
    "                \n",
    "                jurnal_ditemukan_per_prodi += len(jurnals)\n",
    "\n",
    "                for jurnal in jurnals:\n",
    "                    # Proses scraping detail jurnal (tetap sama)\n",
    "                    jurnal_url = jurnal.select_one('a.gray.button')['href']\n",
    "                    response = requests.get(jurnal_url)\n",
    "                    response.raise_for_status()\n",
    "                    isi = BeautifulSoup(response.content, \"html.parser\").select_one('div#content_journal')\n",
    "                    if not isi: continue\n",
    "\n",
    "                    judul = isi.select_one('a.title').text.strip()\n",
    "                    penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1].strip()\n",
    "                    pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1].strip()\n",
    "                    pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(':')[1].strip()\n",
    "                    # --- PERUBAHAN PADA EKSTRAKSI ABSTRAK ---\n",
    "                    abstract_paragraphs = isi.select('p[align=\"justify\"]')\n",
    "                    \n",
    "                    # Ambil teks mentah dari abstrak Bahasa Indonesia\n",
    "                    text_indo_mentah = abstract_paragraphs[0].text if len(abstract_paragraphs) > 0 else \"Tidak ada abstrak\"\n",
    "                    # 2. BERSIHKAN TEKS MENTAH\n",
    "                    abstrak_indonesia = re.sub(r'\\s+', ' ', text_indo_mentah).strip()\n",
    "\n",
    "                    # Ambil teks mentah dari abstrak Bahasa Inggris\n",
    "                    text_inggris_mentah = abstract_paragraphs[1].text if len(abstract_paragraphs) > 1 else \"No abstract available\"\n",
    "                    # 2. BERSIHKAN TEKS MENTAH\n",
    "                    abstrak_inggris = re.sub(r'\\s+', ' ', text_inggris_mentah).strip()\n",
    "                    # --- AKHIR PERUBAHAN ---\n",
    "\n",
    "                    # Menambahkan semua data ke dictionary\n",
    "                    data[\"nama_fakultas\"].append(nama_fakultas)\n",
    "                    data[\"id_prodi\"].append(prodi_id)\n",
    "                    data[\"nama_prodi\"].append(nama_prodi)\n",
    "                    data[\"penulis\"].append(penulis)\n",
    "                    data[\"judul\"].append(judul)\n",
    "                    data[\"pembimbing_pertama\"].append(pembimbing_pertama)\n",
    "                    data[\"pembimbing_kedua\"].append(pembimbing_kedua)\n",
    "                    data[\"abstrak\"].append(abstrak_indonesia)\n",
    "                    data[\"abstrak_inggris\"].append(abstrak_inggris)\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(1)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error pada Prodi {nama_prodi} (ID: {prodi_id}) halaman {page}: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"✔️ Selesai: {nama_fakultas} - {nama_prodi} (ID: {prodi_id}) | Total {jurnal_ditemukan_per_prodi} jurnal.\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"pta_lengkap_fakultas_prodi.csv\", index=False)\n",
    "    print(\"\\n✅ Proses scraping selesai. Data disimpan ke 'pta_lengkap_fakultas_prodi.csv'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Untuk menjalankan seluruh proses scraping\n",
    "# df_final = scrape_all()\n",
    "# print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ed2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_all()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
