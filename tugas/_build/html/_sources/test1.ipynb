{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6fbe26",
   "metadata": {},
   "source": [
    "# tugas 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da22d2fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import concurrent.futures\n",
    "import random # Impor modul random untuk jeda\n",
    "\n",
    "# --- PENYESUAIAN KESEIMBANGAN ---\n",
    "# Kurangi jumlah pekerja agar tidak terlalu agresif\n",
    "MAX_WORKERS = 2\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'ganti lek mu wak'\n",
    "}\n",
    "\n",
    "# ... (fungsi get_fakultas_prodi_list tidak berubah) ...\n",
    "def get_fakultas_prodi_list():\n",
    "    prodi_list = []\n",
    "    try:\n",
    "        url_nav = \"https://pta.trunojoyo.ac.id/c_search/byfac\"\n",
    "        r = requests.get(url_nav, timeout=10, headers=HEADERS)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        sidebar_nav = soup.select_one('div.box.sidebar_nav')\n",
    "        if not sidebar_nav: return []\n",
    "        fakultas_items = sidebar_nav.select_one('ul').find_all('li', recursive=False)\n",
    "        for item_fakultas in fakultas_items:\n",
    "            anchor_fakultas = item_fakultas.find('a', recursive=False)\n",
    "            if not anchor_fakultas: continue\n",
    "            nama_fakultas = anchor_fakultas.get_text(strip=True)\n",
    "            ul_prodi = item_fakultas.find('ul')\n",
    "            if not ul_prodi: continue\n",
    "            for link_prodi in ul_prodi.select('li a'):\n",
    "                nama_prodi = link_prodi.get_text(strip=True)\n",
    "                href = link_prodi.get('href')\n",
    "                prodi_id = href.strip('/').split('/')[-1]\n",
    "                if prodi_id.isdigit():\n",
    "                    prodi_list.append({\n",
    "                        \"id_prodi\": int(prodi_id),\n",
    "                        \"nama_prodi\": nama_prodi,\n",
    "                        \"nama_fakultas\": nama_fakultas\n",
    "                    })\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengambil daftar prodi: {e}\")\n",
    "    return prodi_list\n",
    "\n",
    "\n",
    "def scrape_jurnal_detail(jurnal_url):\n",
    "    try:\n",
    "        # --- PENYESUAIAN KESEIMBANGAN ---\n",
    "        # Beri jeda acak kecil sebelum setiap permintaan detail\n",
    "        time.sleep(random.uniform(0.5, 1.5)) \n",
    "        \n",
    "        response = requests.get(jurnal_url, timeout=15, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        # ... (sisa fungsi sama)\n",
    "        isi = BeautifulSoup(response.content, \"html.parser\").select_one('div#content_journal')\n",
    "        if not isi: return None\n",
    "        judul = isi.select_one('a.title').text.strip()\n",
    "        penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1].strip()\n",
    "        pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1].strip()\n",
    "        pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(':')[1].strip()\n",
    "        abstract_paragraphs = isi.select('p[align=\"justify\"]')\n",
    "        text_indo_mentah = abstract_paragraphs[0].text if len(abstract_paragraphs) > 0 else \"\"\n",
    "        abstrak_indonesia = re.sub(r'\\s+', ' ', text_indo_mentah).strip()\n",
    "        text_inggris_mentah = abstract_paragraphs[1].text if len(abstract_paragraphs) > 1 else \"\"\n",
    "        abstrak_inggris = re.sub(r'\\s+', ' ', text_inggris_mentah).strip()\n",
    "        return {\n",
    "            \"penulis\": penulis, \"judul\": judul, \"pembimbing_pertama\": pembimbing_pertama,\n",
    "            \"pembimbing_kedua\": pembimbing_kedua, \"abstrak\": abstrak_indonesia,\n",
    "            \"abstrak_inggris\": abstrak_inggris\n",
    "        }\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "# ... (Fungsi scrape_prodi dan main tetap sama seperti versi multithreading sebelumnya) ...\n",
    "def scrape_prodi(prodi):\n",
    "    jurnal_urls = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        try:\n",
    "            url = f\"https://pta.trunojoyo.ac.id/c_search/byprod/{prodi['id_prodi']}/{page}\"\n",
    "            r = requests.get(url, timeout=10, headers=HEADERS)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            jurnals_on_page = soup.select('li[data-cat=\"#luxury\"] a.gray.button')\n",
    "            if not jurnals_on_page: break\n",
    "            for a_tag in jurnals_on_page:\n",
    "                jurnal_urls.append(a_tag['href'])\n",
    "            page += 1\n",
    "        except requests.exceptions.RequestException:\n",
    "            break\n",
    "\n",
    "    if not jurnal_urls:\n",
    "        print(f\"\\n✔️ Selesai: {prodi['nama_fakultas']} - {prodi['nama_prodi']} (ID: {prodi['id_prodi']}) | Ditemukan 0 jurnal.\")\n",
    "        return [{'id_prodi': prodi['id_prodi'], 'nama_prodi': prodi['nama_prodi'], 'nama_fakultas': prodi['nama_fakultas'], 'judul': 'Tidak ada jurnal', 'penulis': None, 'pembimbing_pertama': None, 'pembimbing_kedua': None, 'abstrak': None, 'abstrak_inggris': None}]\n",
    "\n",
    "    all_jurnal_data = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        hasil_scrape = executor.map(scrape_jurnal_detail, jurnal_urls)\n",
    "        for hasil in hasil_scrape:\n",
    "            if hasil:\n",
    "                hasil['id_prodi'] = prodi['id_prodi']\n",
    "                hasil['nama_prodi'] = prodi['nama_prodi']\n",
    "                hasil['nama_fakultas'] = prodi['nama_fakultas']\n",
    "                all_jurnal_data.append(hasil)\n",
    "    \n",
    "    print(f\"\\n✔️ Selesai: {prodi['nama_fakultas']} - {prodi['nama_prodi']} (ID: {prodi['id_prodi']}) | Berhasil mengambil {len(all_jurnal_data)} dari {len(jurnal_urls)} jurnal yang ditemukan.\")\n",
    "    return all_jurnal_data\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    daftar_prodi = get_fakultas_prodi_list()\n",
    "    if not daftar_prodi: return pd.DataFrame()\n",
    "\n",
    "    final_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        results_per_prodi = executor.map(scrape_prodi, daftar_prodi)\n",
    "        for list_jurnal in results_per_prodi:\n",
    "            final_results.extend(list_jurnal)\n",
    "\n",
    "    df = pd.DataFrame(final_results)\n",
    "    if not df.empty:\n",
    "        original_prodi_order = [p['id_prodi'] for p in daftar_prodi]\n",
    "        df['id_prodi'] = pd.Categorical(df['id_prodi'], categories=original_prodi_order, ordered=True)\n",
    "        df = df.sort_values('id_prodi').reset_index(drop=True)\n",
    "        df = df[['nama_fakultas', 'id_prodi', 'nama_prodi', 'judul', 'penulis', 'pembimbing_pertama', 'pembimbing_kedua', 'abstrak', 'abstrak_inggris']]\n",
    "    \n",
    "    df.to_csv(\"pta_semua_jurnal_concurrent.csv\", index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"\\n\\n✅ Proses scraping selesai.\")\n",
    "    print(f\"Total baris data yang dihasilkan: {len(df)}.\")\n",
    "    print(f\"Total waktu eksekusi: {end_time - start_time:.2f} detik.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     df_hasil_semua = main()\n",
    "#     df_hasil_semua\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367b749",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
