{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89da0803",
   "metadata": {},
   "source": [
    "# CBOW Berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03765425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\laragon\\bin\\python\\python-3.10\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4319e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rizky\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rizky\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\laragon\\bin\\python\\python-3.10\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e3d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 1: MEMPERSIAPKAN CORPUS ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses persiapan corpus selesai.\n",
      "Berikut adalah contoh 1 dokumen (berita) yang sudah diubah menjadi daftar token:\n",
      "['kejar', 'tanjung', 'perak', 'surabaya', 'geledah', 'kantor', 'pt', 'pelindo', 'regional', 'surabaya', 'duga', 'kait', 'korupsi', 'kolam', 'labuh', 'kepala', 'kejar', 'tanjung', 'perak', 'ricky', 'setiawan', 'anas', 'geledah', 'kamis', 'wib', 'tim', 'sidik', 'kejar', 'tanjung', 'perak', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'dasar', 'tetap', 'pn', 'tipikor', 'surabaya', 'nomor', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'terang', 'kamis', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'sidik', 'pidsus', 'kejar', 'tanjung', 'perak', 'geledah', 'lokasi', 'tepat', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'dasar', 'tetap', 'geledah', 'pn', 'tipikor', 'surabaya', 'no', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'geledah', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'terang', 'geledah', 'personel', 'jaksa', 'kejar', 'tanjung', 'perak', 'pidsus', 'kejat', 'jatim', 'aman', 'tni', 'geledah', 'orang', 'jaksa', 'sidik', 'orang', 'personil', 'amc', 'kejat', 'jatim', 'orang', 'personil', 'pam', 'tni', 'imbuh', 'ricky', 'geledah', 'kait', 'sidi', 'perkara', 'duga', 'tindak', 'pidana', 'korupsi', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'pt', 'pelindo', 'sub', 'reg', 'bersamasama', 'pt', 'apbs', 'turut', 'duga', 'tipikor', 'turut', 'duga', 'tipikor', 'pt', 'apbs', 'pelindo', 'sub', 'regional', 'capai', 'ratus', 'miliar', 'rupiah', 'nilai', 'giat', 'rp', 'miliar', 'ricky', 'geledah', 'kumpul', 'bukti', 'tambah', 'kait', 'duga', 'tipikor', 'giat', 'eru', 'kolam', 'labuh', 'labuh', 'tanjung', 'perak', 'geledah', 'lokasi', 'sidik', 'laksana', 'giat', 'sita', 'kait', 'buktibukti', 'hubung', 'giat', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'laptop', 'dokumen', 'kait', 'kontrak', 'giat', 'rif']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast # Library untuk mengubah string menjadi list\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "\n",
    "# --- 1. Persiapan Corpus ---\n",
    "print(\"--- TAHAP 1: MEMPERSIAPKAN CORPUS ---\")\n",
    "# Load kembali data Anda (atau lanjutkan dari DataFrame yang sudah ada)\n",
    "df = pd.read_csv('hasil_preprocessing_berita.csv')\n",
    "\n",
    "# --- Konversi kolom 'hasil_preprocessing' dari string ke list ---\n",
    "# Ini adalah langkah penting!\n",
    "# ast.literal_eval akan membaca string \"['a', 'b']\" dan mengubahnya menjadi list ['a', 'b']\n",
    "df['tokens'] = df['hasil_preprocessing'].apply(ast.literal_eval)\n",
    "\n",
    "# Buat corpus yang siap untuk dilatih\n",
    "corpus = df['tokens'].tolist()\n",
    "\n",
    "\n",
    "print(\"Proses persiapan corpus selesai.\")\n",
    "print(\"Berikut adalah contoh 1 dokumen (berita) yang sudah diubah menjadi daftar token:\")\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10107212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:19,792 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:19,794 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 2: MELATIH MODEL WORD2VEC (CBOW) ---\n",
      "Parameter: Dimensi vektor = 150, Arsitektur = CBOW\n",
      "Gensim akan menampilkan log proses training di bawah ini:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:20,151 : INFO : collected 41180 word types from a corpus of 763911 raw words and 3653 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:20,156 : INFO : Creating a fresh vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:20,340 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 21810 unique words (52.96% of original 41180, drops 19370)', 'datetime': '2025-12-18T08:00:20.340960', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:20,342 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 744541 word corpus (97.46% of original 763911, drops 19370)', 'datetime': '2025-12-18T08:00:20.342955', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:20,700 : INFO : deleting the raw counts dictionary of 41180 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:20,706 : INFO : sample=0.001 downsamples 16 most-common words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:20,708 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 736320.3421055112 word corpus (98.9%% of prior 744541)', 'datetime': '2025-12-18T08:00:20.708971', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:21,291 : INFO : estimated required memory for 21810 words and 150 dimensions: 37077000 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:21,292 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:21,326 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-18T08:00:21.326282', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:21,329 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 21810 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-18T08:00:21.329283', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:22,341 : INFO : EPOCH 0 - PROGRESS: at 84.48% examples, 620252 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:22,483 : INFO : EPOCH 0: training on 763911 raw words (736176 effective words) took 1.1s, 641889 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:23,399 : INFO : EPOCH 1: training on 763911 raw words (736263 effective words) took 0.9s, 809071 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:24,351 : INFO : EPOCH 2: training on 763911 raw words (736324 effective words) took 0.9s, 778759 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:25,240 : INFO : EPOCH 3: training on 763911 raw words (736290 effective words) took 0.9s, 834345 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:26,189 : INFO : EPOCH 4: training on 763911 raw words (736236 effective words) took 0.9s, 780957 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:26,191 : INFO : Word2Vec lifecycle event {'msg': 'training on 3819555 raw words (3681289 effective words) took 4.9s, 757385 effective words/s', 'datetime': '2025-12-18T08:00:26.191338', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:00:26,193 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=21810, vector_size=150, alpha=0.025>', 'datetime': '2025-12-18T08:00:26.193341', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pelatihan model selesai!\n",
      "\n",
      "--- Melihat Hasil Pelatihan Model ---\n",
      "Model berhasil mempelajari 21810 kata unik.\n",
      "\n",
      "Contoh kata yang paling mirip dengan 'polisi':\n",
      "[('lidi', 0.8942711353302002), ('massa', 0.8940658569335938), ('teror', 0.8846123218536377), ('anarkis', 0.8782747387886047), ('rusuh', 0.8740360736846924)]\n",
      "\n",
      "Contoh kata yang paling mirip dengan 'surabaya':\n",
      "[('ppns', 0.8210594058036804), ('kupang', 0.7993782162666321), ('bandung', 0.7738825082778931), ('delta', 0.7615419030189514), ('mojokerto', 0.7601076364517212)]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Melatih Model Word2Vec (dengan Proses Terlihat) ---\n",
    "\n",
    "print(\"--- TAHAP 2: MELATIH MODEL WORD2VEC (CBOW) ---\")\n",
    "\n",
    "# Mengaktifkan logging untuk melihat proses training dari Gensim\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "embedding_dim = 150\n",
    "print(f\"Parameter: Dimensi vektor = {embedding_dim}, Arsitektur = CBOW\")\n",
    "print(\"Gensim akan menampilkan log proses training di bawah ini:\")\n",
    "\n",
    "model_cbow = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=embedding_dim,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=0,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "print(\"\\nPelatihan model selesai!\")\n",
    "print(\"\\n--- Melihat Hasil Pelatihan Model ---\")\n",
    "# Mengetahui ukuran kosakata yang berhasil dipelajari model\n",
    "vocab_size = len(model_cbow.wv.index_to_key)\n",
    "print(f\"Model berhasil mempelajari {vocab_size} kata unik.\")\n",
    "\n",
    "# Melihat kata-kata yang paling mirip secara semantik dengan kata tertentu\n",
    "# Ini membuktikan model sudah belajar konteks\n",
    "try:\n",
    "    print(\"\\nContoh kata yang paling mirip dengan 'polisi':\")\n",
    "    print(model_cbow.wv.most_similar('polisi', topn=5))\n",
    "\n",
    "    print(\"\\nContoh kata yang paling mirip dengan 'surabaya':\")\n",
    "    print(model_cbow.wv.most_similar('surabaya', topn=5))\n",
    "except KeyError as e:\n",
    "    print(f\"\\nKata {e} tidak ditemukan di vocabulary (mungkin karena jarang muncul).\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883b7da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 3: MEMBEDAH PROSES AGREGRASI MENJADI VEKTOR DOKUMEN ---\n",
      "Kita akan menganalisis berita pertama:\n",
      "Isi berita (token): ['kejar', 'tanjung', 'perak', 'surabaya', 'geledah', 'kantor', 'pt', 'pelindo', 'regional', 'surabaya', 'duga', 'kait', 'korupsi', 'kolam', 'labuh', 'kepala', 'kejar', 'tanjung', 'perak', 'ricky', 'setiawan', 'anas', 'geledah', 'kamis', 'wib', 'tim', 'sidik', 'kejar', 'tanjung', 'perak', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'dasar', 'tetap', 'pn', 'tipikor', 'surabaya', 'nomor', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'terang', 'kamis', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'sidik', 'pidsus', 'kejar', 'tanjung', 'perak', 'geledah', 'lokasi', 'tepat', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'dasar', 'tetap', 'geledah', 'pn', 'tipikor', 'surabaya', 'no', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'geledah', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'terang', 'geledah', 'personel', 'jaksa', 'kejar', 'tanjung', 'perak', 'pidsus', 'kejat', 'jatim', 'aman', 'tni', 'geledah', 'orang', 'jaksa', 'sidik', 'orang', 'personil', 'amc', 'kejat', 'jatim', 'orang', 'personil', 'pam', 'tni', 'imbuh', 'ricky', 'geledah', 'kait', 'sidi', 'perkara', 'duga', 'tindak', 'pidana', 'korupsi', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'pt', 'pelindo', 'sub', 'reg', 'bersamasama', 'pt', 'apbs', 'turut', 'duga', 'tipikor', 'turut', 'duga', 'tipikor', 'pt', 'apbs', 'pelindo', 'sub', 'regional', 'capai', 'ratus', 'miliar', 'rupiah', 'nilai', 'giat', 'rp', 'miliar', 'ricky', 'geledah', 'kumpul', 'bukti', 'tambah', 'kait', 'duga', 'tipikor', 'giat', 'eru', 'kolam', 'labuh', 'labuh', 'tanjung', 'perak', 'geledah', 'lokasi', 'sidik', 'laksana', 'giat', 'sita', 'kait', 'buktibukti', 'hubung', 'giat', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'laptop', 'dokumen', 'kait', 'kontrak', 'giat', 'rif']\n",
      "\n",
      "Vektor untuk 3 kata pertama dalam berita:\n",
      "  - Vektor kata 'kejar': [ 0.4710994  -0.43595427  0.07947227 -0.0173133   0.28828984]... (ditampilkan 5 dimensi pertama)\n",
      "  - Vektor kata 'tanjung': [ 0.31757742 -0.2188687  -0.20242389 -0.09981272  0.11449753]... (ditampilkan 5 dimensi pertama)\n",
      "  - Vektor kata 'perak': [ 0.45909172 -0.24538437 -0.34102324 -0.19888723  0.19445558]... (ditampilkan 5 dimensi pertama)\n",
      "\n",
      "Hasil vektor dokumen (setelah dirata-ratakan):\n",
      "[ 0.37079355 -0.19124511 -0.26222298  0.03297769  0.10756642 -0.41056207\n",
      " -0.02351834  0.14099379 -0.22156475 -0.1669604 ]... (ditampilkan 10 dimensi pertama)\n",
      "Panjang vektor: 150 dimensi (sesuai yang kita tentukan).\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Membedah Proses Agregasi Vektor Dokumen ---\n",
    "print(\"--- TAHAP 3: MEMBEDAH PROSES AGREGRASI MENJADI VEKTOR DOKUMEN ---\")\n",
    "\n",
    "def create_document_vector(doc, model, num_features):\n",
    "    word_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(num_features)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "contoh_berita = corpus[0]\n",
    "print(\"Kita akan menganalisis berita pertama:\")\n",
    "print(f\"Isi berita (token): {contoh_berita}\")\n",
    "\n",
    "print(\"\\nVektor untuk 3 kata pertama dalam berita:\")\n",
    "for i, word in enumerate(contoh_berita[:3]):\n",
    "    if word in model_cbow.wv:\n",
    "        print(f\"  - Vektor kata '{word}': {model_cbow.wv[word][:5]}... (ditampilkan 5 dimensi pertama)\")\n",
    "    else:\n",
    "        print(f\"  - Kata '{word}' tidak ada di vocabulary model.\")\n",
    "\n",
    "vektor_berita_contoh = create_document_vector(contoh_berita, model_cbow, embedding_dim)\n",
    "print(\"\\nHasil vektor dokumen (setelah dirata-ratakan):\")\n",
    "print(f\"{vektor_berita_contoh[:10]}... (ditampilkan 10 dimensi pertama)\")\n",
    "print(f\"Panjang vektor: {len(vektor_berita_contoh)} dimensi (sesuai yang kita tentukan).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0599b6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 4: MEMBUAT DATAFRAME AKHIR ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses pembuatan DataFrame selesai.\n",
      "Berikut adalah contoh hasil akhirnya:\n",
      "      dim_1     dim_2     dim_3     dim_4     dim_5     dim_6     dim_7  \\\n",
      "0  0.370794 -0.191245 -0.262223  0.032978  0.107566 -0.410562 -0.023518   \n",
      "1  0.032125  0.168098  0.160420 -0.173249  0.230247  0.023133 -0.284255   \n",
      "2  0.233250  0.017448 -0.045338 -0.042828  0.194881 -0.001756 -0.071028   \n",
      "3  0.592948  0.276753  0.109953  0.228239  0.265461 -0.637405 -0.096890   \n",
      "4  0.566952  0.365673 -0.153428  0.285578  0.186864 -0.594003  0.123903   \n",
      "\n",
      "      dim_8     dim_9    dim_10  ...   dim_142   dim_143   dim_144   dim_145  \\\n",
      "0  0.140994 -0.221565 -0.166960  ...  0.211389  0.402449  0.145739  0.083977   \n",
      "1  0.091167  0.220939 -0.220951  ... -0.056963  0.449314  0.234057  0.186703   \n",
      "2  0.085767  0.128335 -0.355567  ...  0.083654  0.558676  0.214778  0.172521   \n",
      "3  0.175559 -0.273930  0.097118  ...  0.389569  0.422502  0.437351  0.208613   \n",
      "4  0.311523 -0.384255  0.045277  ...  0.373673  0.364194  0.509090  0.301110   \n",
      "\n",
      "    dim_146   dim_147   dim_148   dim_149   dim_150  kategori  \n",
      "0 -0.093685 -0.348446 -0.018472  0.160367 -0.025801     Jatim  \n",
      "1  0.115641 -0.461611  0.108019 -0.146815 -0.463822     Jatim  \n",
      "2  0.212771 -0.454915  0.158000 -0.059311 -0.397913     Jatim  \n",
      "3 -0.301135 -0.453829  0.016859  0.260950 -0.274248     Jatim  \n",
      "4 -0.262193 -0.447688 -0.039869  0.271479 -0.106330     Jatim  \n",
      "\n",
      "[5 rows x 151 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil CBOW berhasil disimpan ke file: hasil_cbow_berita.csv ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Membuat DataFrame Akhir ---\n",
    "print(\"--- TAHAP 4: MEMBUAT DATAFRAME AKHIR ---\")\n",
    "doc_vectors = [create_document_vector(doc, model_cbow, embedding_dim) for doc in corpus]\n",
    "cbow_df = pd.DataFrame(doc_vectors, columns=[f'dim_{i+1}' for i in range(embedding_dim)])\n",
    "cbow_df['kategori'] = df['kategori'].values\n",
    "\n",
    "print(\"Proses pembuatan DataFrame selesai.\")\n",
    "print(\"Berikut adalah contoh hasil akhirnya:\")\n",
    "print(cbow_df.head())\n",
    "\n",
    "# Simpan ke file CSV\n",
    "output_file_cbow = \"hasil_cbow_berita.csv\"\n",
    "cbow_df.to_csv(output_file_cbow, index=False)\n",
    "\n",
    "print(f\"\\nHasil CBOW berhasil disimpan ke file: {output_file_cbow} ðŸš€\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}