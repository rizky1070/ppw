{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "901a6e53",
   "metadata": {},
   "source": [
    "# Pagerank Berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f562c7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai crawl dari: https://bangsaonline.com/\n",
      "Domain target: bangsaonline.com\n",
      "Batas halaman: 500\n",
      "\n",
      "[1/500] Crawling: https://bangsaonline.com/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/500] Crawling: https://bangsaonline.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/500] Crawling: https://bangsaonline.com/user/registration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Gagal (HEAD): https://bangsaonline.com/user/registration: 500 Server Error: Internal Server Error for url: https://bangsaonline.com/user/registration\n",
      "[4/500] Crawling: https://bangsaonline.com/feed/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> [SKIP] Mengabaikan tipe konten non-HTML: application/xml\n",
      "[5/500] Crawling: https://bangsaonline.com/live\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/500] Crawling: https://bangsaonline.com/kanal/jawa-timur\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/500] Crawling: https://bangsaonline.com/kanal/jatim-metro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/500] Crawling: https://bangsaonline.com/kanal/jatim-tengah\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/500] Crawling: https://bangsaonline.com/kanal/jatim-utara\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/500] Crawling: https://bangsaonline.com/kanal/jatim-selatan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/500] Crawling: https://bangsaonline.com/kanal/jatim-timur\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/500] Crawling: https://bangsaonline.com/kanal/jatim-barat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/500] Crawling: https://bangsaonline.com/kanal/jatim-madura\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/500] Crawling: https://bangsaonline.com/kanal/nasional\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/500] Crawling: https://bangsaonline.com/kanal/nusantara\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/500] Crawling: https://bangsaonline.com/kanal/politik\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/500] Crawling: https://bangsaonline.com/kanal/birokrasi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/500] Crawling: https://bangsaonline.com/kanal/religia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/500] Crawling: https://bangsaonline.com/kanal/tj-islam-sehari-hari\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/500] Crawling: https://bangsaonline.com/kanal/tafsir-al-quran-aktual\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/500] Crawling: https://bangsaonline.com/kanal/pesantren\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/500] Crawling: https://bangsaonline.com/kanal/ormas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/500] Crawling: https://bangsaonline.com/kanal/hukum-dan-kriminal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/500] Crawling: https://bangsaonline.com/kanal/ekonomi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/500] Crawling: https://bangsaonline.com/kanal/entrepreneur\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/500] Crawling: https://bangsaonline.com/kanal/entertain-sport\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/500] Crawling: https://bangsaonline.com/kanal/selebriti\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque  # Antrian (queue) yang efisien\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "def build_pagerank_dataset(start_url, max_pages=50, delay=0.1):\n",
    "    \"\"\"\n",
    "    Melakukan crawling multi-halaman untuk membangun edge list untuk PageRank.\n",
    "    \"\"\"\n",
    "    \n",
    "    pages_to_visit = deque([start_url])\n",
    "    visited_pages = set()\n",
    "    edge_list = []\n",
    "    \n",
    "    try:\n",
    "        hostname = urlparse(start_url).hostname\n",
    "        if not hostname:\n",
    "            raise ValueError(\"Hostname tidak ditemukan. Pastikan URL diawali 'https://' atau 'http://'\")\n",
    "        base_hostname = hostname.replace('www.', '')\n",
    "    except (ValueError, AttributeError) as e:\n",
    "        print(f\"Error: URL awal tidak valid -> '{start_url}'\")\n",
    "        print(f\"Detail: {e}\")\n",
    "        return pd.DataFrame(columns=[\"Halaman Sumber\", \"Link Keluar (Internal)\"]) \n",
    "\n",
    "    print(f\"Memulai crawl dari: {start_url}\")\n",
    "    print(f\"Domain target: {base_hostname}\")\n",
    "    print(f\"Batas halaman: {max_pages}\\n\")\n",
    "\n",
    "    IGNORED_EXTENSIONS = (\n",
    "        '.png', '.jpg', '.jpeg', '.gif', '.svg', '.bmp', '.tiff', '.webp',\n",
    "        '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',\n",
    "        '.zip', '.rar', '.gz', '.tar', '7z',\n",
    "        '.mp4', '.mkv', '.avi', '.mov', '.mp3', '.wav', '.ogg',\n",
    "        '.css', '.js', '.xml', '.json', '.csv'\n",
    "    )\n",
    "\n",
    "    # Kita akan mencari segmen path 'iklan'\n",
    "    IGNORED_PATH_SEGMENTS = ('iklan',)\n",
    "\n",
    "    while pages_to_visit and len(visited_pages) < max_pages:\n",
    "        current_url = pages_to_visit.popleft()\n",
    "        \n",
    "        if current_url in visited_pages:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            parsed_current_url = urlparse(current_url) # Parse sekali saja\n",
    "            cleaned_url_path = parsed_current_url.path\n",
    "        except ValueError:\n",
    "            print(f\"  -> [SKIP] URL tidak valid: {current_url}\")\n",
    "            continue \n",
    "\n",
    "        if cleaned_url_path.lower().endswith(IGNORED_EXTENSIONS):\n",
    "            print(f\"  -> [SKIP] Mengabaikan file (dari ekstensi): {current_url}\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # Pecah path menjadi segmen: '/berita/iklan/123' -> ['', 'berita', 'iklan', '123']\n",
    "        current_path_segments = set(cleaned_url_path.split('/')) \n",
    "        \n",
    "        # Cek apakah ada irisan (intersection) antara segmen path dan segmen yang diabaikan\n",
    "        if not current_path_segments.isdisjoint(IGNORED_PATH_SEGMENTS):\n",
    "            print(f\"  -> [SKIP] Mengabaikan path terlarang: {current_url}\")\n",
    "            continue\n",
    "       \n",
    "            \n",
    "        visited_pages.add(current_url)\n",
    "        print(f\"[{len(visited_pages)}/{max_pages}] Crawling: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            try:\n",
    "                head_response = requests.head(current_url, timeout=3, allow_redirects=True)\n",
    "                head_response.raise_for_status() \n",
    "                content_type = head_response.headers.get('Content-Type', '')\n",
    "                \n",
    "                if 'text/html' not in content_type:\n",
    "                    print(f\"  -> [SKIP] Mengabaikan tipe konten non-HTML: {content_type}\")\n",
    "                    continue\n",
    "            except requests.exceptions.RequestException as head_err:\n",
    "                if 'head_response' in locals() and head_response.status_code >= 400:\n",
    "                        print(f\"  -> Gagal (HEAD): {current_url}: {head_err}\")\n",
    "                        continue\n",
    "                pass\n",
    "\n",
    "            response = requests.get(current_url, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            final_content_type = response.headers.get('Content-Type', '')\n",
    "            if 'text/html' not in final_content_type:\n",
    "                    print(f\"  -> [SKIP] Mengabaikan tipe konten non-HTML (setelah GET): {final_content_type}\")\n",
    "                    continue\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            \n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                \n",
    "                if href.startswith(('#', 'javascript:', 'mailto:', 'tel:')) or not href.strip():\n",
    "                    continue\n",
    "                \n",
    "                absolute_url = urljoin(current_url, href)\n",
    "                absolute_url = absolute_url.split('#')[0] # Hapus fragment\n",
    "\n",
    "                try:\n",
    "                    parsed_absolute_url = urlparse(absolute_url)\n",
    "                    \n",
    "                    \n",
    "                    absolute_url_path = parsed_absolute_url.path\n",
    "                    found_path_segments = set(absolute_url_path.split('/'))\n",
    "                    \n",
    "                    if not found_path_segments.isdisjoint(IGNORED_PATH_SEGMENTS):\n",
    "                        # Jangan tambahkan ke antrian jika path-nya diabaikan\n",
    "                        continue\n",
    "                    \n",
    "\n",
    "                    link_hostname = parsed_absolute_url.hostname\n",
    "                    if link_hostname and link_hostname.endswith(base_hostname):\n",
    "                        edge_list.append([current_url, absolute_url])\n",
    "                        if absolute_url not in visited_pages:\n",
    "                            pages_to_visit.append(absolute_url)\n",
    "                except ValueError:\n",
    "                    # Jika URL yang ditemukan tidak valid\n",
    "                    continue\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  -> Gagal mengakses (GET) {current_url}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nCrawl selesai. Total {len(visited_pages)} halaman di-visit.\")\n",
    "    print(f\"Total {len(edge_list)} link (edge) ditemukan.\")\n",
    "    \n",
    "    if not edge_list:\n",
    "        print(\"Tidak ada edge yang ditemukan. DataFrame akan kosong.\")\n",
    "        return pd.DataFrame(columns=[\"Halaman Sumber\", \"Link Keluar (Internal)\"])\n",
    "    \n",
    "    df = pd.DataFrame(edge_list, columns=[\"Halaman Sumber\", \"Link Keluar (Internal)\"])\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Dataset akhir memiliki {len(df)} edge.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Bagian Utama untuk Menjalankan Kode ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    target_url = \"https://bangsaonline.com/\"\n",
    "    batas_halaman = 500\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    pagerank_df = build_pagerank_dataset(target_url, max_pages=batas_halaman, delay=0.1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    if not pagerank_df.empty:\n",
    "        print(\"\\n--- Contoh Hasil Dataset (Head) ---\")\n",
    "        print(pagerank_df.head(10))\n",
    "        \n",
    "        output_filename = \"pagerank500_edges_berita_new.csv\"\n",
    "        pagerank_df.to_csv(output_filename, index=False)\n",
    "        print(f\"\\nFile berhasil disimpan ke {output_filename}\")\n",
    "    else:\n",
    "        print(\"\\nDataset akhir kosong, tidak ada file CSV yang disimpan.\")\n",
    "\n",
    "    print(f\"\\n--- Selesai ---\")\n",
    "    minutes = int(duration // 60)\n",
    "    seconds = int(duration % 60)\n",
    "    print(f\"Total waktu eksekusi: {minutes} menit {seconds} detik ({duration:.2f} detik)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}