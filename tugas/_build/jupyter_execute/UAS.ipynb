{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0J_sTmAkn1I",
    "outputId": "755f22a9-6cfa-4f58-d63e-bbfc4467a8dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf4llm in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: nltk in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: networkx in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (3.4.2)\n",
      "Requirement already satisfied: matplotlib in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: numpy in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pymupdf>=1.26.6 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pymupdf4llm) (1.26.7)\n",
      "Requirement already satisfied: tabulate in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pymupdf4llm) (0.9.0)\n",
      "Requirement already satisfied: click in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rizky\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rizky\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rizky\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rizky\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\laragon\\bin\\python\\python-3.10\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf4llm nltk networkx matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E58sCivHBUY-",
    "outputId": "3657e892-8f92-4663-a73a-a1ec5d2fce88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rizky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rizky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Rizky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymupdf4llm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Konfigurasi Awal ---\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt_tab')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def main(pdf_path):\n",
    "    # ==========================================\n",
    "    # TAHAP 1: Ekstrak Text\n",
    "    # ==========================================\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"TAHAP 1: EKSTRAKSI TEKS\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    text_content = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Gunakan dummy text jika PDF tidak terbaca/kosong\n",
    "    if len(text_content) < 50:\n",
    "        text_content = \"\"\"\n",
    "        Nasi goreng is a delicious Indonesian fried rice dish.\n",
    "        It is distinguished from other Asian fried rice recipes by its aromatic, earthy, and smoky flavor.\n",
    "        The dish is typically spiced with garlic, shallot, and chili.\n",
    "        Visit https://food.com for recipes. DOIs: https://doi.org/10.1016/j.food\n",
    "        \"\"\"\n",
    "        print(\"(Menggunakan teks dummy internal)\")\n",
    "\n",
    "    print(f\"Jumlah Karakter: {len(text_content)}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # TAHAP 2: Preprocessing & Tokenisasi (Output Detail)\n",
    "    # ==========================================\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"TAHAP 2: PREPROCESSING (CONTOH 3 TAHAP)\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    sentences = sent_tokenize(text_content)\n",
    "    print(f\"Jumlah Kalimat Total: {len(sentences)}\\n\")\n",
    "\n",
    "    # --- BAGIAN INI DIMODIFIKASI UNTUK MENAMPILKAN CONTOH ---\n",
    "\n",
    "    # Kita panggil fungsi khusus yang mengembalikan contoh & data\n",
    "    sequences, examples = preprocess_text_with_examples(sentences)\n",
    "\n",
    "    # Tampilkan Contoh\n",
    "    print(\"--- A. Contoh 3 Kalimat SEBELUM Preprocessing (Asli) ---\")\n",
    "    for i, s in enumerate(examples['raw']):\n",
    "        print(f\"{i+1}. {s}\")\n",
    "\n",
    "    print(\"\\n--- B. Contoh 3 Kalimat SETELAH Preprocessing (Cleaning) ---\")\n",
    "    for i, s in enumerate(examples['cleaned']):\n",
    "        print(f\"{i+1}. {s}\")\n",
    "\n",
    "    print(\"\\n--- C. Contoh 3 Kalimat SETELAH Tokenisasi (Unigram + Bigram) ---\")\n",
    "    for i, s in enumerate(examples['tokenized']):\n",
    "        # Tampilkan sebagian jika terlalu panjang\n",
    "        display_s = s[:10] + ['...'] if len(s) > 10 else s\n",
    "        print(f\"{i+1}. {display_s}\")\n",
    "\n",
    "    print(f\"\\nJumlah Sequence: {len(sequences)}\")\n",
    "    total_tokens = sum(len(s) for s in sequences)\n",
    "    print(f\"\\nTotal Token Siap Proses: {total_tokens}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # TAHAP 3: Membangun Graph\n",
    "    # ==========================================\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"TAHAP 3: MEMBANGUN GRAPH\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    G = build_graph(sequences, window_size=3)\n",
    "    print(f\"Graph Terbentuk: {G.number_of_nodes()} Node, {G.number_of_edges()} Edge\")\n",
    "\n",
    "    # ==========================================\n",
    "    # TAHAP 4: Perbandingan Score\n",
    "    # ==========================================\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"TAHAP 4: PERBANDINGAN SCORE 4 METODE\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    pagerank = nx.pagerank(G, weight='weight')\n",
    "    degree = nx.degree_centrality(G)\n",
    "    betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    closeness = nx.closeness_centrality(G)\n",
    "\n",
    "    top_n = 20\n",
    "    list_pr = get_top_list(pagerank, top_n)\n",
    "    list_deg = get_top_list(degree, top_n)\n",
    "    list_bet = get_top_list(betweenness, top_n)\n",
    "    list_clo = get_top_list(closeness, top_n)\n",
    "\n",
    "    def fmt_cell(item):\n",
    "        return f\"{item[0]} ({item[1]:.4f})\"\n",
    "\n",
    "    comparison_data = {\n",
    "        'Rank': range(1, top_n + 1),\n",
    "        'PageRank': [fmt_cell(x) for x in list_pr],\n",
    "        'Degree': [fmt_cell(x) for x in list_deg],\n",
    "        'Betweenness': [fmt_cell(x) for x in list_bet],\n",
    "        'Closeness': [fmt_cell(x) for x in list_clo]\n",
    "    }\n",
    "\n",
    "    df_compare = pd.DataFrame(comparison_data)\n",
    "    df_compare.set_index('Rank', inplace=True)\n",
    "\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', 30)\n",
    "    pd.set_option('display.width', 1000)\n",
    "\n",
    "    print(df_compare)\n",
    "\n",
    "    # ==========================================\n",
    "    # TAHAP 5: Consensus Result\n",
    "    # ==========================================\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"TAHAP 5: HASIL FINAL (CONSENSUS SCORE)\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    final_ranking = calculate_consensus(\n",
    "        [x[0] for x in list_pr],\n",
    "        [x[0] for x in list_deg],\n",
    "        [x[0] for x in list_bet],\n",
    "        [x[0] for x in list_clo]\n",
    "    )\n",
    "\n",
    "    df_final = pd.DataFrame(final_ranking, columns=['Keyword', 'Total Score'])\n",
    "    df_final.index += 1\n",
    "    print(df_final)\n",
    "\n",
    "    # ==========================================\n",
    "    # TAHAP 6: Visualisasi\n",
    "    # ==========================================\n",
    "    top_20_words_list = [item[0] for item in final_ranking]\n",
    "    # Memanggil visualisasi TANPA filter top_keywords (None) -> Akan memplot SEMUA\n",
    "    visualize_graph(G, top_keywords=top_20_words_list)\n",
    "    visualize_graph(G, top_keywords=None)\n",
    "\n",
    "# --- FUNGSI PENDUKUNG ---\n",
    "\n",
    "def extract_text_from_pdf(filepath):\n",
    "    try:\n",
    "        doc = fitz.open(filepath)\n",
    "        text = pymupdf4llm.to_markdown(doc)\n",
    "        doc.close()\n",
    "        return text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text_with_examples(sentences_list):\n",
    "    \"\"\"\n",
    "    Fungsi ini dimodifikasi untuk mengembalikan sequences DAN contoh kalimat\n",
    "    di setiap tahapannya.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('indonesian') + stopwords.words('english'))\n",
    "    additional_junk = {'doi', 'http', 'https', 'www', 'com', 'org', 'journal', 'vol', 'issue', 'page'}\n",
    "    stop_words.update(additional_junk)\n",
    "\n",
    "    processed_sequences = []\n",
    "\n",
    "    # Tempat menyimpan contoh (maksimal 3)\n",
    "    examples = {\n",
    "        'raw': [],\n",
    "        'cleaned': [],\n",
    "        'tokenized': []\n",
    "    }\n",
    "\n",
    "    example_count = 0\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        # 1. Simpan Contoh RAW (Asli)\n",
    "        original_sentence = sentence.strip()\n",
    "        if not original_sentence: continue\n",
    "\n",
    "        # --- TAHAP PEMBERSIHAN (CLEANING) ---\n",
    "        clean_sentence = re.sub(r'http\\S+', '', sentence)\n",
    "        clean_sentence = re.sub(r'www\\S+', '', clean_sentence)\n",
    "        clean_sentence = re.sub(r'\\S*doi\\S*', '', clean_sentence, flags=re.IGNORECASE)\n",
    "        clean_sentence = re.sub(r'\\d+', '', clean_sentence)\n",
    "        clean_sentence = re.sub(r'[^a-zA-Z\\s]', '', clean_sentence).lower()\n",
    "        clean_sentence = clean_sentence.strip()\n",
    "\n",
    "        # 2. Simpan Contoh CLEANED\n",
    "        # (Hanya jika belum mencapai 3 contoh)\n",
    "\n",
    "        # --- TAHAP TOKENISASI ---\n",
    "        tokens = word_tokenize(clean_sentence)\n",
    "        filtered = []\n",
    "        for w in tokens:\n",
    "            if w not in stop_words and len(w) > 2 and len(w) < 20:\n",
    "                filtered.append(w)\n",
    "\n",
    "        if not filtered: continue\n",
    "\n",
    "        interleaved = []\n",
    "        for i in range(len(filtered)):\n",
    "            interleaved.append(filtered[i])\n",
    "            if i < len(filtered) - 1:\n",
    "                interleaved.append(f\"{filtered[i]} {filtered[i+1]}\")\n",
    "\n",
    "        processed_sequences.append(interleaved)\n",
    "\n",
    "        # 3. Simpan Contoh TOKENIZED & Update Counter\n",
    "        if example_count < 3:\n",
    "            examples['raw'].append(original_sentence)\n",
    "            examples['cleaned'].append(clean_sentence)\n",
    "            examples['tokenized'].append(interleaved)\n",
    "            example_count += 1\n",
    "\n",
    "    return processed_sequences, examples\n",
    "\n",
    "def build_graph(sequences, window_size=3):\n",
    "    d = defaultdict(int)\n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq)):\n",
    "            for j in range(1, window_size + 1):\n",
    "                if i + j < len(seq):\n",
    "                    u, v = seq[i], seq[i+j]\n",
    "                    if u == v: continue\n",
    "                    d[tuple(sorted((u, v)))] += 1\n",
    "    G = nx.Graph()\n",
    "    for (u, v), w in d.items():\n",
    "        G.add_edge(u, v, weight=w)\n",
    "    return G\n",
    "\n",
    "def get_top_list(metric_dict, n):\n",
    "    return sorted(metric_dict.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "def calculate_consensus(l1, l2, l3, l4):\n",
    "    scores = defaultdict(int)\n",
    "    lists = [l1, l2, l3, l4]\n",
    "    for lst in lists:\n",
    "        for rank, word in enumerate(lst):\n",
    "            scores[word] += (20 - rank)\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "def visualize_graph(G, top_keywords=None):\n",
    "    # Setting Ukuran Canvas Besar untuk menampung seluruh kata\n",
    "    plt.figure(figsize=(25, 25))\n",
    "\n",
    "    if top_keywords:\n",
    "        H = G.subgraph(top_keywords)\n",
    "        title_text = \"Top Keywords Graph\"\n",
    "        k_val = 1.0\n",
    "        font_s = 10\n",
    "        edge_alpha = 0.5\n",
    "    else:\n",
    "        # VISUALISASI SELURUH GRAPH\n",
    "        H = G\n",
    "        title_text = f\"Full Word Graph ({G.number_of_nodes()} words)\"\n",
    "        # k lebih kecil agar node menyebar lebih luas, tapi tetap bergerombol\n",
    "        k_val = 0.3\n",
    "        font_s = 7 # Font kecil agar tidak saling menimpa parah\n",
    "        edge_alpha = 0.2 # Garis tipis agar tidak ruwet\n",
    "\n",
    "    if H.number_of_nodes() > 0:\n",
    "        print(f\"Sedang menggambar {H.number_of_nodes()} node... Mohon tunggu.\")\n",
    "        pos = nx.spring_layout(H, k=k_val, iterations=50, seed=42)\n",
    "\n",
    "        d = dict(H.degree)\n",
    "        # Ukuran node disesuaikan: yang penting besar, yang tidak penting kecil\n",
    "        # Menggunakan log scale atau pembagian agar node raksasa tidak menutupi semua\n",
    "        node_sizes = [v * 10 + 50 for v in d.values()]\n",
    "\n",
    "        nx.draw_networkx_nodes(H, pos, node_size=node_sizes, node_color='lightgreen', alpha=0.8)\n",
    "        nx.draw_networkx_edges(H, pos, width=0.5, alpha=edge_alpha, edge_color='gray')\n",
    "        nx.draw_networkx_labels(H, pos, font_size=font_s)\n",
    "\n",
    "        plt.title(title_text, fontsize=20)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Graph kosong.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "74c4fa68",
    "outputId": "7a2a3a7f-7fb2-4547-9975-803558a3c0a3"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Hapus file PDF sebelumnya jika ada untuk menghindari konflik nama\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Hapus file PDF sebelumnya jika ada untuk menghindari konflik nama\n",
    "!rm -f *.pdf\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    print(f'Uploaded file: {filename}')\n",
    "    # Pastikan hanya satu file yang diproses untuk demonstrasi ini\n",
    "    # Anda bisa memodifikasi ini jika ingin memproses beberapa file\n",
    "    pdf_file_path = filename\n",
    "    print(f'Processing {pdf_file_path}...')\n",
    "    main(pdf_file_path)\n",
    "    break # Hanya proses file pertama yang diupload\n",
    "\n",
    "if not uploaded:\n",
    "    print(\"No file uploaded. Please upload a PDF file to proceed.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}