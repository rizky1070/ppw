{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f0a5a8",
   "metadata": {},
   "source": [
    "# Crawling Berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fafde603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mencari daftar kategori dan sub-kategori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gagal mengambil kategori: HTTPSConnectionPool(host='www.cnnindonesia.com', port=443): Read timed out. (read timeout=10)\n",
      "Ditemukan 0 sub-kategori unik.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import concurrent.futures\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# --- PENGATURAN ---\n",
    "MAX_WORKERS = 10\n",
    "MAX_ARTICLES_PER_SUBCATEGORY = 25\n",
    "BASE_URL = \"https://www.cnnindonesia.com\"\n",
    "\n",
    "def get_category_links():\n",
    "    \"\"\"Tahap 1: Mengambil semua link kategori dan sub-kategori dari navigasi utama.\"\"\"\n",
    "    print(\"Mencari daftar kategori dan sub-kategori...\")\n",
    "    categories = []\n",
    "    try:\n",
    "        response = requests.get(BASE_URL, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        nav_items = soup.select(\"nav > ul > li.header-nhl\")\n",
    "        if not nav_items:\n",
    "            nav_items = soup.select(\"nav > ul > li\")\n",
    "\n",
    "        for nav_item in nav_items:\n",
    "            main_category_tag = nav_item.find('a', recursive=False)\n",
    "            if not main_category_tag or not main_category_tag.has_attr('href'):\n",
    "                continue\n",
    "            \n",
    "            main_category_name = main_category_tag.text.strip()\n",
    "            \n",
    "            sub_menu_div = nav_item.find('div', class_='navbar__item-dropdown')\n",
    "            if sub_menu_div:\n",
    "                sub_category_links = sub_menu_div.find_all('a')\n",
    "                for sub_link in sub_category_links:\n",
    "                    if sub_link.has_attr('href'):\n",
    "                        categories.append({\n",
    "                            'kategori': main_category_name,\n",
    "                            'sub_kategori': sub_link.text.strip(),\n",
    "                            'url': urljoin(BASE_URL, sub_link['href'])\n",
    "                        })\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengambil kategori: {e}\")\n",
    "    \n",
    "    unique_categories = [dict(t) for t in {tuple(d.items()) for d in categories if d['sub_kategori']}]\n",
    "    print(f\"Ditemukan {len(unique_categories)} sub-kategori unik.\\n\")\n",
    "    return unique_categories\n",
    "\n",
    "def get_article_info_from_subcategory(subcategory):\n",
    "    \"\"\"\n",
    "    Tahap 2: Mengambil info artikel (URL + Kategori) dari satu halaman sub-kategori.\n",
    "    MODIFIED: Now returns a list of dictionaries, not just URLs.\n",
    "    \"\"\"\n",
    "    articles_to_scrape = []\n",
    "    page = 1\n",
    "    print(f\"--> Mengambil artikel dari '{subcategory['kategori']} - {subcategory['sub_kategori']}'...\")\n",
    "    while len(articles_to_scrape) < MAX_ARTICLES_PER_SUBCATEGORY:\n",
    "        try:\n",
    "            paginated_url = f\"{subcategory['url']}/page/{page}\"\n",
    "            response = requests.get(paginated_url, timeout=10)\n",
    "            if response.status_code != 200: break\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            articles_on_page = soup.select(\"article a\")\n",
    "            \n",
    "            if not articles_on_page: break\n",
    "            \n",
    "            for link in articles_on_page:\n",
    "                if link.has_attr('href'):\n",
    "                    # Simpan URL bersama dengan info kategori dan sub-kategorinya\n",
    "                    articles_to_scrape.append({\n",
    "                        'url': link['href'],\n",
    "                        'kategori': subcategory['kategori'],\n",
    "                        'sub_kategori': subcategory['sub_kategori']\n",
    "                    })\n",
    "                    if len(articles_to_scrape) >= MAX_ARTICLES_PER_SUBCATEGORY:\n",
    "                        break\n",
    "            page += 1\n",
    "            time.sleep(0.2)\n",
    "        except requests.exceptions.RequestException:\n",
    "            break\n",
    "    return articles_to_scrape\n",
    "\n",
    "def scrape_article_detail(article_info):\n",
    "    \"\"\"\n",
    "    Tahap 3: Mengekstrak detail dari satu artikel.\n",
    "    MODIFIED: Now accepts a dictionary 'article_info' to access the passed-down category names.\n",
    "    \"\"\"\n",
    "    article_url = article_info['url']\n",
    "    try:\n",
    "        response = requests.get(article_url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        id_match = re.search(r'-(\\d+)$', article_url.split('/')[-1])\n",
    "        id_berita = id_match.group(1) if id_match else None\n",
    "\n",
    "        judul_berita = soup.select_one(\"h1.text-xl\").text.strip()\n",
    "\n",
    "        content_element = soup.select_one(\"div.detail-text\")\n",
    "        isi_berita = \"Isi berita tidak ditemukan\"\n",
    "        if content_element:\n",
    "            for unwanted in content_element.select(\"div, table, blockquote\"):\n",
    "                unwanted.decompose()\n",
    "            isi_berita_mentah = content_element.get_text(separator=\" \", strip=True)\n",
    "            isi_berita = re.sub(r'\\s+', ' ', isi_berita_mentah).strip()\n",
    "\n",
    "        # Gabungkan data yang di-scrape dengan data kategori yang sudah ada\n",
    "        return {\n",
    "            \"id_berita\": id_berita,\n",
    "            \"kategori\": article_info['kategori'],\n",
    "            \"sub_kategori\": article_info['sub_kategori'],\n",
    "            \"judul_berita\": judul_berita,\n",
    "            \"isi_berita\": isi_berita,\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fungsi utama untuk mengorkestrasi semua tahapan scraping.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    subcategories = get_category_links()\n",
    "    if not subcategories: return\n",
    "    \n",
    "    all_articles_to_scrape = []\n",
    "    for subcat in subcategories:\n",
    "        # Nama fungsi diubah agar lebih jelas\n",
    "        info_list = get_article_info_from_subcategory(subcat)\n",
    "        all_articles_to_scrape.extend(info_list)\n",
    "        print(f\"    Terkumpul {len(info_list)} artikel.\")\n",
    "    \n",
    "    print(f\"\\nTotal artikel yang akan di-scrape: {len(all_articles_to_scrape)}\\n\")\n",
    "    \n",
    "    final_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Kirim list of dictionaries ke fungsi scrape_article_detail\n",
    "        results = executor.map(scrape_article_detail, all_articles_to_scrape)\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            if result:\n",
    "                final_results.append(result)\n",
    "                print(f\"✔️ Selesai: Artikel ke-{i+1} | {result['judul_berita'][:50]}...\")\n",
    "\n",
    "    df = pd.DataFrame(final_results)\n",
    "    if not df.empty:\n",
    "        # Tambahkan kolom baru ke urutan\n",
    "        df = df[['id_berita', 'kategori', 'sub_kategori', 'judul_berita', 'isi_berita']]\n",
    "    \n",
    "    output_filename = \"berita_cnn_indonesia_lengkap.csv\"\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"\\n\\n✅ Proses scraping selesai.\")\n",
    "    print(f\"Total data yang berhasil diambil: {len(df)} baris.\")\n",
    "    print(f\"File disimpan sebagai: {output_filename}\")\n",
    "    print(f\"Total waktu eksekusi: {end_time - start_time:.2f} detik.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_hasil_cnn = main()\n",
    "    if df_hasil_cnn is not None and not df_hasil_cnn.empty:\n",
    "        print(\"\\nPratinjau Hasil Data:\")\n",
    "        print(df_hasil_cnn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93362122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mencari daftar kategori dan sub-kategori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ditemukan 0 sub-kategori unik.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Impor library yang dibutuhkan\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import concurrent.futures\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# --- PENGATURAN GLOBAL ---\n",
    "# Aturan ini bisa diubah sesuai kebutuhan\n",
    "MAX_WORKERS = 10  # Jumlah \"pekerja\" paralel untuk mempercepat proses\n",
    "MAX_ARTICLES_PER_SUBCATEGORY = 50 # Batas artikel per sub-kategori agar tidak terlalu lama\n",
    "BASE_URL = \"https://www.cnnindonesia.com\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "\n",
    "def get_category_links():\n",
    "    \"\"\"TAHAP 1: DISCOVERY\n",
    "    Menemukan semua 'alamat' sub-kategori dari menu navigasi utama.\n",
    "    Ini adalah langkah perencanaan untuk mengetahui area mana saja yang akan di-scrape.\"\"\"\n",
    "    print(\"Mencari daftar kategori dan sub-kategori...\")\n",
    "    categories = []\n",
    "    try:\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Selektor untuk menemukan setiap item di menu navigasi\n",
    "        nav_items = soup.select(\"nav > ul > li.header-nhl_dropdown-item\")\n",
    "        \n",
    "        for nav_item in nav_items:\n",
    "            main_category_tag = nav_item.find('a', recursive=False)\n",
    "            if not main_category_tag: continue\n",
    "            \n",
    "            main_category_name = main_category_tag.text.strip()\n",
    "            \n",
    "            # Selektor untuk menemukan 'kotak' dropdown berisi sub-kategori\n",
    "            sub_menu_div = nav_item.find('div', class_='navbar_item-dropdown')\n",
    "            if sub_menu_div:\n",
    "                for sub_link in sub_menu_div.find_all('a'):\n",
    "                    if sub_link.has_attr('href'):\n",
    "                        categories.append({\n",
    "                            'kategori': main_category_name,\n",
    "                            'sub_kategori': sub_link.text.strip(),\n",
    "                            'url': urljoin(BASE_URL, sub_link['href'])\n",
    "                        })\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal mengambil kategori: {e}\")\n",
    "    \n",
    "    unique_categories = [dict(t) for t in {tuple(d.items()) for d in categories if d['sub_kategori']}]\n",
    "    print(f\"Ditemukan {len(unique_categories)} sub-kategori unik.\\n\")\n",
    "    return unique_categories\n",
    "\n",
    "def get_article_info_from_subcategory(subcategory):\n",
    "    \"\"\"TAHAP 2: COLLECTION\n",
    "    Mengunjungi setiap 'alamat' sub-kategori dan mengumpulkan semua link artikel di dalamnya,\n",
    "    lengkap dengan info kategori agar tidak hilang.\"\"\"\n",
    "    articles_to_scrape = []\n",
    "    page = 1\n",
    "    print(f\"--> Mengambil artikel dari '{subcategory['kategori']} - {subcategory['sub_kategori']}'...\")\n",
    "    while len(articles_to_scrape) < MAX_ARTICLES_PER_SUBCATEGORY:\n",
    "        try:\n",
    "            paginated_url = f\"{subcategory['url']}/page/{page}\"\n",
    "            response = requests.get(paginated_url, headers=HEADERS, timeout=10)\n",
    "            if response.status_code != 200: break\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            # Selektor untuk menemukan link artikel di halaman kategori\n",
    "            articles_on_page = soup.select(\"article a\")\n",
    "            \n",
    "            if not articles_on_page: break\n",
    "            \n",
    "            for link in articles_on_page:\n",
    "                if link.has_attr('href'):\n",
    "                    articles_to_scrape.append({\n",
    "                        'url': link['href'],\n",
    "                        'kategori': subcategory['kategori'],\n",
    "                        'sub_kategori': subcategory['sub_kategori']\n",
    "                    })\n",
    "                    if len(articles_to_scrape) >= MAX_ARTICLES_PER_SUBCATEGORY: break\n",
    "            page += 1\n",
    "            time.sleep(0.2)\n",
    "        except Exception:\n",
    "            break\n",
    "    return articles_to_scrape\n",
    "\n",
    "def scrape_article_detail(article_info):\n",
    "    \"\"\"TAHAP 3: EXTRACTION\n",
    "    Mengunjungi satu per satu link artikel dan 'mencomot' data spesifik\n",
    "    seperti ID, judul (yang akurat), dan isi beritanya.\"\"\"\n",
    "    article_url = article_info['url']\n",
    "    try:\n",
    "        response = requests.get(article_url, headers=HEADERS, timeout=15)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Ekstrak ID unik dari URL\n",
    "        id_match = re.search(r'-(\\d+)$', article_url.split('/')[-1])\n",
    "        id_berita = id_match.group(1) if id_match else None\n",
    "\n",
    "        # Ekstrak judul dari tag H1 untuk akurasi maksimal\n",
    "        judul_berita = soup.select_one(\"h1.text-xl\").text.strip()\n",
    "\n",
    "        # Ekstrak semua paragraf dari container isi berita\n",
    "        content_element = soup.select_one(\"div.detail-text\")\n",
    "        isi_berita = \"Isi berita tidak ditemukan\"\n",
    "        if content_element:\n",
    "            paragraf = [p.get_text(strip=True) for p in content_element.select(\"p\")]\n",
    "            isi_berita = \" \".join(paragraf)\n",
    "\n",
    "        return {\n",
    "            \"id_berita\": id_berita,\n",
    "            \"kategori\": article_info['kategori'],\n",
    "            \"sub_kategori\": article_info['sub_kategori'],\n",
    "            \"judul_berita\": judul_berita,\n",
    "            \"isi_berita\": isi_berita,\n",
    "            \"link\": article_url\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"FUNGSI UTAMA: ORKESTRATOR\n",
    "    Mengatur alur kerja dari awal hingga akhir, dari perencanaan hingga penyimpanan hasil.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Menjalankan Tahap 1\n",
    "    subcategories = get_category_links()\n",
    "    if not subcategories: return\n",
    "    \n",
    "    # Menjalankan Tahap 2\n",
    "    all_articles_to_scrape = []\n",
    "    for subcat in subcategories:\n",
    "        info_list = get_article_info_from_subcategory(subcat)\n",
    "        all_articles_to_scrape.extend(info_list)\n",
    "        print(f\"    Terkumpul {len(info_list)} artikel.\")\n",
    "    \n",
    "    print(f\"\\nTotal artikel yang akan di-scrape: {len(all_articles_to_scrape)}\\n\")\n",
    "    \n",
    "    # Menjalankan Tahap 3 secara paralel untuk kecepatan\n",
    "    final_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        results = executor.map(scrape_article_detail, all_articles_to_scrape)\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            if result and result['isi_berita'] and len(result['isi_berita']) > 50: # Filter berita tanpa isi\n",
    "                final_results.append(result)\n",
    "                print(f\"✔️ Selesai: Artikel ke-{i+1} | {result['judul_berita'][:50]}...\")\n",
    "\n",
    "    # Tahap Akhir: Menyimpan hasil ke dalam file\n",
    "    df = pd.DataFrame(final_results)\n",
    "    if not df.empty:\n",
    "        df = df[['id_berita', 'kategori', 'sub_kategori', 'judul_berita', 'isi_berita', 'link']]\n",
    "    \n",
    "    output_filename = \"berita_cnn_indonesia_final.csv\"\n",
    "    df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"\\n\\n✅ Proses scraping selesai.\")\n",
    "    print(f\"Total data yang berhasil diambil: {len(df)} baris.\")\n",
    "    print(f\"File disimpan sebagai: {output_filename}\")\n",
    "    print(f\"Total waktu eksekusi: {end_time - start_time:.2f} detik.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Titik awal program dijalankan\n",
    "if __name__ == \"__main__\":\n",
    "    df_hasil_cnn = main()\n",
    "    if df_hasil_cnn is not None and not df_hasil_cnn.empty:\n",
    "        print(\"\\nPratinjau Hasil Data:\")\n",
    "        # Menampilkan hasil dengan lebar kolom lebih besar agar mudah dibaca\n",
    "        pd.set_option('display.max_colwidth', 80)\n",
    "        print(df_hasil_cnn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024ce026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import uuid # Meskipun tidak digunakan untuk ID dari web, bisa untuk ID unik jika diperlukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d17af14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\urllib3\\connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\http\\client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\requests\\adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\urllib3\\connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\http\\client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36 Edg/140.0.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Ambil halaman utama\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mC:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\requests\\adapters.py:659\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    645\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    646\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    655\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    656\u001b[0m     )\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    663\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "# URL homepage CNN Indonesia\n",
    "url = \"https://www.cnnindonesia.com/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36 Edg/140.0.0.0\"}\n",
    "\n",
    "# Ambil halaman utama\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# Loop semua link artikel di homepage\n",
    "for artikel in soup.select(\"article a\"):\n",
    "    link = artikel.get(\"href\")\n",
    "    judul = artikel.get_text(strip=True)\n",
    "\n",
    "    # Skip jika link kosong atau bukan http\n",
    "    if not link or not link.startswith(\"http\"):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Ambil halaman detail berita\n",
    "        resp_detail = requests.get(link, headers=headers)\n",
    "        soup_detail = BeautifulSoup(resp_detail.text, \"html.parser\")\n",
    "\n",
    "        # --- PERUBAHAN DIMULAI DI SINI ---\n",
    "        \n",
    "        # Ambil ID Berita dari link\n",
    "        id_berita = None\n",
    "        try:\n",
    "            # Contoh path: /ekonomi/20250910153012-92-1234567/nama-artikel\n",
    "            # Kita ambil bagian sebelum nama artikel, yaitu '20250910153012-92-1234567'\n",
    "            id_segment = link.split(\"/\")[-2]\n",
    "            # Kemudian kita pisah dengan '-' dan ambil bagian terakhirnya\n",
    "            id_berita = id_segment.split(\"-\")[-1]\n",
    "        except (IndexError, AttributeError):\n",
    "            # Jika struktur URL berbeda dan gagal, ID akan tetap None\n",
    "            id_berita = None\n",
    "\n",
    "        # Ambil kategori dari link (path pertama setelah domain)\n",
    "        kategori = None\n",
    "        try:\n",
    "            path = link.replace(\"https://www.cnnindonesia.com/\", \"\")\n",
    "            kategori = path.split(\"/\")[0] if path else None\n",
    "        except Exception:\n",
    "            kategori = None\n",
    "\n",
    "        # Ambil isi berita\n",
    "        paragraf = [p.get_text(strip=True) for p in soup_detail.select(\"div.detail-text p\")]\n",
    "        isi = \" \".join(paragraf)\n",
    "\n",
    "        if isi:  # hanya simpan kalau ada isi berita\n",
    "            data.append({\n",
    "                \"id_berita\": id_berita, # <-- Kolom baru ditambahkan di sini\n",
    "                \"judul\": judul,\n",
    "                \"kategori\": kategori,\n",
    "                \"isi\": isi,\n",
    "                \"link\": link\n",
    "            })\n",
    "            \n",
    "        # --- PERUBAHAN SELESAI DI SINI ---\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal ambil {link}: {e}\")\n",
    "\n",
    "# Simpan ke dataframe Pandas\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Simpan ke file CSV\n",
    "df.to_csv(\"berita_cnn_dengan_id.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Tampilkan hasil di notebook\n",
    "pd.set_option(\"display.max_colwidth\", 100)  # biar isi tidak kepotong\n",
    "print(\"Proses scraping selesai. Berikut adalah contoh datanya:\")\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}