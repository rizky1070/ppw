{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "085e27af",
   "metadata": {},
   "source": [
    "# Crawling Berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb1f771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mencari kategori berita di bangsaonline.com...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ditemukan 37 kategori berita valid.\n",
      "\n",
      "--- Scraping Kategori: JATIM ---\n",
      "   -> Mengambil halaman 1: https://bangsaonline.com/kanal/jawa-timur\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/25) Berhasil scrape: PT SIG Pabrik Tuban Rehab Rumah Milik Lansia di Merakurak...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2/25) Berhasil scrape: Festival Kopi Quds Royal Hotel Surabaya Meriahkan Akhir Tahu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3/25) Berhasil scrape: PT Freeport Gelar Pelatihan Apartemen Kepiting sebagai Solus...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4/25) Berhasil scrape: Diabetes dan Hipertensi Meningkat, BPJS Kesehatan Sidoarjo G...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5/25) Berhasil scrape: Cegah Korupsi, Pemkab Lamongan Terapkan Strategi Digitalisas...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6/25) Berhasil scrape: Dugaan Korupsi Dana BLUD RSUD Moh Zyn, Kejari Sampang Ungkap...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7/25) Berhasil scrape: Bupati Madiun Sampaikan Keputusan Hasil Raperdes APBDes, Sel...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8/25) Berhasil scrape: Pramuka Jatim Salurkan Bantuan Rp605 Juta untuk Bencana di S...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9/25) Berhasil scrape: Apresiasi Capaian Positif, Pemkab Mojokerto Anugerahkan 22 P...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10/25) Berhasil scrape: Update Harga Emas UBS Terbaru Beserta Buyback Hari Ini, 1 Gr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11/25) Berhasil scrape: Bupati Gresik Lantik Kades Wadak Kidul PAW, Ingatkan soal De...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12/25) Berhasil scrape: Di UIN Jakarta, Kiai Asep Ungkap Kunci Sukses Mengajar: Muri...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13/25) Berhasil scrape: Polda Jatim Tangkap Anggota Polsek Krucil Probolinggo Terkai...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14/25) Berhasil scrape: Mubes, Hari Terpilih Jadi Ketua FPRB Kabupaten Kediri Period...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15/25) Berhasil scrape: Lepas 55 Transmigran Jatim, Gubernur Khofifah Dorong Transfo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16/25) Berhasil scrape: BPJS Pastikan Peserta JKN Tetap Terlayani di Luar Domisili s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17/25) Berhasil scrape: Grand Mercure Malang Hadirkan Tema Fur and  Fiction di Peray...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18/25) Berhasil scrape: Naik Lagi, Simak Pergerakan Harga Emas Antam Terbaru Hari In...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 180\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# --- Untuk Menjalankan Seluruh Proses Scraping ---\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 180\u001b[0m     df_hasil \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_semua_berita\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df_hasil \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m         pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 160\u001b[0m, in \u001b[0;36mscrape_semua_berita\u001b[1;34m()\u001b[0m\n\u001b[0;32m    157\u001b[0m             artikel_diambil \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    158\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00martikel_diambil\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mARTIKEL_PER_KATEGORI\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) Berhasil scrape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjudul[:\u001b[38;5;241m60\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 160\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Pindah ke halaman berikutnya untuk iterasi selanjutnya\u001b[39;00m\n\u001b[0;32m    163\u001b[0m halaman_ke \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# Menambahkan header User-Agent adalah praktik yang baik\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "# Jumlah maksimal percobaan jika request gagal\n",
    "MAX_RETRIES = 3\n",
    "# Jumlah artikel yang diinginkan per kategori\n",
    "ARTIKEL_PER_KATEGORI = 25\n",
    "\n",
    "def dapatkan_kategori_berita():\n",
    "    \"\"\"\n",
    "    Fungsi untuk mengambil daftar semua kategori berita dari menu navigasi\n",
    "    website bangsaonline.com.\n",
    "    \"\"\"\n",
    "    print(\"Mencari kategori berita di bangsaonline.com...\")\n",
    "    kategori_list = {}\n",
    "    url_home = \"https://bangsaonline.com/\"\n",
    "    try:\n",
    "        response = requests.get(url_home, headers=HEADERS, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        nav_menu = soup.select_one('ul#nav')\n",
    "        if not nav_menu:\n",
    "            print(\"Menu navigasi (ul#nav) tidak ditemukan.\")\n",
    "            return {}\n",
    "\n",
    "        for item in nav_menu.find_all(\"a\"):\n",
    "            href = item.get(\"href\")\n",
    "            nama_kategori = item.get_text(strip=True)\n",
    "            \n",
    "            if href and nama_kategori:\n",
    "                path_parts = urlparse(href).path.strip(\"/\").split(\"/\")\n",
    "                \n",
    "                if len(path_parts) == 2 and path_parts[0] == 'kanal':\n",
    "                    url_lengkap = urljoin(url_home, href)\n",
    "                    if nama_kategori not in kategori_list:\n",
    "                        kategori_list[nama_kategori] = url_lengkap\n",
    "\n",
    "        print(f\"Ditemukan {len(kategori_list)} kategori berita valid.\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengambil daftar kategori berita: {e}\")\n",
    "        \n",
    "    return kategori_list\n",
    "\n",
    "def scrape_semua_berita():\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk melakukan scraping berita dari semua kategori yang ditemukan,\n",
    "    dengan implementasi pagination dan retry mechanism.\n",
    "    \"\"\"\n",
    "    daftar_kategori = dapatkan_kategori_berita()\n",
    "\n",
    "    if not daftar_kategori:\n",
    "        print(\"Tidak ada kategori yang bisa di-scrape. Program berhenti.\")\n",
    "        return\n",
    "\n",
    "    data_berita = []\n",
    "    scraped_links = set()\n",
    "    url_home = \"https://bangsaonline.com/\"\n",
    "\n",
    "    for nama_kategori, url_kategori in daftar_kategori.items():\n",
    "        print(f\"\\n--- Scraping Kategori: {nama_kategori.upper()} ---\")\n",
    "        artikel_diambil = 0\n",
    "        halaman_ke = 1\n",
    "        \n",
    "        # Loop akan berjalan selama artikel yang diambil < target\n",
    "        while artikel_diambil < ARTIKEL_PER_KATEGORI:\n",
    "            \n",
    "            # --- LOGIKA PAGINATION BARU ---\n",
    "            if halaman_ke == 1:\n",
    "                current_url = url_kategori\n",
    "            else:\n",
    "                current_url = f\"{url_kategori}?page={halaman_ke}\"\n",
    "            \n",
    "            print(f\"   -> Mengambil halaman {halaman_ke}: {current_url}\")\n",
    "            \n",
    "            response_kategori = None\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    response_kategori = requests.get(current_url, headers=HEADERS, timeout=15)\n",
    "                    response_kategori.raise_for_status()\n",
    "                    break\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"      Gagal mengambil halaman kategori (percobaan {attempt + 1}/{MAX_RETRIES}): {e}\")\n",
    "                    time.sleep(2)\n",
    "            \n",
    "            if not response_kategori:\n",
    "                print(f\"   Gagal mengambil halaman kategori {current_url} setelah {MAX_RETRIES} percobaan. Lanjut ke kategori berikutnya.\")\n",
    "                break\n",
    "\n",
    "            soup_kategori = BeautifulSoup(response_kategori.text, \"html.parser\")\n",
    "            list_artikel = soup_kategori.select(\"h3.entry-title a\")\n",
    "            \n",
    "            # --- KONDISI BERHENTI BARU ---\n",
    "            # Jika halaman tidak ada artikelnya, hentikan untuk kategori ini\n",
    "            if not list_artikel:\n",
    "                print(\"   Tidak ada artikel di halaman ini, selesai untuk kategori ini.\")\n",
    "                break\n",
    "\n",
    "            for artikel in list_artikel:\n",
    "                if artikel_diambil >= ARTIKEL_PER_KATEGORI:\n",
    "                    break\n",
    "                \n",
    "                link = urljoin(url_home, artikel.get(\"href\", \"\"))\n",
    "                if not link or link in scraped_links:\n",
    "                    continue\n",
    "\n",
    "                scraped_links.add(link)\n",
    "                \n",
    "                resp_detail = None\n",
    "                for attempt in range(MAX_RETRIES):\n",
    "                    try:\n",
    "                        resp_detail = requests.get(link, headers=HEADERS, timeout=15)\n",
    "                        resp_detail.raise_for_status()\n",
    "                        break\n",
    "                    except requests.exceptions.RequestException as e:\n",
    "                        print(f\"      Gagal mengambil detail dari {link} (percobaan {attempt + 1}/{MAX_RETRIES})\")\n",
    "                        time.sleep(1)\n",
    "\n",
    "                if not resp_detail:\n",
    "                    print(f\"   -> Gagal total mengambil detail dari {link}. Melewatkan artikel ini.\")\n",
    "                    continue\n",
    "                        \n",
    "                soup_detail = BeautifulSoup(resp_detail.text, \"html.parser\")\n",
    "                judul_element = soup_detail.select_one(\"h1.entry-title\")\n",
    "                konten_berita = soup_detail.select_one(\"div.post\")\n",
    "                \n",
    "                if judul_element and konten_berita:\n",
    "                    judul = judul_element.get_text(strip=True)\n",
    "                    \n",
    "                    id_berita = None\n",
    "                    try:\n",
    "                        path_parts = urlparse(link).path.strip(\"/\").split(\"/\")\n",
    "                        if len(path_parts) > 1 and path_parts[1].isdigit():\n",
    "                            id_berita = path_parts[1]\n",
    "                    except (IndexError, AttributeError):\n",
    "                        id_berita = None\n",
    "                    \n",
    "                    for unwanted in konten_berita.select(\"div.baca-juga, div.shared-icons\"):\n",
    "                        unwanted.decompose()\n",
    "                    \n",
    "                    paragraf = [p.get_text(strip=True) for p in konten_berita.select(\"p\")]\n",
    "                    isi = \" \".join(paragraf)\n",
    "\n",
    "                    if isi:\n",
    "                        data_berita.append({\n",
    "                            \"id_berita\": id_berita, \"kategori\": nama_kategori,\n",
    "                            \"judul\": judul, \"isi_berita\": isi, \"link\": link\n",
    "                        })\n",
    "                        artikel_diambil += 1\n",
    "                        print(f\"({artikel_diambil}/{ARTIKEL_PER_KATEGORI}) Berhasil scrape: {judul[:60]}...\")\n",
    "                \n",
    "                time.sleep(1) \n",
    "\n",
    "            # Pindah ke halaman berikutnya untuk iterasi selanjutnya\n",
    "            halaman_ke += 1\n",
    "\n",
    "    if not data_berita:\n",
    "        print(\"\\nTidak ada berita yang berhasil di-scrape.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data_berita)\n",
    "    df = df[[\"id_berita\", \"kategori\", \"judul\", \"isi_berita\", \"link\"]]\n",
    "    \n",
    "    nama_file = \"hasil_scraping_berita_bangsaonline.csv\"\n",
    "    df.to_csv(nama_file, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\n✅ Proses scraping selesai. {len(df)} berita disimpan ke '{nama_file}'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Untuk Menjalankan Seluruh Proses Scraping ---\n",
    "if __name__ == \"__main__\":\n",
    "    df_hasil = scrape_semua_berita()\n",
    "    if df_hasil is not None:\n",
    "        pd.set_option('display.max_colwidth', 100)\n",
    "        print(\"\\nContoh hasil data:\")\n",
    "        print(df_hasil.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140c532b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_berita</th>\n",
       "      <th>kategori</th>\n",
       "      <th>judul</th>\n",
       "      <th>isi_berita</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>153055</td>\n",
       "      <td>Jatim</td>\n",
       "      <td>Ada Santri yang Diduga Dihukum untuk Cor Bangunan Musala Ambruk Ponpes Al Khoziny</td>\n",
       "      <td>SIDOARJO,BANGSAONLINE.com- Proses pencarian korban para santri tertimbun reruntuhan musala ambru...</td>\n",
       "      <td>https://bangsaonline.com/berita/153055/ada-santri-yang-diduga-dihukum-untuk-cor-bangunan-musala-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153054</td>\n",
       "      <td>Jatim</td>\n",
       "      <td>Warga Dringu Lapor Dugaan Penipuan Kavling, Polres Probolinggo Kota Siap Tindaklanjuti</td>\n",
       "      <td>PROBOLINGGO, BANGSAONLINE.com- Polres Probolinggo Kota menerima laporan dari warga Desa/Kecamata...</td>\n",
       "      <td>https://bangsaonline.com/berita/153054/warga-dringu-lapor-dugaan-penipuan-kavling-polres-proboli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153053</td>\n",
       "      <td>Jatim</td>\n",
       "      <td>Hadiri Pengukuhan Pengurus Kormi Kota Kediri, Gus Qowim Dorong Olahraga untuk Semua Usia</td>\n",
       "      <td>KOTA KEDIRI, BANGSAONLINE.com- Wakil Wali Kota Kediri, Qowimuddin Thoha atau yang akrab disapa G...</td>\n",
       "      <td>https://bangsaonline.com/berita/153053/hadiri-pengukuhan-pengurus-kormi-kota-kediri-gus-qowim-do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153052</td>\n",
       "      <td>Jatim</td>\n",
       "      <td>Datang ke RS William Booth Perpanjang Rujukan Anak, Pria di Surabaya Malah Curi Motor</td>\n",
       "      <td>SURABAYA,BANGSAONLINE.com- Polsek Wonokromo  menangkap pelaku pencurian motor di area parkir Rum...</td>\n",
       "      <td>https://bangsaonline.com/berita/153052/datang-ke-rs-william-booth-perpanjang-rujukan-anak-pria-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153050</td>\n",
       "      <td>Jatim</td>\n",
       "      <td>Polwan Polresta Sidoarjo Layani Keluarga Korban Musala Ambruk Ponpes Al Khoziny</td>\n",
       "      <td>SIDOARJO,BANGSAONLINE.com- Polwan Polresta Sidoarjo memberikan pelayanan bagi keluarga korban tr...</td>\n",
       "      <td>https://bangsaonline.com/berita/153050/polwan-polresta-sidoarjo-layani-keluarga-korban-musala-am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_berita kategori  \\\n",
       "0    153055    Jatim   \n",
       "1    153054    Jatim   \n",
       "2    153053    Jatim   \n",
       "3    153052    Jatim   \n",
       "4    153050    Jatim   \n",
       "\n",
       "                                                                                      judul  \\\n",
       "0         Ada Santri yang Diduga Dihukum untuk Cor Bangunan Musala Ambruk Ponpes Al Khoziny   \n",
       "1    Warga Dringu Lapor Dugaan Penipuan Kavling, Polres Probolinggo Kota Siap Tindaklanjuti   \n",
       "2  Hadiri Pengukuhan Pengurus Kormi Kota Kediri, Gus Qowim Dorong Olahraga untuk Semua Usia   \n",
       "3     Datang ke RS William Booth Perpanjang Rujukan Anak, Pria di Surabaya Malah Curi Motor   \n",
       "4           Polwan Polresta Sidoarjo Layani Keluarga Korban Musala Ambruk Ponpes Al Khoziny   \n",
       "\n",
       "                                                                                            isi_berita  \\\n",
       "0  SIDOARJO,BANGSAONLINE.com- Proses pencarian korban para santri tertimbun reruntuhan musala ambru...   \n",
       "1  PROBOLINGGO, BANGSAONLINE.com- Polres Probolinggo Kota menerima laporan dari warga Desa/Kecamata...   \n",
       "2  KOTA KEDIRI, BANGSAONLINE.com- Wakil Wali Kota Kediri, Qowimuddin Thoha atau yang akrab disapa G...   \n",
       "3  SURABAYA,BANGSAONLINE.com- Polsek Wonokromo  menangkap pelaku pencurian motor di area parkir Rum...   \n",
       "4  SIDOARJO,BANGSAONLINE.com- Polwan Polresta Sidoarjo memberikan pelayanan bagi keluarga korban tr...   \n",
       "\n",
       "                                                                                                  link  \n",
       "0  https://bangsaonline.com/berita/153055/ada-santri-yang-diduga-dihukum-untuk-cor-bangunan-musala-...  \n",
       "1  https://bangsaonline.com/berita/153054/warga-dringu-lapor-dugaan-penipuan-kavling-polres-proboli...  \n",
       "2  https://bangsaonline.com/berita/153053/hadiri-pengukuhan-pengurus-kormi-kota-kediri-gus-qowim-do...  \n",
       "3  https://bangsaonline.com/berita/153052/datang-ke-rs-william-booth-perpanjang-rujukan-anak-pria-d...  \n",
       "4  https://bangsaonline.com/berita/153050/polwan-polresta-sidoarjo-layani-keluarga-korban-musala-am...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hasil.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}