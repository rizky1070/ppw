{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "085e27af",
   "metadata": {},
   "source": [
    "# Crawling Berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb1f771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mencari kategori berita di bangsaonline.com...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ditemukan 37 kategori berita valid.\n",
      "\n",
      "--- Scraping Kategori: JATIM ---\n",
      "   -> Mengambil halaman 1: https://bangsaonline.com/kanal/jawa-timur\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/25) Berhasil scrape: Festival Santri 2025 Resmi Dibuka di Jember, Kembangkan Sema...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2/25) Berhasil scrape: Satgas TMMD ke-126 Sidoarjo Bangun Lapangan Voli di Tulangan...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3/25) Berhasil scrape: Pemkab Pamekasan Salurkan Bantuan untuk Warga Terdampak Angi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4/25) Berhasil scrape: Perluas Cakupan, Perumda Tirta Lestari Tuban Kembangkan Sekt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5/25) Berhasil scrape: Kasus Dugaan Penyelewengan Dana Desa Karangjati Pasuruan Jal...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6/25) Berhasil scrape: Maling Pompa Air Masjid di Pasuruan Ditangkap...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7/25) Berhasil scrape: Timbulkan Polusi, Pabrik Pembakaran Batu Kapur di Tuban Dike...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8/25) Berhasil scrape: Lantik Ketua dan Pengurus GOW 2025-2030, Wali Kota Kediri Do...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9/25) Berhasil scrape: Satya JKN Awards 2025, BPJS Kesehatan Apresiasi 110 Badan Us...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10/25) Berhasil scrape: 604 Pendonor Darah Sukarela Raih Penghargaan, Gubernur Khofi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11/25) Berhasil scrape: Wow! Jokowi Targetkan PSI Raih 30 Kursi DPR, Butuh Dana Rp 2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12/25) Berhasil scrape: DPD PSI Gresik Klaim Sejumlah Kader Partai Siap Bergabung...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13/25) Berhasil scrape: Sarasehan GMNI Surabaya Teguhkan Persatuan Kader, Akhiri Dua...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14/25) Berhasil scrape: TMMD ke-126 Sidoarjo Sinergi dengan Warga Tulangan Wujudkan ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15/25) Berhasil scrape: 40 Item Koleksi Museum Cakraningrat Bangkalan Raib, Termasuk...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16/25) Berhasil scrape: Gerindra Gresik Sapa Ribuan Warga Lewat Senam Aerobik Berhad...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17/25) Berhasil scrape: Kecewa Lantaran Diduga Tak Diusulkan PPPK, Puluhan Tenaga Ke...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18/25) Berhasil scrape: Unik, Ternyata Setiap Tanggal 13 Oktober Diperingati sebagai...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 180\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# --- Untuk Menjalankan Seluruh Proses Scraping ---\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 180\u001b[0m     df_hasil \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_semua_berita\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df_hasil \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m         pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 160\u001b[0m, in \u001b[0;36mscrape_semua_berita\u001b[1;34m()\u001b[0m\n\u001b[0;32m    157\u001b[0m             artikel_diambil \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    158\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00martikel_diambil\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mARTIKEL_PER_KATEGORI\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) Berhasil scrape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjudul[:\u001b[38;5;241m60\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 160\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Pindah ke halaman berikutnya untuk iterasi selanjutnya\u001b[39;00m\n\u001b[0;32m    163\u001b[0m halaman_ke \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# Menambahkan header User-Agent adalah praktik yang baik\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "# Jumlah maksimal percobaan jika request gagal\n",
    "MAX_RETRIES = 3\n",
    "# Jumlah artikel yang diinginkan per kategori\n",
    "ARTIKEL_PER_KATEGORI = 25\n",
    "\n",
    "def dapatkan_kategori_berita():\n",
    "    \"\"\"\n",
    "    Fungsi untuk mengambil daftar semua kategori berita dari menu navigasi\n",
    "    website bangsaonline.com.\n",
    "    \"\"\"\n",
    "    print(\"Mencari kategori berita di bangsaonline.com...\")\n",
    "    kategori_list = {}\n",
    "    url_home = \"https://bangsaonline.com/\"\n",
    "    try:\n",
    "        response = requests.get(url_home, headers=HEADERS, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        nav_menu = soup.select_one('ul#nav')\n",
    "        if not nav_menu:\n",
    "            print(\"Menu navigasi (ul#nav) tidak ditemukan.\")\n",
    "            return {}\n",
    "\n",
    "        for item in nav_menu.find_all(\"a\"):\n",
    "            href = item.get(\"href\")\n",
    "            nama_kategori = item.get_text(strip=True)\n",
    "            \n",
    "            if href and nama_kategori:\n",
    "                path_parts = urlparse(href).path.strip(\"/\").split(\"/\")\n",
    "                \n",
    "                if len(path_parts) == 2 and path_parts[0] == 'kanal':\n",
    "                    url_lengkap = urljoin(url_home, href)\n",
    "                    if nama_kategori not in kategori_list:\n",
    "                        kategori_list[nama_kategori] = url_lengkap\n",
    "\n",
    "        print(f\"Ditemukan {len(kategori_list)} kategori berita valid.\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengambil daftar kategori berita: {e}\")\n",
    "        \n",
    "    return kategori_list\n",
    "\n",
    "def scrape_semua_berita():\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk melakukan scraping berita dari semua kategori yang ditemukan,\n",
    "    dengan implementasi pagination dan retry mechanism.\n",
    "    \"\"\"\n",
    "    daftar_kategori = dapatkan_kategori_berita()\n",
    "\n",
    "    if not daftar_kategori:\n",
    "        print(\"Tidak ada kategori yang bisa di-scrape. Program berhenti.\")\n",
    "        return\n",
    "\n",
    "    data_berita = []\n",
    "    scraped_links = set()\n",
    "    url_home = \"https://bangsaonline.com/\"\n",
    "\n",
    "    for nama_kategori, url_kategori in daftar_kategori.items():\n",
    "        print(f\"\\n--- Scraping Kategori: {nama_kategori.upper()} ---\")\n",
    "        artikel_diambil = 0\n",
    "        halaman_ke = 1\n",
    "        \n",
    "        # Loop akan berjalan selama artikel yang diambil < target\n",
    "        while artikel_diambil < ARTIKEL_PER_KATEGORI:\n",
    "            \n",
    "            # --- LOGIKA PAGINATION BARU ---\n",
    "            if halaman_ke == 1:\n",
    "                current_url = url_kategori\n",
    "            else:\n",
    "                current_url = f\"{url_kategori}?page={halaman_ke}\"\n",
    "            \n",
    "            print(f\"   -> Mengambil halaman {halaman_ke}: {current_url}\")\n",
    "            \n",
    "            response_kategori = None\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    response_kategori = requests.get(current_url, headers=HEADERS, timeout=15)\n",
    "                    response_kategori.raise_for_status()\n",
    "                    break\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"      Gagal mengambil halaman kategori (percobaan {attempt + 1}/{MAX_RETRIES}): {e}\")\n",
    "                    time.sleep(2)\n",
    "            \n",
    "            if not response_kategori:\n",
    "                print(f\"   Gagal mengambil halaman kategori {current_url} setelah {MAX_RETRIES} percobaan. Lanjut ke kategori berikutnya.\")\n",
    "                break\n",
    "\n",
    "            soup_kategori = BeautifulSoup(response_kategori.text, \"html.parser\")\n",
    "            list_artikel = soup_kategori.select(\"h3.entry-title a\")\n",
    "            \n",
    "            # --- KONDISI BERHENTI BARU ---\n",
    "            # Jika halaman tidak ada artikelnya, hentikan untuk kategori ini\n",
    "            if not list_artikel:\n",
    "                print(\"   Tidak ada artikel di halaman ini, selesai untuk kategori ini.\")\n",
    "                break\n",
    "\n",
    "            for artikel in list_artikel:\n",
    "                if artikel_diambil >= ARTIKEL_PER_KATEGORI:\n",
    "                    break\n",
    "                \n",
    "                link = urljoin(url_home, artikel.get(\"href\", \"\"))\n",
    "                if not link or link in scraped_links:\n",
    "                    continue\n",
    "\n",
    "                scraped_links.add(link)\n",
    "                \n",
    "                resp_detail = None\n",
    "                for attempt in range(MAX_RETRIES):\n",
    "                    try:\n",
    "                        resp_detail = requests.get(link, headers=HEADERS, timeout=15)\n",
    "                        resp_detail.raise_for_status()\n",
    "                        break\n",
    "                    except requests.exceptions.RequestException as e:\n",
    "                        print(f\"      Gagal mengambil detail dari {link} (percobaan {attempt + 1}/{MAX_RETRIES})\")\n",
    "                        time.sleep(1)\n",
    "\n",
    "                if not resp_detail:\n",
    "                    print(f\"   -> Gagal total mengambil detail dari {link}. Melewatkan artikel ini.\")\n",
    "                    continue\n",
    "                        \n",
    "                soup_detail = BeautifulSoup(resp_detail.text, \"html.parser\")\n",
    "                judul_element = soup_detail.select_one(\"h1.entry-title\")\n",
    "                konten_berita = soup_detail.select_one(\"div.post\")\n",
    "                \n",
    "                if judul_element and konten_berita:\n",
    "                    judul = judul_element.get_text(strip=True)\n",
    "                    \n",
    "                    id_berita = None\n",
    "                    try:\n",
    "                        path_parts = urlparse(link).path.strip(\"/\").split(\"/\")\n",
    "                        if len(path_parts) > 1 and path_parts[1].isdigit():\n",
    "                            id_berita = path_parts[1]\n",
    "                    except (IndexError, AttributeError):\n",
    "                        id_berita = None\n",
    "                    \n",
    "                    for unwanted in konten_berita.select(\"div.baca-juga, div.shared-icons\"):\n",
    "                        unwanted.decompose()\n",
    "                    \n",
    "                    paragraf = [p.get_text(strip=True) for p in konten_berita.select(\"p\")]\n",
    "                    isi = \" \".join(paragraf)\n",
    "\n",
    "                    if isi:\n",
    "                        data_berita.append({\n",
    "                            \"id_berita\": id_berita, \"kategori\": nama_kategori,\n",
    "                            \"judul\": judul, \"isi_berita\": isi, \"link\": link\n",
    "                        })\n",
    "                        artikel_diambil += 1\n",
    "                        print(f\"({artikel_diambil}/{ARTIKEL_PER_KATEGORI}) Berhasil scrape: {judul[:60]}...\")\n",
    "                \n",
    "                time.sleep(1) \n",
    "\n",
    "            # Pindah ke halaman berikutnya untuk iterasi selanjutnya\n",
    "            halaman_ke += 1\n",
    "\n",
    "    if not data_berita:\n",
    "        print(\"\\nTidak ada berita yang berhasil di-scrape.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data_berita)\n",
    "    df = df[[\"id_berita\", \"kategori\", \"judul\", \"isi_berita\", \"link\"]]\n",
    "    \n",
    "    nama_file = \"hasil_scraping_berita_bangsaonline.csv\"\n",
    "    df.to_csv(nama_file, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\n✅ Proses scraping selesai. {len(df)} berita disimpan ke '{nama_file}'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Untuk Menjalankan Seluruh Proses Scraping ---\n",
    "if __name__ == \"__main__\":\n",
    "    df_hasil = scrape_semua_berita()\n",
    "    if df_hasil is not None:\n",
    "        pd.set_option('display.max_colwidth', 100)\n",
    "        print(\"\\nContoh hasil data:\")\n",
    "        print(df_hasil.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140c532b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_berita</th>\n",
       "      <th>kategori</th>\n",
       "      <th>judul</th>\n",
       "      <th>isi_berita</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>153055</td>\n",
       "      <td>Jatim</td>\n",
       "      <td>Ada Santri yang Diduga Dihukum untuk Cor Bangunan Musala Ambruk Ponpes Al Khoziny</td>\n",
       "      <td>SIDOARJO,BANGSAONLINE.com- Proses pencarian korban para santri tertimbun reruntuhan musala ambru...</td>\n",
       "      <td>https://bangsaonline.com/berita/153055/ada-santri-yang-diduga-dihukum-untuk-cor-bangunan-musala-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153054</td>\n",
       "      <td>Jatim</td>\n",
       "      <td>Warga Dringu Lapor Dugaan Penipuan Kavling, Polres Probolinggo Kota Siap Tindaklanjuti</td>\n",
       "      <td>PROBOLINGGO, BANGSAONLINE.com- Polres Probolinggo Kota menerima laporan dari warga Desa/Kecamata...</td>\n",
       "      <td>https://bangsaonline.com/berita/153054/warga-dringu-lapor-dugaan-penipuan-kavling-polres-proboli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153053</td>\n",
       "      <td>Jatim</td>\n",
       "      <td>Hadiri Pengukuhan Pengurus Kormi Kota Kediri, Gus Qowim Dorong Olahraga untuk Semua Usia</td>\n",
       "      <td>KOTA KEDIRI, BANGSAONLINE.com- Wakil Wali Kota Kediri, Qowimuddin Thoha atau yang akrab disapa G...</td>\n",
       "      <td>https://bangsaonline.com/berita/153053/hadiri-pengukuhan-pengurus-kormi-kota-kediri-gus-qowim-do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153052</td>\n",
       "      <td>Jatim</td>\n",
       "      <td>Datang ke RS William Booth Perpanjang Rujukan Anak, Pria di Surabaya Malah Curi Motor</td>\n",
       "      <td>SURABAYA,BANGSAONLINE.com- Polsek Wonokromo  menangkap pelaku pencurian motor di area parkir Rum...</td>\n",
       "      <td>https://bangsaonline.com/berita/153052/datang-ke-rs-william-booth-perpanjang-rujukan-anak-pria-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153050</td>\n",
       "      <td>Jatim</td>\n",
       "      <td>Polwan Polresta Sidoarjo Layani Keluarga Korban Musala Ambruk Ponpes Al Khoziny</td>\n",
       "      <td>SIDOARJO,BANGSAONLINE.com- Polwan Polresta Sidoarjo memberikan pelayanan bagi keluarga korban tr...</td>\n",
       "      <td>https://bangsaonline.com/berita/153050/polwan-polresta-sidoarjo-layani-keluarga-korban-musala-am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_berita kategori  \\\n",
       "0    153055    Jatim   \n",
       "1    153054    Jatim   \n",
       "2    153053    Jatim   \n",
       "3    153052    Jatim   \n",
       "4    153050    Jatim   \n",
       "\n",
       "                                                                                      judul  \\\n",
       "0         Ada Santri yang Diduga Dihukum untuk Cor Bangunan Musala Ambruk Ponpes Al Khoziny   \n",
       "1    Warga Dringu Lapor Dugaan Penipuan Kavling, Polres Probolinggo Kota Siap Tindaklanjuti   \n",
       "2  Hadiri Pengukuhan Pengurus Kormi Kota Kediri, Gus Qowim Dorong Olahraga untuk Semua Usia   \n",
       "3     Datang ke RS William Booth Perpanjang Rujukan Anak, Pria di Surabaya Malah Curi Motor   \n",
       "4           Polwan Polresta Sidoarjo Layani Keluarga Korban Musala Ambruk Ponpes Al Khoziny   \n",
       "\n",
       "                                                                                            isi_berita  \\\n",
       "0  SIDOARJO,BANGSAONLINE.com- Proses pencarian korban para santri tertimbun reruntuhan musala ambru...   \n",
       "1  PROBOLINGGO, BANGSAONLINE.com- Polres Probolinggo Kota menerima laporan dari warga Desa/Kecamata...   \n",
       "2  KOTA KEDIRI, BANGSAONLINE.com- Wakil Wali Kota Kediri, Qowimuddin Thoha atau yang akrab disapa G...   \n",
       "3  SURABAYA,BANGSAONLINE.com- Polsek Wonokromo  menangkap pelaku pencurian motor di area parkir Rum...   \n",
       "4  SIDOARJO,BANGSAONLINE.com- Polwan Polresta Sidoarjo memberikan pelayanan bagi keluarga korban tr...   \n",
       "\n",
       "                                                                                                  link  \n",
       "0  https://bangsaonline.com/berita/153055/ada-santri-yang-diduga-dihukum-untuk-cor-bangunan-musala-...  \n",
       "1  https://bangsaonline.com/berita/153054/warga-dringu-lapor-dugaan-penipuan-kavling-polres-proboli...  \n",
       "2  https://bangsaonline.com/berita/153053/hadiri-pengukuhan-pengurus-kormi-kota-kediri-gus-qowim-do...  \n",
       "3  https://bangsaonline.com/berita/153052/datang-ke-rs-william-booth-perpanjang-rujukan-anak-pria-d...  \n",
       "4  https://bangsaonline.com/berita/153050/polwan-polresta-sidoarjo-layani-keluarga-korban-musala-am...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hasil.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}