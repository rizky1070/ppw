{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a7f461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terjadi error: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byprod/10/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terjadi error: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byprod/10/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terjadi error: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byprod/10/3\n",
      "Empty DataFrame\n",
      "Columns: [penulis, judul, pembimbing_pertama, pembimbing_kedua, abstrak, abstrak_inggris]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def ptaa_updated():\n",
    "    # Inisialisasi dictionary dengan menambahkan kunci baru \"abstrak_inggris\"\n",
    "    data = {\n",
    "        \"penulis\": [],\n",
    "        \"judul\": [],\n",
    "        \"pembimbing_pertama\": [],\n",
    "        \"pembimbing_kedua\": [],\n",
    "        \"abstrak\": [],\n",
    "        \"abstrak_inggris\": []  # Kolom baru untuk abstrak Bahasa Inggris\n",
    "    }\n",
    "\n",
    "    # Looping tetap sama\n",
    "    for i in range(1, 4):\n",
    "        url = f\"https://pta.trunojoyo.ac.id/c_search/byprod/10/{i}\"\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            r.raise_for_status()  # Memeriksa jika ada error pada request\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            jurnals = soup.select('li[data-cat=\"#luxury\"]')\n",
    "\n",
    "            for jurnal in jurnals:\n",
    "                jurnal_url = jurnal.select_one('a.gray.button')['href']\n",
    "                response = requests.get(jurnal_url)\n",
    "                response.raise_for_status()\n",
    "                soup1 = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "                isi = soup1.select_one('div#content_journal')\n",
    "\n",
    "                if not isi:\n",
    "                    continue\n",
    "\n",
    "                # Mengambil data yang sudah ada\n",
    "                judul = isi.select_one('a.title').text.strip()\n",
    "                penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1].strip()\n",
    "                pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1].strip()\n",
    "                pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(':')[1].strip()\n",
    "\n",
    "                # --- MODIFIKASI UNTUK ABSTRAK ---\n",
    "                # Memilih semua paragraf dengan 'align=\"justify\"'\n",
    "                abstract_paragraphs = isi.select('p[align=\"justify\"]')\n",
    "\n",
    "                # Paragraf pertama adalah abstrak Bahasa Indonesia\n",
    "                abstrak_indonesia = abstract_paragraphs[0].text.strip() if len(abstract_paragraphs) > 0 else \"Tidak ada abstrak\"\n",
    "\n",
    "                # Paragraf kedua adalah abstrak Bahasa Inggris\n",
    "                abstrak_inggris = abstract_paragraphs[1].text.strip() if len(abstract_paragraphs) > 1 else \"No abstract available\"\n",
    "                # --- AKHIR DARI MODIFIKASI ---\n",
    "\n",
    "                # Menambahkan semua data ke dictionary\n",
    "                data[\"penulis\"].append(penulis)\n",
    "                data[\"judul\"].append(judul)\n",
    "                data[\"pembimbing_pertama\"].append(pembimbing_pertama)\n",
    "                data[\"pembimbing_kedua\"].append(pembimbing_kedua)\n",
    "                data[\"abstrak\"].append(abstrak_indonesia)\n",
    "                data[\"abstrak_inggris\"].append(abstrak_inggris) # Menambahkan data baru\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Terjadi error: {e}\")\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"pta_updated.csv\", index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Untuk menjalankan fungsi\n",
    "updated_dataframe = ptaa_updated()\n",
    "print(updated_dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0f3938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terjadi error: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byprod/10/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terjadi error: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byprod/10/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terjadi error: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byprod/10/3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>penulis</th>\n",
       "      <th>judul</th>\n",
       "      <th>pembimbing_pertama</th>\n",
       "      <th>pembimbing_kedua</th>\n",
       "      <th>abstrak</th>\n",
       "      <th>abstrak_inggris</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [penulis, judul, pembimbing_pertama, pembimbing_kedua, abstrak, abstrak_inggris]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptaa_updated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfebd319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "def get_fakultas_prodi_list():\n",
    "    \"\"\"\n",
    "    Fungsi untuk mengambil daftar semua Fakultas dan Prodi di dalamnya.\n",
    "    (Fungsi ini tidak berubah dari sebelumnya)\n",
    "    \"\"\"\n",
    "    prodi_list = []\n",
    "    try:\n",
    "        url_nav = \"https://pta.trunojoyo.ac.id/c_search/byfac\"\n",
    "        r = requests.get(url_nav)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        \n",
    "        sidebar_nav = soup.select_one('div.box.sidebar_nav')\n",
    "        if not sidebar_nav:\n",
    "            print(\"Sidebar navigasi tidak ditemukan.\")\n",
    "            return []\n",
    "\n",
    "        fakultas_items = sidebar_nav.select_one('ul').find_all('li', recursive=False)\n",
    "        \n",
    "        for item_fakultas in fakultas_items:\n",
    "            anchor_fakultas = item_fakultas.find('a', recursive=False)\n",
    "            if not anchor_fakultas: continue\n",
    "            nama_fakultas = anchor_fakultas.get_text(strip=True)\n",
    "            \n",
    "            ul_prodi = item_fakultas.find('ul')\n",
    "            if not ul_prodi: continue\n",
    "\n",
    "            for link_prodi in ul_prodi.select('li a'):\n",
    "                nama_prodi = link_prodi.get_text(strip=True)\n",
    "                href = link_prodi.get('href')\n",
    "                prodi_id = href.strip('/').split('/')[-1]\n",
    "                \n",
    "                if prodi_id.isdigit():\n",
    "                    prodi_list.append({\n",
    "                        \"id_prodi\": int(prodi_id),\n",
    "                        \"nama_prodi\": nama_prodi,\n",
    "                        \"nama_fakultas\": nama_fakultas\n",
    "                    })\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengambil daftar fakultas dan prodi: {e}\")\n",
    "        \n",
    "    return prodi_list\n",
    "\n",
    "def scrape_all():\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk scraping data, dimodifikasi untuk mengambil\n",
    "    maksimal 4 data per prodi.\n",
    "    \"\"\"\n",
    "    daftar_prodi_lengkap = get_fakultas_prodi_list()\n",
    "    if not daftar_prodi_lengkap:\n",
    "        print(\"Tidak ada prodi yang bisa di-scrape. Program berhenti.\")\n",
    "        return\n",
    "\n",
    "    data = {\n",
    "        \"nama_fakultas\": [], \"id_prodi\": [], \"nama_prodi\": [],\n",
    "        \"penulis\": [], \"judul\": [], \"pembimbing_pertama\": [],\n",
    "        \"pembimbing_kedua\": [], \"abstrak\": [], \"abstrak_inggris\": []\n",
    "    }\n",
    "\n",
    "    for prodi in daftar_prodi_lengkap:\n",
    "        prodi_id = prodi[\"id_prodi\"]\n",
    "        nama_prodi = prodi[\"nama_prodi\"]\n",
    "        nama_fakultas = prodi[\"nama_fakultas\"]\n",
    "        page = 1\n",
    "        \n",
    "        # --- PERUBAHAN DIMULAI DI SINI ---\n",
    "        \n",
    "        # 1. Tambahkan counter untuk menghitung jurnal yang sudah diambil per prodi\n",
    "        jurnal_diambil_count = 0\n",
    "        \n",
    "        # 2. Tambahkan 'flag' untuk menandai jika batas sudah tercapai\n",
    "        limit_tercapai = False\n",
    "        \n",
    "        # --- AKHIR PERUBAHAN ---\n",
    "\n",
    "        while True:\n",
    "            url = f\"https://pta.trunojoyo.ac.id/c_search/byprod/{prodi_id}/{page}\"\n",
    "            try:\n",
    "                r = requests.get(url)\n",
    "                r.raise_for_status()\n",
    "                soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "                jurnals = soup.select('li[data-cat=\"#luxury\"]')\n",
    "\n",
    "                if not jurnals:\n",
    "                    break\n",
    "\n",
    "                for jurnal in jurnals:\n",
    "                    # --- PERUBAHAN DIMULAI DI SINI ---\n",
    "                    \n",
    "                    # 3. Cek apakah counter sudah mencapai 4, jika ya, hentikan loop\n",
    "                    if jurnal_diambil_count >= 1:\n",
    "                        limit_tercapai = True # Set flag menjadi True\n",
    "                        break # Hentikan loop 'for jurnal in jurnals'\n",
    "                    \n",
    "                    # --- AKHIR PERUBAHAN ---\n",
    "\n",
    "                    # Proses scraping detail (tetap sama)\n",
    "                    jurnal_url = jurnal.select_one('a.gray.button')['href']\n",
    "                    response = requests.get(jurnal_url)\n",
    "                    response.raise_for_status()\n",
    "                    isi = BeautifulSoup(response.content, \"html.parser\").select_one('div#content_journal')\n",
    "                    if not isi: continue\n",
    "\n",
    "                    # (Kode untuk mengambil judul, penulis, dll. tidak diubah)\n",
    "                    judul = isi.select_one('a.title').text.strip()\n",
    "                    penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1].strip()\n",
    "                    pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1].strip()\n",
    "                    pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(':')[1].strip()\n",
    "                    # --- PERUBAHAN PADA EKSTRAKSI ABSTRAK ---\n",
    "                    abstract_paragraphs = isi.select('p[align=\"justify\"]')\n",
    "                    \n",
    "                    # Ambil teks mentah dari abstrak Bahasa Indonesia\n",
    "                    text_indo_mentah = abstract_paragraphs[0].text if len(abstract_paragraphs) > 0 else \"Tidak ada abstrak\"\n",
    "                    # 2. BERSIHKAN TEKS MENTAH\n",
    "                    abstrak_indonesia = re.sub(r'\\s+', ' ', text_indo_mentah).strip()\n",
    "\n",
    "                    # Ambil teks mentah dari abstrak Bahasa Inggris\n",
    "                    text_inggris_mentah = abstract_paragraphs[1].text if len(abstract_paragraphs) > 1 else \"No abstract available\"\n",
    "                    # 2. BERSIHKAN TEKS MENTAH\n",
    "                    abstrak_inggris = re.sub(r'\\s+', ' ', text_inggris_mentah).strip()\n",
    "                    # --- AKHIR PERUBAHAN ---\n",
    "\n",
    "                    # Menambahkan semua data ke dictionary\n",
    "                    data[\"nama_fakultas\"].append(nama_fakultas)\n",
    "                    data[\"id_prodi\"].append(prodi_id)\n",
    "                    data[\"nama_prodi\"].append(nama_prodi)\n",
    "                    data[\"penulis\"].append(penulis)\n",
    "                    data[\"judul\"].append(judul)\n",
    "                    data[\"pembimbing_pertama\"].append(pembimbing_pertama)\n",
    "                    data[\"pembimbing_kedua\"].append(pembimbing_kedua)\n",
    "                    data[\"abstrak\"].append(abstrak_indonesia)\n",
    "                    data[\"abstrak_inggris\"].append(abstrak_inggris)\n",
    "                    \n",
    "                    # --- PERUBAHAN DIMULAI DI SINI ---\n",
    "                    \n",
    "                    # 4. Tambah nilai counter setiap kali satu jurnal berhasil diambil\n",
    "                    jurnal_diambil_count += 1\n",
    "                    \n",
    "                    # --- AKHIR PERUBAHAN ---\n",
    "                \n",
    "                # --- PERUBAHAN DIMULAI DI SINI ---\n",
    "                \n",
    "                # 5. Cek flag, jika True, hentikan juga loop halaman (while)\n",
    "                if limit_tercapai:\n",
    "                    break\n",
    "                    \n",
    "                # --- AKHIR PERUBAHAN ---\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(1)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error pada Prodi {nama_prodi} (ID: {prodi_id}) halaman {page}: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"✔️ Selesai: {nama_fakultas} - {nama_prodi} (ID: {prodi_id}) | Berhasil mengambil {jurnal_diambil_count} jurnal.\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"pta_4_data_per_prodi.csv\", index=False)\n",
    "    print(\"\\n✅ Proses scraping selesai. Data disimpan ke 'pta_4_data_per_prodi.csv'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Untuk menjalankan seluruh proses scraping\n",
    "# df_final = scrape_all()\n",
    "# print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ebef33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40a92ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gagal mengambil daftar fakultas dan prodi: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byfac\n",
      "Tidak ada prodi yang bisa di-scrape. Program berhenti.\n"
     ]
    }
   ],
   "source": [
    "scrape_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d907f8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gagal mengambil daftar prodi: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byfac\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import concurrent.futures\n",
    "\n",
    "# MAKSIMAL PEKERJA SIMULTAN (BISA DISESUAIKAN)\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "def get_fakultas_prodi_list():\n",
    "    # Fungsi ini tidak berubah, tetap sama seperti sebelumnya\n",
    "    prodi_list = []\n",
    "    try:\n",
    "        url_nav = \"https://pta.trunojoyo.ac.id/c_search/byfac\"\n",
    "        r = requests.get(url_nav, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        sidebar_nav = soup.select_one('div.box.sidebar_nav')\n",
    "        if not sidebar_nav: return []\n",
    "        fakultas_items = sidebar_nav.select_one('ul').find_all('li', recursive=False)\n",
    "        for item_fakultas in fakultas_items:\n",
    "            anchor_fakultas = item_fakultas.find('a', recursive=False)\n",
    "            if not anchor_fakultas: continue\n",
    "            nama_fakultas = anchor_fakultas.get_text(strip=True)\n",
    "            ul_prodi = item_fakultas.find('ul')\n",
    "            if not ul_prodi: continue\n",
    "            for link_prodi in ul_prodi.select('li a'):\n",
    "                nama_prodi = link_prodi.get_text(strip=True)\n",
    "                href = link_prodi.get('href')\n",
    "                prodi_id = href.strip('/').split('/')[-1]\n",
    "                if prodi_id.isdigit():\n",
    "                    prodi_list.append({\n",
    "                        \"id_prodi\": int(prodi_id),\n",
    "                        \"nama_prodi\": nama_prodi,\n",
    "                        \"nama_fakultas\": nama_fakultas\n",
    "                    })\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengambil daftar prodi: {e}\")\n",
    "    return prodi_list\n",
    "\n",
    "def scrape_jurnal_detail(jurnal_url):\n",
    "    \"\"\"\n",
    "    FUNGSI PEKERJA: Hanya bertugas men-scrape detail SATU jurnal.\n",
    "    Fungsi inilah yang akan dijalankan oleh setiap thread.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(jurnal_url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        isi = BeautifulSoup(response.content, \"html.parser\").select_one('div#content_journal')\n",
    "        if not isi: return None\n",
    "\n",
    "        judul = isi.select_one('a.title').text.strip()\n",
    "        penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1].strip()\n",
    "        pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1].strip()\n",
    "        pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(':')[1].strip()\n",
    "        \n",
    "        abstract_paragraphs = isi.select('p[align=\"justify\"]')\n",
    "        text_indo_mentah = abstract_paragraphs[0].text if len(abstract_paragraphs) > 0 else \"\"\n",
    "        abstrak_indonesia = re.sub(r'\\s+', ' ', text_indo_mentah).strip()\n",
    "        text_inggris_mentah = abstract_paragraphs[1].text if len(abstract_paragraphs) > 1 else \"\"\n",
    "        abstrak_inggris = re.sub(r'\\s+', ' ', text_inggris_mentah).strip()\n",
    "\n",
    "        return {\n",
    "            \"penulis\": penulis, \"judul\": judul, \"pembimbing_pertama\": pembimbing_pertama,\n",
    "            \"pembimbing_kedua\": pembimbing_kedua, \"abstrak\": abstrak_indonesia,\n",
    "            \"abstrak_inggris\": abstrak_inggris\n",
    "        }\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None # Return None jika gagal scrape satu jurnal\n",
    "\n",
    "def scrape_prodi(prodi):\n",
    "    \"\"\"\n",
    "    FUNGSI MANAJER: Bertugas mencari URL jurnal untuk SATU prodi,\n",
    "    lalu menyuruh 'pekerja' untuk men-scrape detailnya.\n",
    "    \"\"\"\n",
    "    jurnal_urls = []\n",
    "    page = 1\n",
    "    while len(jurnal_urls) < 4:\n",
    "        try:\n",
    "            url = f\"https://pta.trunojoyo.ac.id/c_search/byprod/{prodi['id_prodi']}/{page}\"\n",
    "            r = requests.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            jurnals_on_page = soup.select('li[data-cat=\"#luxury\"] a.gray.button')\n",
    "\n",
    "            if not jurnals_on_page:\n",
    "                break # Hentikan jika tidak ada jurnal lagi di halaman\n",
    "\n",
    "            for a_tag in jurnals_on_page:\n",
    "                if len(jurnal_urls) < 4:\n",
    "                    jurnal_urls.append(a_tag['href'])\n",
    "                else:\n",
    "                    break\n",
    "            page += 1\n",
    "        except requests.exceptions.RequestException:\n",
    "            break # Hentikan jika ada error jaringan\n",
    "\n",
    "    if not jurnal_urls:\n",
    "        return []\n",
    "\n",
    "    all_jurnal_data = []\n",
    "    # Jalankan thread pool untuk men-scrape semua URL detail yang sudah terkumpul\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        hasil_scrape = executor.map(scrape_jurnal_detail, jurnal_urls)\n",
    "        for hasil in hasil_scrape:\n",
    "            if hasil: # Pastikan hasilnya tidak None\n",
    "                # Tambahkan info prodi dan fakultas ke hasil scrape\n",
    "                hasil['id_prodi'] = prodi['id_prodi']\n",
    "                hasil['nama_prodi'] = prodi['nama_prodi']\n",
    "                hasil['nama_fakultas'] = prodi['nama_fakultas']\n",
    "                all_jurnal_data.append(hasil)\n",
    "    \n",
    "    print(f\"✔️ Selesai: {prodi['nama_fakultas']} - {prodi['nama_prodi']} | Berhasil mengambil {len(all_jurnal_data)} jurnal.\")\n",
    "    return all_jurnal_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fungsi utama untuk mengorkestrasi seluruh proses.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    daftar_prodi = get_fakultas_prodi_list()\n",
    "    if not daftar_prodi: return\n",
    "\n",
    "    final_results = []\n",
    "    # Kita bahkan bisa menjalankan scraping antar prodi secara konkuren!\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Jalankan fungsi scrape_prodi untuk setiap item di daftar_prodi\n",
    "        results_per_prodi = executor.map(scrape_prodi, daftar_prodi)\n",
    "        for list_jurnal in results_per_prodi:\n",
    "            final_results.extend(list_jurnal)\n",
    "\n",
    "    df = pd.DataFrame(final_results)\n",
    "    # Atur urutan kolom agar lebih rapi\n",
    "    if not df.empty:\n",
    "        df = df[['nama_fakultas', 'id_prodi', 'nama_prodi', 'judul', 'penulis', 'pembimbing_pertama', 'pembimbing_kedua', 'abstrak', 'abstrak_inggris']]\n",
    "    \n",
    "    df.to_csv(\"pta_final_concurrent.csv\", index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"\\n✅ Proses scraping selesai.\")\n",
    "    print(f\"Total data yang berhasil diambil: {len(df)} jurnal.\")\n",
    "    print(f\"Total waktu eksekusi: {end_time - start_time:.2f} detik.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9bdb64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gagal mengambil daftar prodi: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byfac\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import concurrent.futures\n",
    "\n",
    "# MAKSIMAL PEKERJA SIMULTAN (BISA DISESUAIKAN)\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "def get_fakultas_prodi_list():\n",
    "    # Fungsi ini tidak berubah, tetap sama seperti sebelumnya\n",
    "    prodi_list = []\n",
    "    try:\n",
    "        url_nav = \"https://pta.trunojoyo.ac.id/c_search/byfac\"\n",
    "        r = requests.get(url_nav, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        sidebar_nav = soup.select_one('div.box.sidebar_nav')\n",
    "        if not sidebar_nav: return []\n",
    "        fakultas_items = sidebar_nav.select_one('ul').find_all('li', recursive=False)\n",
    "        for item_fakultas in fakultas_items:\n",
    "            anchor_fakultas = item_fakultas.find('a', recursive=False)\n",
    "            if not anchor_fakultas: continue\n",
    "            nama_fakultas = anchor_fakultas.get_text(strip=True)\n",
    "            ul_prodi = item_fakultas.find('ul')\n",
    "            if not ul_prodi: continue\n",
    "            for link_prodi in ul_prodi.select('li a'):\n",
    "                nama_prodi = link_prodi.get_text(strip=True)\n",
    "                href = link_prodi.get('href')\n",
    "                prodi_id = href.strip('/').split('/')[-1]\n",
    "                if prodi_id.isdigit():\n",
    "                    prodi_list.append({\n",
    "                        \"id_prodi\": int(prodi_id),\n",
    "                        \"nama_prodi\": nama_prodi,\n",
    "                        \"nama_fakultas\": nama_fakultas\n",
    "                    })\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengambil daftar prodi: {e}\")\n",
    "    return prodi_list\n",
    "\n",
    "def scrape_jurnal_detail(jurnal_url):\n",
    "    # Fungsi ini tidak berubah\n",
    "    try:\n",
    "        response = requests.get(jurnal_url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        isi = BeautifulSoup(response.content, \"html.parser\").select_one('div#content_journal')\n",
    "        if not isi: return None\n",
    "\n",
    "        judul = isi.select_one('a.title').text.strip()\n",
    "        penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1].strip()\n",
    "        pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1].strip()\n",
    "        pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(':')[1].strip()\n",
    "        \n",
    "        abstract_paragraphs = isi.select('p[align=\"justify\"]')\n",
    "        text_indo_mentah = abstract_paragraphs[0].text if len(abstract_paragraphs) > 0 else \"\"\n",
    "        abstrak_indonesia = re.sub(r'\\s+', ' ', text_indo_mentah).strip()\n",
    "        text_inggris_mentah = abstract_paragraphs[1].text if len(abstract_paragraphs) > 1 else \"\"\n",
    "        abstrak_inggris = re.sub(r'\\s+', ' ', text_inggris_mentah).strip()\n",
    "\n",
    "        return {\n",
    "            \"penulis\": penulis, \"judul\": judul, \"pembimbing_pertama\": pembimbing_pertama,\n",
    "            \"pembimbing_kedua\": pembimbing_kedua, \"abstrak\": abstrak_indonesia,\n",
    "            \"abstrak_inggris\": abstrak_inggris\n",
    "        }\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "def scrape_prodi(prodi):\n",
    "    # Fungsi ini tidak berubah\n",
    "    jurnal_urls = []\n",
    "    page = 1\n",
    "    while len(jurnal_urls) < 4:\n",
    "        try:\n",
    "            url = f\"https://pta.trunojoyo.ac.id/c_search/byprod/{prodi['id_prodi']}/{page}\"\n",
    "            r = requests.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            jurnals_on_page = soup.select('li[data-cat=\"#luxury\"] a.gray.button')\n",
    "            if not jurnals_on_page: break\n",
    "            for a_tag in jurnals_on_page:\n",
    "                if len(jurnal_urls) < 4:\n",
    "                    jurnal_urls.append(a_tag['href'])\n",
    "                else: break\n",
    "            page += 1\n",
    "        except requests.exceptions.RequestException:\n",
    "            break\n",
    "\n",
    "    if not jurnal_urls: return []\n",
    "\n",
    "    all_jurnal_data = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        hasil_scrape = executor.map(scrape_jurnal_detail, jurnal_urls)\n",
    "        for hasil in hasil_scrape:\n",
    "            if hasil:\n",
    "                hasil['id_prodi'] = prodi['id_prodi']\n",
    "                hasil['nama_prodi'] = prodi['nama_prodi']\n",
    "                hasil['nama_fakultas'] = prodi['nama_fakultas']\n",
    "                all_jurnal_data.append(hasil)\n",
    "    \n",
    "    print(f\"✔️ Selesai: {prodi['nama_fakultas']} - {prodi['nama_prodi']} | Berhasil mengambil {len(all_jurnal_data)} jurnal.\")\n",
    "    return all_jurnal_data\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    daftar_prodi = get_fakultas_prodi_list()\n",
    "    if not daftar_prodi: return pd.DataFrame() # Return DataFrame kosong jika gagal\n",
    "\n",
    "    final_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        results_per_prodi = executor.map(scrape_prodi, daftar_prodi)\n",
    "        for list_jurnal in results_per_prodi:\n",
    "            final_results.extend(list_jurnal)\n",
    "\n",
    "    df = pd.DataFrame(final_results)\n",
    "    if not df.empty:\n",
    "        df = df[['nama_fakultas', 'id_prodi', 'nama_prodi', 'judul', 'penulis', 'pembimbing_pertama', 'pembimbing_kedua', 'abstrak', 'abstrak_inggris']]\n",
    "    \n",
    "    df.to_csv(\"pta_final_concurrent.csv\", index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"\\n✅ Proses scraping selesai.\")\n",
    "    print(f\"Total data yang berhasil diambil: {len(df)} jurnal.\")\n",
    "    print(f\"Total waktu eksekusi: {end_time - start_time:.2f} detik.\")\n",
    "    \n",
    "    # --- PERUBAHAN DI SINI ---\n",
    "    # 1. Kembalikan DataFrame yang sudah jadi\n",
    "    return df\n",
    "\n",
    "# --- DAN PERUBAHAN DI SINI ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 2. Panggil fungsi dan simpan hasilnya ke variabel\n",
    "    df_hasil = main()\n",
    "    \n",
    "    # 3. Letakkan variabel di baris terakhir untuk menampilkannya di Colab/Jupyter\n",
    "    df_hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c807a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gagal mengambil daftar prodi: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byfac\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9ef2b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import concurrent.futures\n",
    "\n",
    "# MAKSIMAL PEKERJA SIMULTAN (BISA DISESUAIKAN)\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "def get_fakultas_prodi_list():\n",
    "    # Fungsi ini tidak berubah\n",
    "    prodi_list = []\n",
    "    try:\n",
    "        url_nav = \"https://pta.trunojoyo.ac.id/c_search/byfac\"\n",
    "        r = requests.get(url_nav, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        sidebar_nav = soup.select_one('div.box.sidebar_nav')\n",
    "        if not sidebar_nav: return []\n",
    "        fakultas_items = sidebar_nav.select_one('ul').find_all('li', recursive=False)\n",
    "        for item_fakultas in fakultas_items:\n",
    "            anchor_fakultas = item_fakultas.find('a', recursive=False)\n",
    "            if not anchor_fakultas: continue\n",
    "            nama_fakultas = anchor_fakultas.get_text(strip=True)\n",
    "            ul_prodi = item_fakultas.find('ul')\n",
    "            if not ul_prodi: continue\n",
    "            for link_prodi in ul_prodi.select('li a'):\n",
    "                nama_prodi = link_prodi.get_text(strip=True)\n",
    "                href = link_prodi.get('href')\n",
    "                prodi_id = href.strip('/').split('/')[-1]\n",
    "                if prodi_id.isdigit():\n",
    "                    prodi_list.append({\n",
    "                        \"id_prodi\": int(prodi_id),\n",
    "                        \"nama_prodi\": nama_prodi,\n",
    "                        \"nama_fakultas\": nama_fakultas\n",
    "                    })\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengambil daftar prodi: {e}\")\n",
    "    return prodi_list\n",
    "\n",
    "def scrape_jurnal_detail(jurnal_url):\n",
    "    # Fungsi ini tidak berubah\n",
    "    try:\n",
    "        response = requests.get(jurnal_url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        isi = BeautifulSoup(response.content, \"html.parser\").select_one('div#content_journal')\n",
    "        if not isi: return None\n",
    "        judul = isi.select_one('a.title').text.strip()\n",
    "        penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1].strip()\n",
    "        pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1].strip()\n",
    "        pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(':')[1].strip()\n",
    "        abstract_paragraphs = isi.select('p[align=\"justify\"]')\n",
    "        text_indo_mentah = abstract_paragraphs[0].text if len(abstract_paragraphs) > 0 else \"\"\n",
    "        abstrak_indonesia = re.sub(r'\\s+', ' ', text_indo_mentah).strip()\n",
    "        text_inggris_mentah = abstract_paragraphs[1].text if len(abstract_paragraphs) > 1 else \"\"\n",
    "        abstrak_inggris = re.sub(r'\\s+', ' ', text_inggris_mentah).strip()\n",
    "        return {\n",
    "            \"penulis\": penulis, \"judul\": judul, \"pembimbing_pertama\": pembimbing_pertama,\n",
    "            \"pembimbing_kedua\": pembimbing_kedua, \"abstrak\": abstrak_indonesia,\n",
    "            \"abstrak_inggris\": abstrak_inggris\n",
    "        }\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "# --- PERUBAHAN UTAMA ADA DI FUNGSI BERIKUT ---\n",
    "def scrape_prodi(prodi):\n",
    "    \"\"\"\n",
    "    FUNGSI MANAJER: Sekarang akan mengembalikan baris data kosong\n",
    "    jika tidak ada jurnal yang ditemukan.\n",
    "    \"\"\"\n",
    "    jurnal_urls = []\n",
    "    page = 1\n",
    "    while len(jurnal_urls) < 4:\n",
    "        try:\n",
    "            url = f\"https://pta.trunojoyo.ac.id/c_search/byprod/{prodi['id_prodi']}/{page}\"\n",
    "            r = requests.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            jurnals_on_page = soup.select('li[data-cat=\"#luxury\"] a.gray.button')\n",
    "            if not jurnals_on_page: break\n",
    "            for a_tag in jurnals_on_page:\n",
    "                if len(jurnal_urls) < 4:\n",
    "                    jurnal_urls.append(a_tag['href'])\n",
    "                else: break\n",
    "            page += 1\n",
    "        except requests.exceptions.RequestException:\n",
    "            break\n",
    "\n",
    "    # --- BAGIAN YANG DIMODIFIKASI ---\n",
    "    # Jika setelah mencari, tidak ada URL jurnal yang ditemukan\n",
    "    if not jurnal_urls:\n",
    "        print(f\"✔️ Selesai: {prodi['nama_fakultas']} - {prodi['nama_prodi']} | Ditemukan 0 jurnal.\")\n",
    "        # Buat satu baris data placeholder\n",
    "        empty_row = {\n",
    "            'id_prodi': prodi['id_prodi'],\n",
    "            'nama_prodi': prodi['nama_prodi'],\n",
    "            'nama_fakultas': prodi['nama_fakultas'],\n",
    "            'judul': 'Tidak ada jurnal',\n",
    "            'penulis': None,\n",
    "            'pembimbing_pertama': None,\n",
    "            'pembimbing_kedua': None,\n",
    "            'abstrak': None,\n",
    "            'abstrak_inggris': None\n",
    "        }\n",
    "        return [empty_row] # Kembalikan sebagai list berisi satu dictionary\n",
    "    # --- AKHIR MODIFIKASI ---\n",
    "\n",
    "    # Jika ada URL, proses berjalan seperti biasa\n",
    "    all_jurnal_data = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        hasil_scrape = executor.map(scrape_jurnal_detail, jurnal_urls)\n",
    "        for hasil in hasil_scrape:\n",
    "            if hasil:\n",
    "                hasil['id_prodi'] = prodi['id_prodi']\n",
    "                hasil['nama_prodi'] = prodi['nama_prodi']\n",
    "                hasil['nama_fakultas'] = prodi['nama_fakultas']\n",
    "                all_jurnal_data.append(hasil)\n",
    "    \n",
    "    print(f\"✔️ Selesai: {prodi['nama_fakultas']} - {prodi['nama_prodi']} | Berhasil mengambil {len(all_jurnal_data)} jurnal.\")\n",
    "    return all_jurnal_data\n",
    "\n",
    "def main():\n",
    "    # Fungsi ini tidak berubah\n",
    "    start_time = time.time()\n",
    "    daftar_prodi = get_fakultas_prodi_list()\n",
    "    if not daftar_prodi: return pd.DataFrame()\n",
    "\n",
    "    final_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        results_per_prodi = executor.map(scrape_prodi, daftar_prodi)\n",
    "        for list_jurnal in results_per_prodi:\n",
    "            final_results.extend(list_jurnal)\n",
    "\n",
    "    df = pd.DataFrame(final_results)\n",
    "    if not df.empty:\n",
    "        df = df[['nama_fakultas', 'id_prodi', 'nama_prodi', 'judul', 'penulis', 'pembimbing_pertama', 'pembimbing_kedua', 'abstrak', 'abstrak_inggris']]\n",
    "    \n",
    "    df.to_csv(\"pta_final_concurrent_lengkap.csv\", index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"\\n✅ Proses scraping selesai.\")\n",
    "    print(f\"Total baris data yang dihasilkan: {len(df)}.\")\n",
    "    print(f\"Total waktu eksekusi: {end_time - start_time:.2f} detik.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     df_hasil = main()\n",
    "#     df_hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d522ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gagal mengambil daftar prodi: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byfac\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2512906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import concurrent.futures\n",
    "\n",
    "# MAKSIMAL PEKERJA SIMULTAN (BISA DISESUAIKAN)\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "def get_fakultas_prodi_list():\n",
    "    # ... (fungsi ini tidak berubah)\n",
    "    prodi_list = []\n",
    "    try:\n",
    "        url_nav = \"https://pta.trunojoyo.ac.id/c_search/byfac\"\n",
    "        r = requests.get(url_nav, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        sidebar_nav = soup.select_one('div.box.sidebar_nav')\n",
    "        if not sidebar_nav: return []\n",
    "        fakultas_items = sidebar_nav.select_one('ul').find_all('li', recursive=False)\n",
    "        for item_fakultas in fakultas_items:\n",
    "            anchor_fakultas = item_fakultas.find('a', recursive=False)\n",
    "            if not anchor_fakultas: continue\n",
    "            nama_fakultas = anchor_fakultas.get_text(strip=True)\n",
    "            ul_prodi = item_fakultas.find('ul')\n",
    "            if not ul_prodi: continue\n",
    "            for link_prodi in ul_prodi.select('li a'):\n",
    "                nama_prodi = link_prodi.get_text(strip=True)\n",
    "                href = link_prodi.get('href')\n",
    "                prodi_id = href.strip('/').split('/')[-1]\n",
    "                if prodi_id.isdigit():\n",
    "                    prodi_list.append({\n",
    "                        \"id_prodi\": int(prodi_id),\n",
    "                        \"nama_prodi\": nama_prodi,\n",
    "                        \"nama_fakultas\": nama_fakultas\n",
    "                    })\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengambil daftar prodi: {e}\")\n",
    "    return prodi_list\n",
    "\n",
    "def scrape_jurnal_detail(jurnal_url):\n",
    "    # ... (fungsi ini tidak berubah)\n",
    "    try:\n",
    "        response = requests.get(jurnal_url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        isi = BeautifulSoup(response.content, \"html.parser\").select_one('div#content_journal')\n",
    "        if not isi: return None\n",
    "        judul = isi.select_one('a.title').text.strip()\n",
    "        penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1].strip()\n",
    "        pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1].strip()\n",
    "        pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(':')[1].strip()\n",
    "        abstract_paragraphs = isi.select('p[align=\"justify\"]')\n",
    "        text_indo_mentah = abstract_paragraphs[0].text if len(abstract_paragraphs) > 0 else \"\"\n",
    "        abstrak_indonesia = re.sub(r'\\s+', ' ', text_indo_mentah).strip()\n",
    "        text_inggris_mentah = abstract_paragraphs[1].text if len(abstract_paragraphs) > 1 else \"\"\n",
    "        abstrak_inggris = re.sub(r'\\s+', ' ', text_inggris_mentah).strip()\n",
    "        return {\n",
    "            \"penulis\": penulis, \"judul\": judul, \"pembimbing_pertama\": pembimbing_pertama,\n",
    "            \"pembimbing_kedua\": pembimbing_kedua, \"abstrak\": abstrak_indonesia,\n",
    "            \"abstrak_inggris\": abstrak_inggris\n",
    "        }\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "def scrape_prodi(prodi):\n",
    "    # ... (fungsi ini tidak berubah, kecuali pada bagian print)\n",
    "    jurnal_urls = []\n",
    "    page = 1\n",
    "    while len(jurnal_urls) < 4:\n",
    "        try:\n",
    "            url = f\"https://pta.trunojoyo.ac.id/c_search/byprod/{prodi['id_prodi']}/{page}\"\n",
    "            r = requests.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            jurnals_on_page = soup.select('li[data-cat=\"#luxury\"] a.gray.button')\n",
    "            if not jurnals_on_page: break\n",
    "            for a_tag in jurnals_on_page:\n",
    "                if len(jurnal_urls) < 4:\n",
    "                    jurnal_urls.append(a_tag['href'])\n",
    "                else: break\n",
    "            page += 1\n",
    "        except requests.exceptions.RequestException:\n",
    "            break\n",
    "\n",
    "    if not jurnal_urls:\n",
    "        # Tambahkan \\n di awal print agar lebih rapi\n",
    "        print(f\"\\n✔️ Selesai: {prodi['nama_fakultas']} - {prodi['nama_prodi']} | Ditemukan 0 jurnal.\")\n",
    "        return [{\n",
    "            'id_prodi': prodi['id_prodi'], 'nama_prodi': prodi['nama_prodi'],\n",
    "            'nama_fakultas': prodi['nama_fakultas'], 'judul': 'Tidak ada jurnal',\n",
    "            'penulis': None, 'pembimbing_pertama': None, 'pembimbing_kedua': None,\n",
    "            'abstrak': None, 'abstrak_inggris': None\n",
    "        }]\n",
    "\n",
    "    all_jurnal_data = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        hasil_scrape = executor.map(scrape_jurnal_detail, jurnal_urls)\n",
    "        for hasil in hasil_scrape:\n",
    "            if hasil:\n",
    "                hasil['id_prodi'] = prodi['id_prodi']\n",
    "                hasil['nama_prodi'] = prodi['nama_prodi']\n",
    "                hasil['nama_fakultas'] = prodi['nama_fakultas']\n",
    "                all_jurnal_data.append(hasil)\n",
    "    \n",
    "    # Tambahkan \\n di awal print agar lebih rapi\n",
    "    print(f\"\\n✔️ Selesai: {prodi['nama_fakultas']} - {prodi['nama_prodi']} | Berhasil mengambil {len(all_jurnal_data)} jurnal.\")\n",
    "    return all_jurnal_data\n",
    "\n",
    "def main():\n",
    "    # ... (fungsi ini tidak berubah)\n",
    "    start_time = time.time()\n",
    "    daftar_prodi = get_fakultas_prodi_list()\n",
    "    if not daftar_prodi: return pd.DataFrame()\n",
    "\n",
    "    final_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        results_per_prodi = executor.map(scrape_prodi, daftar_prodi)\n",
    "        for list_jurnal in results_per_prodi:\n",
    "            final_results.extend(list_jurnal)\n",
    "\n",
    "    df = pd.DataFrame(final_results)\n",
    "    if not df.empty:\n",
    "        # Mengurutkan DataFrame berdasarkan urutan prodi asli\n",
    "        original_prodi_order = [p['id_prodi'] for p in daftar_prodi]\n",
    "        df['id_prodi'] = pd.Categorical(df['id_prodi'], categories=original_prodi_order, ordered=True)\n",
    "        df = df.sort_values('id_prodi').reset_index(drop=True)\n",
    "        \n",
    "        df = df[['nama_fakultas', 'id_prodi', 'nama_prodi', 'judul', 'penulis', 'pembimbing_pertama', 'pembimbing_kedua', 'abstrak', 'abstrak_inggris']]\n",
    "    \n",
    "    df.to_csv(\"pta_final_concurrent_sorted.csv\", index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"\\n\\n✅ Proses scraping selesai.\") # Tambah baris baru agar rapi\n",
    "    print(f\"Total baris data yang dihasilkan: {len(df)}.\")\n",
    "    print(f\"Total waktu eksekusi: {end_time - start_time:.2f} detik.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     df_hasil_cepat = main()\n",
    "#     df_hasil_cepat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "407c3457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gagal mengambil daftar prodi: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byfac\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78b68dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gagal mengambil daftar prodi: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/c_search/byfac\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import concurrent.futures\n",
    "\n",
    "# MAKSIMAL PEKERJA SIMULTAN (BISA DISESUAIKAN)\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "def get_fakultas_prodi_list():\n",
    "    # Fungsi ini tidak berubah\n",
    "    prodi_list = []\n",
    "    try:\n",
    "        url_nav = \"https://pta.trunojoyo.ac.id/c_search/byfac\"\n",
    "        r = requests.get(url_nav, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        sidebar_nav = soup.select_one('div.box.sidebar_nav')\n",
    "        if not sidebar_nav: return []\n",
    "        fakultas_items = sidebar_nav.select_one('ul').find_all('li', recursive=False)\n",
    "        for item_fakultas in fakultas_items:\n",
    "            anchor_fakultas = item_fakultas.find('a', recursive=False)\n",
    "            if not anchor_fakultas: continue\n",
    "            nama_fakultas = anchor_fakultas.get_text(strip=True)\n",
    "            ul_prodi = item_fakultas.find('ul')\n",
    "            if not ul_prodi: continue\n",
    "            for link_prodi in ul_prodi.select('li a'):\n",
    "                nama_prodi = link_prodi.get_text(strip=True)\n",
    "                href = link_prodi.get('href')\n",
    "                prodi_id = href.strip('/').split('/')[-1]\n",
    "                if prodi_id.isdigit():\n",
    "                    prodi_list.append({\n",
    "                        \"id_prodi\": int(prodi_id),\n",
    "                        \"nama_prodi\": nama_prodi,\n",
    "                        \"nama_fakultas\": nama_fakultas\n",
    "                    })\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengambil daftar prodi: {e}\")\n",
    "    return prodi_list\n",
    "\n",
    "def scrape_jurnal_detail(jurnal_url):\n",
    "    # Fungsi ini tidak berubah\n",
    "    try:\n",
    "        response = requests.get(jurnal_url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        isi = BeautifulSoup(response.content, \"html.parser\").select_one('div#content_journal')\n",
    "        if not isi: return None\n",
    "        judul = isi.select_one('a.title').text.strip()\n",
    "        penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1].strip()\n",
    "        pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1].strip()\n",
    "        pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(':')[1].strip()\n",
    "        abstract_paragraphs = isi.select('p[align=\"justify\"]')\n",
    "        text_indo_mentah = abstract_paragraphs[0].text if len(abstract_paragraphs) > 0 else \"\"\n",
    "        abstrak_indonesia = re.sub(r'\\s+', ' ', text_indo_mentah).strip()\n",
    "        text_inggris_mentah = abstract_paragraphs[1].text if len(abstract_paragraphs) > 1 else \"\"\n",
    "        abstrak_inggris = re.sub(r'\\s+', ' ', text_inggris_mentah).strip()\n",
    "        return {\n",
    "            \"penulis\": penulis, \"judul\": judul, \"pembimbing_pertama\": pembimbing_pertama,\n",
    "            \"pembimbing_kedua\": pembimbing_kedua, \"abstrak\": abstrak_indonesia,\n",
    "            \"abstrak_inggris\": abstrak_inggris\n",
    "        }\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "# --- PERUBAHAN UTAMA ADA DI FUNGSI BERIKUT ---\n",
    "def scrape_prodi(prodi):\n",
    "    \"\"\"\n",
    "    Fungsi MANAJER: Sekarang mengambil SEMUA jurnal yang tersedia\n",
    "    dan menampilkan ID prodi di log.\n",
    "    \"\"\"\n",
    "    jurnal_urls = []\n",
    "    page = 1\n",
    "    \n",
    "    # 1. MENGHAPUS BATAS 4 JURNAL\n",
    "    # Loop akan berjalan terus sampai menemukan halaman kosong\n",
    "    while True:\n",
    "        try:\n",
    "            url = f\"https://pta.trunojoyo.ac.id/c_search/byprod/{prodi['id_prodi']}/{page}\"\n",
    "            r = requests.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            jurnals_on_page = soup.select('li[data-cat=\"#luxury\"] a.gray.button')\n",
    "\n",
    "            # Jika halaman kosong (tidak ada jurnal), hentikan pencarian untuk prodi ini\n",
    "            if not jurnals_on_page:\n",
    "                break\n",
    "            \n",
    "            # Ambil semua URL di halaman ini\n",
    "            for a_tag in jurnals_on_page:\n",
    "                jurnal_urls.append(a_tag['href'])\n",
    "            \n",
    "            page += 1\n",
    "            time.sleep(0.5) # Beri jeda kecil antar halaman\n",
    "        except requests.exceptions.RequestException:\n",
    "            break # Hentikan jika ada error jaringan\n",
    "\n",
    "    # Jika tidak ada URL jurnal yang ditemukan sama sekali\n",
    "    if not jurnal_urls:\n",
    "        # 2. MENAMBAHKAN ID PRODI PADA LOG\n",
    "        print(f\"\\n✔️ Selesai: {prodi['nama_fakultas']} - {prodi['nama_prodi']} (ID: {prodi['id_prodi']}) | Ditemukan 0 jurnal.\")\n",
    "        return [{\n",
    "            'id_prodi': prodi['id_prodi'], 'nama_prodi': prodi['nama_prodi'],\n",
    "            'nama_fakultas': prodi['nama_fakultas'], 'judul': 'Tidak ada jurnal',\n",
    "            'penulis': None, 'pembimbing_pertama': None, 'pembimbing_kedua': None,\n",
    "            'abstrak': None, 'abstrak_inggris': None\n",
    "        }]\n",
    "\n",
    "    # Jika ada URL, proses scrape detail berjalan secara konkuren\n",
    "    all_jurnal_data = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        hasil_scrape = executor.map(scrape_jurnal_detail, jurnal_urls)\n",
    "        for hasil in hasil_scrape:\n",
    "            if hasil:\n",
    "                hasil['id_prodi'] = prodi['id_prodi']\n",
    "                hasil['nama_prodi'] = prodi['nama_prodi']\n",
    "                hasil['nama_fakultas'] = prodi['nama_fakultas']\n",
    "                all_jurnal_data.append(hasil)\n",
    "    \n",
    "    # 2. MENAMBAHKAN ID PRODI PADA LOG\n",
    "    print(f\"\\n✔️ Selesai: {prodi['nama_fakultas']} - {prodi['nama_prodi']} (ID: {prodi['id_prodi']}) | Berhasil mengambil {len(all_jurnal_data)} dari {len(jurnal_urls)} jurnal yang ditemukan.\")\n",
    "    return all_jurnal_data\n",
    "\n",
    "def main():\n",
    "    # Fungsi ini tidak berubah\n",
    "    start_time = time.time()\n",
    "    daftar_prodi = get_fakultas_prodi_list()\n",
    "    if not daftar_prodi: return pd.DataFrame()\n",
    "\n",
    "    final_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        results_per_prodi = executor.map(scrape_prodi, daftar_prodi)\n",
    "        for list_jurnal in results_per_prodi:\n",
    "            final_results.extend(list_jurnal)\n",
    "\n",
    "    df = pd.DataFrame(final_results)\n",
    "    if not df.empty:\n",
    "        original_prodi_order = [p['id_prodi'] for p in daftar_prodi]\n",
    "        df['id_prodi'] = pd.Categorical(df['id_prodi'], categories=original_prodi_order, ordered=True)\n",
    "        df = df.sort_values('id_prodi').reset_index(drop=True)\n",
    "        df = df[['nama_fakultas', 'id_prodi', 'nama_prodi', 'judul', 'penulis', 'pembimbing_pertama', 'pembimbing_kedua', 'abstrak', 'abstrak_inggris']]\n",
    "    \n",
    "    df.to_csv(\"pta_semua_jurnal_concurrent.csv\", index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"\\n\\n✅ Proses scraping selesai.\")\n",
    "    print(f\"Total baris data yang dihasilkan: {len(df)}.\")\n",
    "    print(f\"Total waktu eksekusi: {end_time - start_time:.2f} detik.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_hasil_semua = main()\n",
    "    df_hasil_semua"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}