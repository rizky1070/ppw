{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89da0803",
   "metadata": {},
   "source": [
    "# CBOW Berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03765425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4319e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e3d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 1: MEMPERSIAPKAN CORPUS ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses persiapan corpus selesai.\n",
      "Berikut adalah contoh 1 dokumen (berita) yang sudah diubah menjadi daftar token:\n",
      "['kejar', 'tanjung', 'perak', 'surabaya', 'geledah', 'kantor', 'pt', 'pelindo', 'regional', 'surabaya', 'duga', 'kait', 'korupsi', 'kolam', 'labuh', 'kepala', 'kejar', 'tanjung', 'perak', 'ricky', 'setiawan', 'anas', 'geledah', 'kamis', 'wib', 'tim', 'sidik', 'kejar', 'tanjung', 'perak', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'dasar', 'tetap', 'pn', 'tipikor', 'surabaya', 'nomor', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'terang', 'kamis', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'sidik', 'pidsus', 'kejar', 'tanjung', 'perak', 'geledah', 'lokasi', 'tepat', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'dasar', 'tetap', 'geledah', 'pn', 'tipikor', 'surabaya', 'no', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'geledah', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'terang', 'geledah', 'personel', 'jaksa', 'kejar', 'tanjung', 'perak', 'pidsus', 'kejat', 'jatim', 'aman', 'tni', 'geledah', 'orang', 'jaksa', 'sidik', 'orang', 'personil', 'amc', 'kejat', 'jatim', 'orang', 'personil', 'pam', 'tni', 'imbuh', 'ricky', 'geledah', 'kait', 'sidi', 'perkara', 'duga', 'tindak', 'pidana', 'korupsi', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'pt', 'pelindo', 'sub', 'reg', 'bersamasama', 'pt', 'apbs', 'turut', 'duga', 'tipikor', 'turut', 'duga', 'tipikor', 'pt', 'apbs', 'pelindo', 'sub', 'regional', 'capai', 'ratus', 'miliar', 'rupiah', 'nilai', 'giat', 'rp', 'miliar', 'ricky', 'geledah', 'kumpul', 'bukti', 'tambah', 'kait', 'duga', 'tipikor', 'giat', 'eru', 'kolam', 'labuh', 'labuh', 'tanjung', 'perak', 'geledah', 'lokasi', 'sidik', 'laksana', 'giat', 'sita', 'kait', 'buktibukti', 'hubung', 'giat', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'laptop', 'dokumen', 'kait', 'kontrak', 'giat', 'rif']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast # Library untuk mengubah string menjadi list\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "\n",
    "# --- 1. Persiapan Corpus ---\n",
    "print(\"--- TAHAP 1: MEMPERSIAPKAN CORPUS ---\")\n",
    "# Load kembali data Anda (atau lanjutkan dari DataFrame yang sudah ada)\n",
    "df = pd.read_csv('hasil_preprocessing_berita.csv')\n",
    "\n",
    "# --- Konversi kolom 'hasil_preprocessing' dari string ke list ---\n",
    "# Ini adalah langkah penting!\n",
    "# ast.literal_eval akan membaca string \"['a', 'b']\" dan mengubahnya menjadi list ['a', 'b']\n",
    "df['tokens'] = df['hasil_preprocessing'].apply(ast.literal_eval)\n",
    "\n",
    "# Buat corpus yang siap untuk dilatih\n",
    "corpus = df['tokens'].tolist()\n",
    "\n",
    "\n",
    "print(\"Proses persiapan corpus selesai.\")\n",
    "print(\"Berikut adalah contoh 1 dokumen (berita) yang sudah diubah menjadi daftar token:\")\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10107212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:43,975 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:43,976 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:44,090 : INFO : collected 41180 word types from a corpus of 763911 raw words and 3653 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:44,091 : INFO : Creating a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 2: MELATIH MODEL WORD2VEC (CBOW) ---\n",
      "Parameter: Dimensi vektor = 150, Arsitektur = CBOW\n",
      "Gensim akan menampilkan log proses training di bawah ini:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:44,162 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 21810 unique words (52.96% of original 41180, drops 19370)', 'datetime': '2025-10-15T00:01:44.162487', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:44,163 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 744541 word corpus (97.46% of original 763911, drops 19370)', 'datetime': '2025-10-15T00:01:44.163487', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:44,244 : INFO : deleting the raw counts dictionary of 41180 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:44,245 : INFO : sample=0.001 downsamples 16 most-common words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:44,246 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 736320.3421055112 word corpus (98.9%% of prior 744541)', 'datetime': '2025-10-15T00:01:44.246697', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:44,361 : INFO : estimated required memory for 21810 words and 150 dimensions: 37077000 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:44,362 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:44,373 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-15T00:01:44.373651', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:44,374 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 21810 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-15T00:01:44.374672', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:44,830 : INFO : EPOCH 0: training on 763911 raw words (736267 effective words) took 0.5s, 1624967 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:45,247 : INFO : EPOCH 1: training on 763911 raw words (736383 effective words) took 0.4s, 1778798 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:45,642 : INFO : EPOCH 2: training on 763911 raw words (736334 effective words) took 0.4s, 1877469 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:46,031 : INFO : EPOCH 3: training on 763911 raw words (736222 effective words) took 0.4s, 1907065 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:46,417 : INFO : EPOCH 4: training on 763911 raw words (736324 effective words) took 0.4s, 1915321 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:46,418 : INFO : Word2Vec lifecycle event {'msg': 'training on 3819555 raw words (3681530 effective words) took 2.0s, 1801938 effective words/s', 'datetime': '2025-10-15T00:01:46.418514', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 00:01:46,419 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=21810, vector_size=150, alpha=0.025>', 'datetime': '2025-10-15T00:01:46.419540', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pelatihan model selesai!\n",
      "\n",
      "--- Melihat Hasil Pelatihan Model ---\n",
      "Model berhasil mempelajari 21810 kata unik.\n",
      "\n",
      "Contoh kata yang paling mirip dengan 'polisi':\n",
      "[('lidi', 0.9040094017982483), ('polsek', 0.9031964540481567), ('kriminalitas', 0.8940128684043884), ('teror', 0.8730384111404419), ('massa', 0.8675649762153625)]\n",
      "\n",
      "Contoh kata yang paling mirip dengan 'surabaya':\n",
      "[('jabodetabek', 0.8062219619750977), ('lokatantra', 0.7965030074119568), ('darmo', 0.7894113063812256), ('bandung', 0.7862809896469116), ('armuji', 0.7820705771446228)]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Melatih Model Word2Vec (dengan Proses Terlihat) ---\n",
    "\n",
    "print(\"--- TAHAP 2: MELATIH MODEL WORD2VEC (CBOW) ---\")\n",
    "\n",
    "# Mengaktifkan logging untuk melihat proses training dari Gensim\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "embedding_dim = 150\n",
    "print(f\"Parameter: Dimensi vektor = {embedding_dim}, Arsitektur = CBOW\")\n",
    "print(\"Gensim akan menampilkan log proses training di bawah ini:\")\n",
    "\n",
    "model_cbow = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=embedding_dim,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=0,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "print(\"\\nPelatihan model selesai!\")\n",
    "print(\"\\n--- Melihat Hasil Pelatihan Model ---\")\n",
    "# Mengetahui ukuran kosakata yang berhasil dipelajari model\n",
    "vocab_size = len(model_cbow.wv.index_to_key)\n",
    "print(f\"Model berhasil mempelajari {vocab_size} kata unik.\")\n",
    "\n",
    "# Melihat kata-kata yang paling mirip secara semantik dengan kata tertentu\n",
    "# Ini membuktikan model sudah belajar konteks\n",
    "try:\n",
    "    print(\"\\nContoh kata yang paling mirip dengan 'polisi':\")\n",
    "    print(model_cbow.wv.most_similar('polisi', topn=5))\n",
    "\n",
    "    print(\"\\nContoh kata yang paling mirip dengan 'surabaya':\")\n",
    "    print(model_cbow.wv.most_similar('surabaya', topn=5))\n",
    "except KeyError as e:\n",
    "    print(f\"\\nKata {e} tidak ditemukan di vocabulary (mungkin karena jarang muncul).\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883b7da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 3: MEMBEDAH PROSES AGREGRASI MENJADI VEKTOR DOKUMEN ---\n",
      "Kita akan menganalisis berita pertama:\n",
      "Isi berita (token): ['kejar', 'tanjung', 'perak', 'surabaya', 'geledah', 'kantor', 'pt', 'pelindo', 'regional', 'surabaya', 'duga', 'kait', 'korupsi', 'kolam', 'labuh', 'kepala', 'kejar', 'tanjung', 'perak', 'ricky', 'setiawan', 'anas', 'geledah', 'kamis', 'wib', 'tim', 'sidik', 'kejar', 'tanjung', 'perak', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'dasar', 'tetap', 'pn', 'tipikor', 'surabaya', 'nomor', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'terang', 'kamis', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'sidik', 'pidsus', 'kejar', 'tanjung', 'perak', 'geledah', 'lokasi', 'tepat', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'dasar', 'tetap', 'geledah', 'pn', 'tipikor', 'surabaya', 'no', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'geledah', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'terang', 'geledah', 'personel', 'jaksa', 'kejar', 'tanjung', 'perak', 'pidsus', 'kejat', 'jatim', 'aman', 'tni', 'geledah', 'orang', 'jaksa', 'sidik', 'orang', 'personil', 'amc', 'kejat', 'jatim', 'orang', 'personil', 'pam', 'tni', 'imbuh', 'ricky', 'geledah', 'kait', 'sidi', 'perkara', 'duga', 'tindak', 'pidana', 'korupsi', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'pt', 'pelindo', 'sub', 'reg', 'bersamasama', 'pt', 'apbs', 'turut', 'duga', 'tipikor', 'turut', 'duga', 'tipikor', 'pt', 'apbs', 'pelindo', 'sub', 'regional', 'capai', 'ratus', 'miliar', 'rupiah', 'nilai', 'giat', 'rp', 'miliar', 'ricky', 'geledah', 'kumpul', 'bukti', 'tambah', 'kait', 'duga', 'tipikor', 'giat', 'eru', 'kolam', 'labuh', 'labuh', 'tanjung', 'perak', 'geledah', 'lokasi', 'sidik', 'laksana', 'giat', 'sita', 'kait', 'buktibukti', 'hubung', 'giat', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'laptop', 'dokumen', 'kait', 'kontrak', 'giat', 'rif']\n",
      "\n",
      "Vektor untuk 3 kata pertama dalam berita:\n",
      "  - Vektor kata 'kejar': [ 0.36153188 -0.341337    0.22766308 -0.1053263   0.28865147]... (ditampilkan 5 dimensi pertama)\n",
      "  - Vektor kata 'tanjung': [ 0.32309547 -0.15074779  0.01448453 -0.09839157  0.05258672]... (ditampilkan 5 dimensi pertama)\n",
      "  - Vektor kata 'perak': [ 0.64268494 -0.33720738 -0.15797544 -0.22875491  0.07537324]... (ditampilkan 5 dimensi pertama)\n",
      "\n",
      "Hasil vektor dokumen (setelah dirata-ratakan):\n",
      "[ 0.4035469  -0.20822111 -0.1857372   0.07720175  0.14711456 -0.3600409\n",
      " -0.15832737  0.14610817 -0.22679132 -0.1785826 ]... (ditampilkan 10 dimensi pertama)\n",
      "Panjang vektor: 150 dimensi (sesuai yang kita tentukan).\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Membedah Proses Agregasi Vektor Dokumen ---\n",
    "print(\"--- TAHAP 3: MEMBEDAH PROSES AGREGRASI MENJADI VEKTOR DOKUMEN ---\")\n",
    "\n",
    "def create_document_vector(doc, model, num_features):\n",
    "    word_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(num_features)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "contoh_berita = corpus[0]\n",
    "print(\"Kita akan menganalisis berita pertama:\")\n",
    "print(f\"Isi berita (token): {contoh_berita}\")\n",
    "\n",
    "print(\"\\nVektor untuk 3 kata pertama dalam berita:\")\n",
    "for i, word in enumerate(contoh_berita[:3]):\n",
    "    if word in model_cbow.wv:\n",
    "        print(f\"  - Vektor kata '{word}': {model_cbow.wv[word][:5]}... (ditampilkan 5 dimensi pertama)\")\n",
    "    else:\n",
    "        print(f\"  - Kata '{word}' tidak ada di vocabulary model.\")\n",
    "\n",
    "vektor_berita_contoh = create_document_vector(contoh_berita, model_cbow, embedding_dim)\n",
    "print(\"\\nHasil vektor dokumen (setelah dirata-ratakan):\")\n",
    "print(f\"{vektor_berita_contoh[:10]}... (ditampilkan 10 dimensi pertama)\")\n",
    "print(f\"Panjang vektor: {len(vektor_berita_contoh)} dimensi (sesuai yang kita tentukan).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0599b6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 4: MEMBUAT DATAFRAME AKHIR ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses pembuatan DataFrame selesai.\n",
      "Berikut adalah contoh hasil akhirnya:\n",
      "      dim_1     dim_2     dim_3     dim_4     dim_5     dim_6     dim_7  \\\n",
      "0  0.403547 -0.208221 -0.185737  0.077202  0.147115 -0.360041 -0.158327   \n",
      "1 -0.018997  0.210361  0.143095 -0.108400  0.250243  0.082247 -0.260284   \n",
      "2  0.211022  0.047539 -0.049166  0.045255  0.198704  0.031257 -0.072597   \n",
      "3  0.606416  0.176815 -0.067102  0.292311  0.445187 -0.367382 -0.209395   \n",
      "4  0.588632  0.258093 -0.295207  0.404603  0.309455 -0.475955 -0.056870   \n",
      "\n",
      "      dim_8     dim_9    dim_10  ...   dim_142   dim_143   dim_144   dim_145  \\\n",
      "0  0.146108 -0.226791 -0.178583  ...  0.115105  0.436662 -0.001523  0.079207   \n",
      "1  0.134297  0.160619 -0.176950  ... -0.191703  0.438775  0.161478  0.274480   \n",
      "2  0.120596  0.018175 -0.287623  ... -0.036783  0.521009  0.065834  0.166813   \n",
      "3  0.321794 -0.177884  0.135616  ...  0.318046  0.608954  0.268765  0.163802   \n",
      "4  0.372528 -0.385297  0.209718  ...  0.327645  0.520377  0.254118  0.254842   \n",
      "\n",
      "    dim_146   dim_147   dim_148   dim_149   dim_150  kategori  \n",
      "0 -0.120666 -0.307630 -0.122666  0.150974 -0.146329     Jatim  \n",
      "1  0.106800 -0.364050  0.034868 -0.198417 -0.499147     Jatim  \n",
      "2  0.179145 -0.352997  0.088641 -0.121084 -0.423821     Jatim  \n",
      "3 -0.309352 -0.377879 -0.101111  0.266065 -0.434790     Jatim  \n",
      "4 -0.324620 -0.338431 -0.152293  0.269945 -0.202531     Jatim  \n",
      "\n",
      "[5 rows x 151 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil CBOW berhasil disimpan ke file: hasil_cbow_berita.csv ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Membuat DataFrame Akhir ---\n",
    "print(\"--- TAHAP 4: MEMBUAT DATAFRAME AKHIR ---\")\n",
    "doc_vectors = [create_document_vector(doc, model_cbow, embedding_dim) for doc in corpus]\n",
    "cbow_df = pd.DataFrame(doc_vectors, columns=[f'dim_{i+1}' for i in range(embedding_dim)])\n",
    "cbow_df['kategori'] = df['kategori'].values\n",
    "\n",
    "print(\"Proses pembuatan DataFrame selesai.\")\n",
    "print(\"Berikut adalah contoh hasil akhirnya:\")\n",
    "print(cbow_df.head())\n",
    "\n",
    "# Simpan ke file CSV\n",
    "output_file_cbow = \"hasil_cbow_berita.csv\"\n",
    "cbow_df.to_csv(output_file_cbow, index=False)\n",
    "\n",
    "print(f\"\\nHasil CBOW berhasil disimpan ke file: {output_file_cbow} ðŸš€\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}