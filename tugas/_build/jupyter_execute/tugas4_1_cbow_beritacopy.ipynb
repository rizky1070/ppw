{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89da0803",
   "metadata": {},
   "source": [
    "# CBOW Berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03765425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4319e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e3d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 1: MEMPERSIAPKAN CORPUS ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses persiapan corpus selesai.\n",
      "Berikut adalah contoh 1 dokumen (berita) yang sudah diubah menjadi daftar token:\n",
      "['kejar', 'tanjung', 'perak', 'surabaya', 'geledah', 'kantor', 'pt', 'pelindo', 'regional', 'surabaya', 'duga', 'kait', 'korupsi', 'kolam', 'labuh', 'kepala', 'kejar', 'tanjung', 'perak', 'ricky', 'setiawan', 'anas', 'geledah', 'kamis', 'wib', 'tim', 'sidik', 'kejar', 'tanjung', 'perak', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'dasar', 'tetap', 'pn', 'tipikor', 'surabaya', 'nomor', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'terang', 'kamis', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'sidik', 'pidsus', 'kejar', 'tanjung', 'perak', 'geledah', 'lokasi', 'tepat', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'dasar', 'tetap', 'geledah', 'pn', 'tipikor', 'surabaya', 'no', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'geledah', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'terang', 'geledah', 'personel', 'jaksa', 'kejar', 'tanjung', 'perak', 'pidsus', 'kejat', 'jatim', 'aman', 'tni', 'geledah', 'orang', 'jaksa', 'sidik', 'orang', 'personil', 'amc', 'kejat', 'jatim', 'orang', 'personil', 'pam', 'tni', 'imbuh', 'ricky', 'geledah', 'kait', 'sidi', 'perkara', 'duga', 'tindak', 'pidana', 'korupsi', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'pt', 'pelindo', 'sub', 'reg', 'bersamasama', 'pt', 'apbs', 'turut', 'duga', 'tipikor', 'turut', 'duga', 'tipikor', 'pt', 'apbs', 'pelindo', 'sub', 'regional', 'capai', 'ratus', 'miliar', 'rupiah', 'nilai', 'giat', 'rp', 'miliar', 'ricky', 'geledah', 'kumpul', 'bukti', 'tambah', 'kait', 'duga', 'tipikor', 'giat', 'eru', 'kolam', 'labuh', 'labuh', 'tanjung', 'perak', 'geledah', 'lokasi', 'sidik', 'laksana', 'giat', 'sita', 'kait', 'buktibukti', 'hubung', 'giat', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'laptop', 'dokumen', 'kait', 'kontrak', 'giat', 'rif']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast # Library untuk mengubah string menjadi list\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "\n",
    "# --- 1. Persiapan Corpus ---\n",
    "print(\"--- TAHAP 1: MEMPERSIAPKAN CORPUS ---\")\n",
    "# Load kembali data Anda (atau lanjutkan dari DataFrame yang sudah ada)\n",
    "df = pd.read_csv('hasil_preprocessing_berita.csv')\n",
    "\n",
    "# --- Konversi kolom 'hasil_preprocessing' dari string ke list ---\n",
    "# Ini adalah langkah penting!\n",
    "# ast.literal_eval akan membaca string \"['a', 'b']\" dan mengubahnya menjadi list ['a', 'b']\n",
    "df['tokens'] = df['hasil_preprocessing'].apply(ast.literal_eval)\n",
    "\n",
    "# Buat corpus yang siap untuk dilatih\n",
    "corpus = df['tokens'].tolist()\n",
    "\n",
    "\n",
    "print(\"Proses persiapan corpus selesai.\")\n",
    "print(\"Berikut adalah contoh 1 dokumen (berita) yang sudah diubah menjadi daftar token:\")\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10107212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,245 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,245 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,335 : INFO : collected 41180 word types from a corpus of 763911 raw words and 3653 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,336 : INFO : Creating a fresh vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,387 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 21810 unique words (52.96% of original 41180, drops 19370)', 'datetime': '2025-10-16T18:33:28.387920', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,388 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 744541 word corpus (97.46% of original 763911, drops 19370)', 'datetime': '2025-10-16T18:33:28.388920', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 2: MELATIH MODEL WORD2VEC (CBOW) ---\n",
      "Parameter: Dimensi vektor = 150, Arsitektur = CBOW\n",
      "Gensim akan menampilkan log proses training di bawah ini:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,456 : INFO : deleting the raw counts dictionary of 41180 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,457 : INFO : sample=0.001 downsamples 16 most-common words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,458 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 736320.3421055112 word corpus (98.9%% of prior 744541)', 'datetime': '2025-10-16T18:33:28.458921', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,617 : INFO : estimated required memory for 21810 words and 150 dimensions: 37077000 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,618 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,630 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-16T18:33:28.630246', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:28,631 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 21810 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-16T18:33:28.631245', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:29,027 : INFO : EPOCH 0: training on 763911 raw words (736410 effective words) took 0.4s, 1877082 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:29,437 : INFO : EPOCH 1: training on 763911 raw words (736244 effective words) took 0.4s, 1805260 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:29,838 : INFO : EPOCH 2: training on 763911 raw words (736357 effective words) took 0.4s, 1848246 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:30,229 : INFO : EPOCH 3: training on 763911 raw words (736264 effective words) took 0.4s, 1896950 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:30,625 : INFO : EPOCH 4: training on 763911 raw words (736417 effective words) took 0.4s, 1873941 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:30,626 : INFO : Word2Vec lifecycle event {'msg': 'training on 3819555 raw words (3681692 effective words) took 2.0s, 1846246 effective words/s', 'datetime': '2025-10-16T18:33:30.626245', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:33:30,627 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=21810, vector_size=150, alpha=0.025>', 'datetime': '2025-10-16T18:33:30.627246', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pelatihan model selesai!\n",
      "\n",
      "--- Melihat Hasil Pelatihan Model ---\n",
      "Model berhasil mempelajari 21810 kata unik.\n",
      "\n",
      "Contoh kata yang paling mirip dengan 'polisi':\n",
      "[('lidi', 0.895498514175415), ('polsek', 0.8926848769187927), ('pistol', 0.8651630282402039), ('lauren', 0.8605530858039856), ('massa', 0.8582267761230469)]\n",
      "\n",
      "Contoh kata yang paling mirip dengan 'surabaya':\n",
      "[('jabodetabek', 0.8079806566238403), ('connie', 0.8016188144683838), ('eri', 0.7803473472595215), ('alakbar', 0.768369197845459), ('rajawali', 0.7672732472419739)]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Melatih Model Word2Vec (dengan Proses Terlihat) ---\n",
    "\n",
    "print(\"--- TAHAP 2: MELATIH MODEL WORD2VEC (CBOW) ---\")\n",
    "\n",
    "# Mengaktifkan logging untuk melihat proses training dari Gensim\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "embedding_dim = 150\n",
    "print(f\"Parameter: Dimensi vektor = {embedding_dim}, Arsitektur = CBOW\")\n",
    "print(\"Gensim akan menampilkan log proses training di bawah ini:\")\n",
    "\n",
    "model_cbow = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=embedding_dim,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=0,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "print(\"\\nPelatihan model selesai!\")\n",
    "print(\"\\n--- Melihat Hasil Pelatihan Model ---\")\n",
    "# Mengetahui ukuran kosakata yang berhasil dipelajari model\n",
    "vocab_size = len(model_cbow.wv.index_to_key)\n",
    "print(f\"Model berhasil mempelajari {vocab_size} kata unik.\")\n",
    "\n",
    "# Melihat kata-kata yang paling mirip secara semantik dengan kata tertentu\n",
    "# Ini membuktikan model sudah belajar konteks\n",
    "try:\n",
    "    print(\"\\nContoh kata yang paling mirip dengan 'polisi':\")\n",
    "    print(model_cbow.wv.most_similar('polisi', topn=5))\n",
    "\n",
    "    print(\"\\nContoh kata yang paling mirip dengan 'surabaya':\")\n",
    "    print(model_cbow.wv.most_similar('surabaya', topn=5))\n",
    "except KeyError as e:\n",
    "    print(f\"\\nKata {e} tidak ditemukan di vocabulary (mungkin karena jarang muncul).\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883b7da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 3: MEMBEDAH PROSES AGREGRASI MENJADI VEKTOR DOKUMEN ---\n",
      "Kita akan menganalisis berita pertama:\n",
      "Isi berita (token): ['kejar', 'tanjung', 'perak', 'surabaya', 'geledah', 'kantor', 'pt', 'pelindo', 'regional', 'surabaya', 'duga', 'kait', 'korupsi', 'kolam', 'labuh', 'kepala', 'kejar', 'tanjung', 'perak', 'ricky', 'setiawan', 'anas', 'geledah', 'kamis', 'wib', 'tim', 'sidik', 'kejar', 'tanjung', 'perak', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'dasar', 'tetap', 'pn', 'tipikor', 'surabaya', 'nomor', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'terang', 'kamis', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'sidik', 'pidsus', 'kejar', 'tanjung', 'perak', 'geledah', 'lokasi', 'tepat', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'dasar', 'tetap', 'geledah', 'pn', 'tipikor', 'surabaya', 'no', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'geledah', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'terang', 'geledah', 'personel', 'jaksa', 'kejar', 'tanjung', 'perak', 'pidsus', 'kejat', 'jatim', 'aman', 'tni', 'geledah', 'orang', 'jaksa', 'sidik', 'orang', 'personil', 'amc', 'kejat', 'jatim', 'orang', 'personil', 'pam', 'tni', 'imbuh', 'ricky', 'geledah', 'kait', 'sidi', 'perkara', 'duga', 'tindak', 'pidana', 'korupsi', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'pt', 'pelindo', 'sub', 'reg', 'bersamasama', 'pt', 'apbs', 'turut', 'duga', 'tipikor', 'turut', 'duga', 'tipikor', 'pt', 'apbs', 'pelindo', 'sub', 'regional', 'capai', 'ratus', 'miliar', 'rupiah', 'nilai', 'giat', 'rp', 'miliar', 'ricky', 'geledah', 'kumpul', 'bukti', 'tambah', 'kait', 'duga', 'tipikor', 'giat', 'eru', 'kolam', 'labuh', 'labuh', 'tanjung', 'perak', 'geledah', 'lokasi', 'sidik', 'laksana', 'giat', 'sita', 'kait', 'buktibukti', 'hubung', 'giat', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'laptop', 'dokumen', 'kait', 'kontrak', 'giat', 'rif']\n",
      "\n",
      "Vektor untuk 3 kata pertama dalam berita:\n",
      "  - Vektor kata 'kejar': [ 0.45257697 -0.3118289   0.29680994 -0.15442027  0.15032502]... (ditampilkan 5 dimensi pertama)\n",
      "  - Vektor kata 'tanjung': [ 0.22083174 -0.06564209 -0.02529565 -0.15197787  0.0654714 ]... (ditampilkan 5 dimensi pertama)\n",
      "  - Vektor kata 'perak': [ 0.45065194 -0.1153901  -0.13401623 -0.26392618  0.0386176 ]... (ditampilkan 5 dimensi pertama)\n",
      "\n",
      "Hasil vektor dokumen (setelah dirata-ratakan):\n",
      "[ 0.4277612  -0.2184069  -0.11334363 -0.02293004  0.06036385 -0.38827676\n",
      " -0.14163052  0.11350027 -0.2215733  -0.11899167]... (ditampilkan 10 dimensi pertama)\n",
      "Panjang vektor: 150 dimensi (sesuai yang kita tentukan).\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Membedah Proses Agregasi Vektor Dokumen ---\n",
    "print(\"--- TAHAP 3: MEMBEDAH PROSES AGREGRASI MENJADI VEKTOR DOKUMEN ---\")\n",
    "\n",
    "def create_document_vector(doc, model, num_features):\n",
    "    word_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(num_features)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "contoh_berita = corpus[0]\n",
    "print(\"Kita akan menganalisis berita pertama:\")\n",
    "print(f\"Isi berita (token): {contoh_berita}\")\n",
    "\n",
    "print(\"\\nVektor untuk 3 kata pertama dalam berita:\")\n",
    "for i, word in enumerate(contoh_berita[:3]):\n",
    "    if word in model_cbow.wv:\n",
    "        print(f\"  - Vektor kata '{word}': {model_cbow.wv[word][:5]}... (ditampilkan 5 dimensi pertama)\")\n",
    "    else:\n",
    "        print(f\"  - Kata '{word}' tidak ada di vocabulary model.\")\n",
    "\n",
    "vektor_berita_contoh = create_document_vector(contoh_berita, model_cbow, embedding_dim)\n",
    "print(\"\\nHasil vektor dokumen (setelah dirata-ratakan):\")\n",
    "print(f\"{vektor_berita_contoh[:10]}... (ditampilkan 10 dimensi pertama)\")\n",
    "print(f\"Panjang vektor: {len(vektor_berita_contoh)} dimensi (sesuai yang kita tentukan).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0599b6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 4: MEMBUAT DATAFRAME AKHIR ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses pembuatan DataFrame selesai.\n",
      "Berikut adalah contoh hasil akhirnya:\n",
      "      dim_1     dim_2     dim_3     dim_4     dim_5     dim_6     dim_7  \\\n",
      "0  0.427761 -0.218407 -0.113344 -0.022930  0.060364 -0.388277 -0.141631   \n",
      "1  0.030143  0.153321  0.194561 -0.233974  0.276297  0.021280 -0.266694   \n",
      "2  0.280340  0.045892  0.041599 -0.086299  0.185929  0.007203 -0.093485   \n",
      "3  0.605321  0.113450 -0.000884  0.020918  0.337251 -0.510218 -0.216502   \n",
      "4  0.608769  0.172980 -0.227981  0.193275  0.171901 -0.522278 -0.063841   \n",
      "\n",
      "      dim_8     dim_9    dim_10  ...   dim_142   dim_143   dim_144   dim_145  \\\n",
      "0  0.113500 -0.221573 -0.118992  ...  0.159526  0.411759  0.076129  0.082384   \n",
      "1  0.048166  0.274799 -0.130876  ... -0.105527  0.347099  0.158297  0.238896   \n",
      "2  0.025078  0.128023 -0.235783  ...  0.054060  0.456936  0.095960  0.205434   \n",
      "3  0.106672 -0.243263  0.325715  ...  0.404612  0.472092  0.363166  0.210609   \n",
      "4  0.203811 -0.398954  0.248756  ...  0.310622  0.428708  0.308700  0.299467   \n",
      "\n",
      "    dim_146   dim_147   dim_148   dim_149   dim_150  kategori  \n",
      "0 -0.079494 -0.451779 -0.009489  0.214923 -0.113247     Jatim  \n",
      "1  0.111107 -0.463134  0.145442 -0.188175 -0.427569     Jatim  \n",
      "2  0.209897 -0.482070  0.188654 -0.065769 -0.335892     Jatim  \n",
      "3 -0.355525 -0.556449  0.016222  0.249641 -0.397314     Jatim  \n",
      "4 -0.366122 -0.532511 -0.111952  0.299630 -0.147050     Jatim  \n",
      "\n",
      "[5 rows x 151 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil CBOW berhasil disimpan ke file: hasil_cbow_berita.csv ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Membuat DataFrame Akhir ---\n",
    "print(\"--- TAHAP 4: MEMBUAT DATAFRAME AKHIR ---\")\n",
    "doc_vectors = [create_document_vector(doc, model_cbow, embedding_dim) for doc in corpus]\n",
    "cbow_df = pd.DataFrame(doc_vectors, columns=[f'dim_{i+1}' for i in range(embedding_dim)])\n",
    "cbow_df['kategori'] = df['kategori'].values\n",
    "\n",
    "print(\"Proses pembuatan DataFrame selesai.\")\n",
    "print(\"Berikut adalah contoh hasil akhirnya:\")\n",
    "print(cbow_df.head())\n",
    "\n",
    "# Simpan ke file CSV\n",
    "output_file_cbow = \"hasil_cbow_berita.csv\"\n",
    "cbow_df.to_csv(output_file_cbow, index=False)\n",
    "\n",
    "print(f\"\\nHasil CBOW berhasil disimpan ke file: {output_file_cbow} ðŸš€\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}