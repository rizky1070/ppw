{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89da0803",
   "metadata": {},
   "source": [
    "# CBOW Berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03765425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4319e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e3d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 1: MEMPERSIAPKAN CORPUS ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses persiapan corpus selesai.\n",
      "Berikut adalah contoh 1 dokumen (berita) yang sudah diubah menjadi daftar token:\n",
      "['kejar', 'tanjung', 'perak', 'surabaya', 'geledah', 'kantor', 'pt', 'pelindo', 'regional', 'surabaya', 'duga', 'kait', 'korupsi', 'kolam', 'labuh', 'kepala', 'kejar', 'tanjung', 'perak', 'ricky', 'setiawan', 'anas', 'geledah', 'kamis', 'wib', 'tim', 'sidik', 'kejar', 'tanjung', 'perak', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'dasar', 'tetap', 'pn', 'tipikor', 'surabaya', 'nomor', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'terang', 'kamis', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'sidik', 'pidsus', 'kejar', 'tanjung', 'perak', 'geledah', 'lokasi', 'tepat', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'dasar', 'tetap', 'geledah', 'pn', 'tipikor', 'surabaya', 'no', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'geledah', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'terang', 'geledah', 'personel', 'jaksa', 'kejar', 'tanjung', 'perak', 'pidsus', 'kejat', 'jatim', 'aman', 'tni', 'geledah', 'orang', 'jaksa', 'sidik', 'orang', 'personil', 'amc', 'kejat', 'jatim', 'orang', 'personil', 'pam', 'tni', 'imbuh', 'ricky', 'geledah', 'kait', 'sidi', 'perkara', 'duga', 'tindak', 'pidana', 'korupsi', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'pt', 'pelindo', 'sub', 'reg', 'bersamasama', 'pt', 'apbs', 'turut', 'duga', 'tipikor', 'turut', 'duga', 'tipikor', 'pt', 'apbs', 'pelindo', 'sub', 'regional', 'capai', 'ratus', 'miliar', 'rupiah', 'nilai', 'giat', 'rp', 'miliar', 'ricky', 'geledah', 'kumpul', 'bukti', 'tambah', 'kait', 'duga', 'tipikor', 'giat', 'eru', 'kolam', 'labuh', 'labuh', 'tanjung', 'perak', 'geledah', 'lokasi', 'sidik', 'laksana', 'giat', 'sita', 'kait', 'buktibukti', 'hubung', 'giat', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'laptop', 'dokumen', 'kait', 'kontrak', 'giat', 'rif']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast # Library untuk mengubah string menjadi list\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "\n",
    "# --- 1. Persiapan Corpus ---\n",
    "print(\"--- TAHAP 1: MEMPERSIAPKAN CORPUS ---\")\n",
    "# Load kembali data Anda (atau lanjutkan dari DataFrame yang sudah ada)\n",
    "df = pd.read_csv('hasil_preprocessing_berita.csv')\n",
    "\n",
    "# --- Konversi kolom 'hasil_preprocessing' dari string ke list ---\n",
    "# Ini adalah langkah penting!\n",
    "# ast.literal_eval akan membaca string \"['a', 'b']\" dan mengubahnya menjadi list ['a', 'b']\n",
    "df['tokens'] = df['hasil_preprocessing'].apply(ast.literal_eval)\n",
    "\n",
    "# Buat corpus yang siap untuk dilatih\n",
    "corpus = df['tokens'].tolist()\n",
    "\n",
    "\n",
    "print(\"Proses persiapan corpus selesai.\")\n",
    "print(\"Berikut adalah contoh 1 dokumen (berita) yang sudah diubah menjadi daftar token:\")\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10107212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:03,694 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:03,695 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:03,785 : INFO : collected 41180 word types from a corpus of 763911 raw words and 3653 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:03,786 : INFO : Creating a fresh vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:03,837 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 21810 unique words (52.96% of original 41180, drops 19370)', 'datetime': '2025-10-16T18:01:03.837808', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:03,838 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 744541 word corpus (97.46% of original 763911, drops 19370)', 'datetime': '2025-10-16T18:01:03.838717', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 2: MELATIH MODEL WORD2VEC (CBOW) ---\n",
      "Parameter: Dimensi vektor = 150, Arsitektur = CBOW\n",
      "Gensim akan menampilkan log proses training di bawah ini:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:03,914 : INFO : deleting the raw counts dictionary of 41180 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:03,916 : INFO : sample=0.001 downsamples 16 most-common words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:03,916 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 736320.3421055112 word corpus (98.9%% of prior 744541)', 'datetime': '2025-10-16T18:01:03.916206', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:04,030 : INFO : estimated required memory for 21810 words and 150 dimensions: 37077000 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:04,031 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:04,043 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-16T18:01:04.043901', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:04,044 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 21810 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-16T18:01:04.044902', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:04,618 : INFO : EPOCH 0: training on 763911 raw words (736314 effective words) took 0.6s, 1289685 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:05,097 : INFO : EPOCH 1: training on 763911 raw words (736320 effective words) took 0.5s, 1549720 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:05,562 : INFO : EPOCH 2: training on 763911 raw words (736303 effective words) took 0.5s, 1596112 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:06,127 : INFO : EPOCH 3: training on 763911 raw words (736326 effective words) took 0.6s, 1311768 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:06,862 : INFO : EPOCH 4: training on 763911 raw words (736409 effective words) took 0.7s, 1009955 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:06,863 : INFO : Word2Vec lifecycle event {'msg': 'training on 3819555 raw words (3681672 effective words) took 2.8s, 1306617 effective words/s', 'datetime': '2025-10-16T18:01:06.863798', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 18:01:06,864 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=21810, vector_size=150, alpha=0.025>', 'datetime': '2025-10-16T18:01:06.864813', 'gensim': '4.3.3', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pelatihan model selesai!\n",
      "\n",
      "--- Melihat Hasil Pelatihan Model ---\n",
      "Model berhasil mempelajari 21810 kata unik.\n",
      "\n",
      "Contoh kata yang paling mirip dengan 'polisi':\n",
      "[('lidi', 0.8979644775390625), ('polsek', 0.8853654861450195), ('aksi', 0.8760371208190918), ('rusuh', 0.8724260330200195), ('massa', 0.8656960129737854)]\n",
      "\n",
      "Contoh kata yang paling mirip dengan 'surabaya':\n",
      "[('didapatkanbangsaonlinemereka', 0.8134261965751648), ('unipa', 0.7956990003585815), ('tunjung', 0.7954279184341431), ('gedung', 0.7881429195404053), ('connie', 0.7814972996711731)]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Melatih Model Word2Vec (dengan Proses Terlihat) ---\n",
    "\n",
    "print(\"--- TAHAP 2: MELATIH MODEL WORD2VEC (CBOW) ---\")\n",
    "\n",
    "# Mengaktifkan logging untuk melihat proses training dari Gensim\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "embedding_dim = 150\n",
    "print(f\"Parameter: Dimensi vektor = {embedding_dim}, Arsitektur = CBOW\")\n",
    "print(\"Gensim akan menampilkan log proses training di bawah ini:\")\n",
    "\n",
    "model_cbow = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=embedding_dim,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=0,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "print(\"\\nPelatihan model selesai!\")\n",
    "print(\"\\n--- Melihat Hasil Pelatihan Model ---\")\n",
    "# Mengetahui ukuran kosakata yang berhasil dipelajari model\n",
    "vocab_size = len(model_cbow.wv.index_to_key)\n",
    "print(f\"Model berhasil mempelajari {vocab_size} kata unik.\")\n",
    "\n",
    "# Melihat kata-kata yang paling mirip secara semantik dengan kata tertentu\n",
    "# Ini membuktikan model sudah belajar konteks\n",
    "try:\n",
    "    print(\"\\nContoh kata yang paling mirip dengan 'polisi':\")\n",
    "    print(model_cbow.wv.most_similar('polisi', topn=5))\n",
    "\n",
    "    print(\"\\nContoh kata yang paling mirip dengan 'surabaya':\")\n",
    "    print(model_cbow.wv.most_similar('surabaya', topn=5))\n",
    "except KeyError as e:\n",
    "    print(f\"\\nKata {e} tidak ditemukan di vocabulary (mungkin karena jarang muncul).\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883b7da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 3: MEMBEDAH PROSES AGREGRASI MENJADI VEKTOR DOKUMEN ---\n",
      "Kita akan menganalisis berita pertama:\n",
      "Isi berita (token): ['kejar', 'tanjung', 'perak', 'surabaya', 'geledah', 'kantor', 'pt', 'pelindo', 'regional', 'surabaya', 'duga', 'kait', 'korupsi', 'kolam', 'labuh', 'kepala', 'kejar', 'tanjung', 'perak', 'ricky', 'setiawan', 'anas', 'geledah', 'kamis', 'wib', 'tim', 'sidik', 'kejar', 'tanjung', 'perak', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'dasar', 'tetap', 'pn', 'tipikor', 'surabaya', 'nomor', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'damping', 'tim', 'amc', 'asintel', 'kejat', 'jatim', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'terang', 'kamis', 'geledah', 'kantor', 'pt', 'pelindo', 'sub', 'regional', 'surabaya', 'ricky', 'sidik', 'pidsus', 'kejar', 'tanjung', 'perak', 'geledah', 'lokasi', 'tepat', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'dasar', 'tetap', 'geledah', 'pn', 'tipikor', 'surabaya', 'no', 'nomor', 'penpidsustpkgldpn', 'sby', 'tanggal', 'oktober', 'geledah', 'kantor', 'pt', 'alur', 'layar', 'barat', 'surabaya', 'apbs', 'terang', 'geledah', 'personel', 'jaksa', 'kejar', 'tanjung', 'perak', 'pidsus', 'kejat', 'jatim', 'aman', 'tni', 'geledah', 'orang', 'jaksa', 'sidik', 'orang', 'personil', 'amc', 'kejat', 'jatim', 'orang', 'personil', 'pam', 'tni', 'imbuh', 'ricky', 'geledah', 'kait', 'sidi', 'perkara', 'duga', 'tindak', 'pidana', 'korupsi', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'pt', 'pelindo', 'sub', 'reg', 'bersamasama', 'pt', 'apbs', 'turut', 'duga', 'tipikor', 'turut', 'duga', 'tipikor', 'pt', 'apbs', 'pelindo', 'sub', 'regional', 'capai', 'ratus', 'miliar', 'rupiah', 'nilai', 'giat', 'rp', 'miliar', 'ricky', 'geledah', 'kumpul', 'bukti', 'tambah', 'kait', 'duga', 'tipikor', 'giat', 'eru', 'kolam', 'labuh', 'labuh', 'tanjung', 'perak', 'geledah', 'lokasi', 'sidik', 'laksana', 'giat', 'sita', 'kait', 'buktibukti', 'hubung', 'giat', 'pelihara', 'usaha', 'kolam', 'labuh', 'tanjung', 'perak', 'laptop', 'dokumen', 'kait', 'kontrak', 'giat', 'rif']\n",
      "\n",
      "Vektor untuk 3 kata pertama dalam berita:\n",
      "  - Vektor kata 'kejar': [ 0.42985618 -0.34527084  0.24697481 -0.04545346  0.24379872]... (ditampilkan 5 dimensi pertama)\n",
      "  - Vektor kata 'tanjung': [ 0.2615779  -0.17968328 -0.19561799 -0.19634289  0.11716372]... (ditampilkan 5 dimensi pertama)\n",
      "  - Vektor kata 'perak': [ 0.40832278 -0.22455387 -0.35284832 -0.22658344  0.05329632]... (ditampilkan 5 dimensi pertama)\n",
      "\n",
      "Hasil vektor dokumen (setelah dirata-ratakan):\n",
      "[ 0.37167716 -0.16021605 -0.17855148  0.08616753  0.12305143 -0.32475507\n",
      " -0.13921586  0.07007308 -0.23829551 -0.08019516]... (ditampilkan 10 dimensi pertama)\n",
      "Panjang vektor: 150 dimensi (sesuai yang kita tentukan).\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Membedah Proses Agregasi Vektor Dokumen ---\n",
    "print(\"--- TAHAP 3: MEMBEDAH PROSES AGREGRASI MENJADI VEKTOR DOKUMEN ---\")\n",
    "\n",
    "def create_document_vector(doc, model, num_features):\n",
    "    word_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(num_features)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "contoh_berita = corpus[0]\n",
    "print(\"Kita akan menganalisis berita pertama:\")\n",
    "print(f\"Isi berita (token): {contoh_berita}\")\n",
    "\n",
    "print(\"\\nVektor untuk 3 kata pertama dalam berita:\")\n",
    "for i, word in enumerate(contoh_berita[:3]):\n",
    "    if word in model_cbow.wv:\n",
    "        print(f\"  - Vektor kata '{word}': {model_cbow.wv[word][:5]}... (ditampilkan 5 dimensi pertama)\")\n",
    "    else:\n",
    "        print(f\"  - Kata '{word}' tidak ada di vocabulary model.\")\n",
    "\n",
    "vektor_berita_contoh = create_document_vector(contoh_berita, model_cbow, embedding_dim)\n",
    "print(\"\\nHasil vektor dokumen (setelah dirata-ratakan):\")\n",
    "print(f\"{vektor_berita_contoh[:10]}... (ditampilkan 10 dimensi pertama)\")\n",
    "print(f\"Panjang vektor: {len(vektor_berita_contoh)} dimensi (sesuai yang kita tentukan).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0599b6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAHAP 4: MEMBUAT DATAFRAME AKHIR ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses pembuatan DataFrame selesai.\n",
      "Berikut adalah contoh hasil akhirnya:\n",
      "      dim_1     dim_2     dim_3     dim_4     dim_5     dim_6     dim_7  \\\n",
      "0  0.371677 -0.160216 -0.178551  0.086168  0.123051 -0.324755 -0.139216   \n",
      "1  0.030923  0.200845  0.176294 -0.186812  0.244480  0.159063 -0.254543   \n",
      "2  0.234711  0.072461  0.002858 -0.026370  0.179861  0.089617 -0.108446   \n",
      "3  0.481805  0.372802  0.025847  0.153337  0.231178 -0.221437 -0.178571   \n",
      "4  0.436440  0.398799 -0.249557  0.317539  0.124715 -0.359583 -0.038896   \n",
      "\n",
      "      dim_8     dim_9    dim_10  ...   dim_142   dim_143   dim_144   dim_145  \\\n",
      "0  0.070073 -0.238296 -0.080195  ...  0.253514  0.370347  0.044236  0.102512   \n",
      "1  0.019972  0.261638 -0.158590  ... -0.030912  0.325295  0.189612  0.235430   \n",
      "2  0.047054  0.107301 -0.257602  ...  0.141634  0.408371  0.101164  0.213162   \n",
      "3  0.129845 -0.089257  0.139130  ...  0.350267  0.453926  0.562840  0.243212   \n",
      "4  0.197730 -0.310077  0.180363  ...  0.324062  0.396947  0.477393  0.336477   \n",
      "\n",
      "    dim_146   dim_147   dim_148   dim_149   dim_150  kategori  \n",
      "0 -0.138579 -0.510194 -0.092914  0.163956 -0.076704     Jatim  \n",
      "1  0.082419 -0.546299  0.030471 -0.115950 -0.349555     Jatim  \n",
      "2  0.166465 -0.609106  0.061104  0.042741 -0.300589     Jatim  \n",
      "3 -0.457178 -0.586126  0.031449  0.323090 -0.163161     Jatim  \n",
      "4 -0.416310 -0.600531 -0.077586  0.315010  0.010652     Jatim  \n",
      "\n",
      "[5 rows x 151 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil CBOW berhasil disimpan ke file: hasil_cbow_berita.csv 🚀\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Membuat DataFrame Akhir ---\n",
    "print(\"--- TAHAP 4: MEMBUAT DATAFRAME AKHIR ---\")\n",
    "doc_vectors = [create_document_vector(doc, model_cbow, embedding_dim) for doc in corpus]\n",
    "cbow_df = pd.DataFrame(doc_vectors, columns=[f'dim_{i+1}' for i in range(embedding_dim)])\n",
    "cbow_df['kategori'] = df['kategori'].values\n",
    "\n",
    "print(\"Proses pembuatan DataFrame selesai.\")\n",
    "print(\"Berikut adalah contoh hasil akhirnya:\")\n",
    "print(cbow_df.head())\n",
    "\n",
    "# Simpan ke file CSV\n",
    "output_file_cbow = \"hasil_cbow_berita.csv\"\n",
    "cbow_df.to_csv(output_file_cbow, index=False)\n",
    "\n",
    "print(f\"\\nHasil CBOW berhasil disimpan ke file: {output_file_cbow} 🚀\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}